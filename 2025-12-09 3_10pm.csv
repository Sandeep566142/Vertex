TABLE_NAME,LOCATION_FOUND,DDL_DEFINITION
TD_DATA_LOAD,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_DATA_LOAD""","create or replace TRANSIENT TABLE TD_DATA_LOAD (
	NR_MIN_TIME_WEEK_KEY NUMBER(10,0),
	NR_TIME_WEEK_KEY NUMBER(10,0),
	NR_TIME_MTH_KEY NUMBER(10,0),
	NR_FULL_TIME_MTH_KEY NUMBER(10,0),
	RX_MIN_TIME_SWEEK_KEY NUMBER(10,0),
	RX_TIME_SWEEK_KEY NUMBER(10,0),
	RX_TIME_MTH_KEY NUMBER(10,0),
	RX_FULL_TIME_MTH_KEY NUMBER(10,0),
	LAAD_CLAIM_TIME_START_DT_KEY NUMBER(10,0),
	LAAD_CLAIM_START_MTH_KEY NUMBER(10,0),
	LAAD_CLAIM_TIME_END_DT_KEY NUMBER(10,0),
	LAAD_CLAIM_TIME_END_MTH_KEY NUMBER(10,0),
	LAAD_CLAIM_TIME_DT_KEY NUMBER(10,0),
	INT_SALES_TIME_DT_KEY NUMBER(10,0),
	INT_SALES_FULL_TIME_WEEK_KEY NUMBER(10,0),
	INT_SALES_FULL_TIME_MTH_KEY NUMBER(10,0),
	INV_DISP_SALES_TIME_DT_KEY NUMBER(10,0),
	INT_SALES_ACTG_TIME_DT_KEY NUMBER(10,0),
	INT_SALES_ACTG_FULL_TIME_WEEK_KEY NUMBER(10,0),
	INT_SALES_ACTG_FULL_TIME_MTH_KEY NUMBER(10,0),
	INV_INV_TIME_DT_KEY NUMBER(10,0),
	SD_SALES_TIME_DT_KEY NUMBER(10,0),
	PREV_SF_ALN_KEY VARCHAR(32),
	CURR_SF_ALN_KEY VARCHAR(32),
	NEW_SF_ALN_KEY VARCHAR(32),
	PREV_IC_SF_ALN_KEY VARCHAR(32),
	CURR_IC_SF_ALN_KEY VARCHAR(32),
	NEW_IC_SF_ALN_KEY VARCHAR(32),
	PAIN_BU_BUDGET_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_BU_CURR_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_BU_PRIOR_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_BU_WORK_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_HYP_ACTUAL_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_HYP_BUDGET_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_HYP_CURR_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_HYP_PRIOR_FORECAST_VERSION_KEY VARCHAR(32),
	PAIN_HYP_WORK_FORECAST_VERSION_KEY VARCHAR(32),
	PROD_SUMM_REPORT_TIME_DT_KEY NUMBER(10,0),
	PROD_SUMM_FULL_TIME_MTH_KEY NUMBER(10,0),
	LOAD_START_DATE NUMBER(10,0)
);"
TD_GEO_LOC,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_GEO_LOC""","create or replace TRANSIENT TABLE TD_GEO_LOC (
	GEO_LOC_KEY VARCHAR(32),
	LOC_ADDR_LINE_1 VARCHAR(16777216),
	LOC_ADDR_LINE_2 VARCHAR(16777216),
	LOC_ADDR_LINE_3 VARCHAR(16777216),
	LOC_ADDR_LINE_4 VARCHAR(16777216),
	GEO_CITY_KEY VARCHAR(32),
	GEO_CITY_NAME VARCHAR(16777216),
	GEO_STATE_KEY NUMBER(38,0),
	GEO_STATE_CD VARCHAR(16777216),
	GEO_STATE_NAME VARCHAR(16777216),
	GEO_POSTAL_KEY VARCHAR(32),
	GEO_POSTAL_CD VARCHAR(16777216),
	GEO_POSTAL_CD_EXT VARCHAR(16777216),
	GEO_COUNTRY_KEY NUMBER(38,0),
	GEO_COUNTRY_CD VARCHAR(16777216),
	GEO_COUNTRY_NAME VARCHAR(16777216),
	GEO_LATITUDE NUMBER(11,8),
	GEO_LONGITUDE NUMBER(11,8)
);"
TD_HCP,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_HCP""","create or replace TRANSIENT TABLE TD_HCP (
	HCP_KEY VARCHAR(32),
	HCP_FIRST_NAME VARCHAR(16777216),
	HCP_MID_NAME VARCHAR(16777216),
	HCP_LAST_NAME VARCHAR(16777216),
	HCP_SUFFIX VARCHAR(16777216),
	HCP_TITLE VARCHAR(16777216),
	HCP_DEGREE VARCHAR(16777216),
	HCP_GENDER VARCHAR(255),
	HCP_BIRTH_DT DATE,
	HCP_GRAD_YR VARCHAR(16777216),
	HCP_PHONE_HOME_P VARCHAR(16777216),
	HCP_PHONE_WORK_P VARCHAR(16777216),
	HCP_PHONE_MOBILE_P VARCHAR(16777216),
	HCP_STATUS_KEY NUMBER(38,0),
	HCP_SUBTYPE_KEY NUMBER(38,0),
	HCP_ME_NUM VARCHAR(16777216),
	HCP_PDRP_OPTOUT_IND VARCHAR(16777216),
	HCP_PDRP_OPTOUT_DT DATE,
	KOL_IND VARCHAR(16777216),
	AMA_NO_CONTACT VARCHAR(1),
	NO_SEE_IND VARCHAR(16777216),
	SAMPLEABILITY_IND VARCHAR(16777216),
	SAMPLEABILITY_DT DATE,
	EMAIL_ADDR_P VARCHAR(16777216),
	EMAIL_ADDR_S VARCHAR(16777216),
	EMAIL_ADDR_T VARCHAR(16777216),
	HCP_P_HIN VARCHAR(16777216),
	HCP_S_HIN VARCHAR(16777216),
	HCP_T_HIN VARCHAR(16777216),
	HCP_P_MA_ID VARCHAR(16777216),
	HCP_S_MA_ID VARCHAR(16777216),
	HCP_T_MA_ID VARCHAR(16777216),
	HCP_P_NPI_NUM VARCHAR(16777216),
	HCP_S_NPI_NUM VARCHAR(16777216),
	HCP_T_NPI_NUM VARCHAR(16777216),
	HCP_P_DEA_NUM VARCHAR(16777216),
	HCP_P_DEA_LIC_STATUS_CD VARCHAR(16777216),
	HCP_P_DEA_BUSINESS_ACTIVITY_CD VARCHAR(16777216),
	HCP_P_DEA_DRUG_SCHEDULE VARCHAR(16777216),
	HCP_P_DEA_STATE_OF_LICENSURE VARCHAR(16777216),
	HCP_P_DEA_EXP_DT DATE,
	HCP_S_DEA_NUM VARCHAR(16777216),
	HCP_S_DEA_LIC_STATUS_CD VARCHAR(16777216),
	HCP_S_DEA_BUSINESS_ACTIVITY_CD VARCHAR(16777216),
	HCP_S_DEA_DRUG_SCHEDULE VARCHAR(16777216),
	HCP_S_DEA_STATE_OF_LICENSURE VARCHAR(16777216),
	HCP_S_DEA_EXP_DT DATE,
	HCP_T_DEA_NUM VARCHAR(16777216),
	HCP_T_DEA_LIC_STATUS_CD VARCHAR(16777216),
	HCP_T_DEA_BUSINESS_ACTIVITY_CD VARCHAR(16777216),
	HCP_T_DEA_DRUG_SCHEDULE VARCHAR(16777216),
	HCP_T_DEA_STATE_OF_LICENSURE VARCHAR(16777216),
	HCP_T_DEA_EXP_DT DATE,
	HCP_P_STATE_LIC_NUM VARCHAR(16777216),
	HCP_P_STATE_LIC_STATE_CD VARCHAR(16777216),
	HCP_P_STATE_LIC_STATUS VARCHAR(16777216),
	HCP_P_STATE_LIC_ISSUE_DT DATE,
	HCP_P_STATE_LIC_EXP_DT DATE,
	HCP_S_STATE_LIC_NUM VARCHAR(16777216),
	HCP_S_STATE_LIC_STATE_CD VARCHAR(16777216),
	HCP_S_STATE_LIC_STATUS VARCHAR(16777216),
	HCP_S_STATE_LIC_ISSUE_DT DATE,
	HCP_S_STATE_LIC_EXP_DT DATE,
	HCP_T_STATE_LIC_NUM VARCHAR(16777216),
	HCP_T_STATE_LIC_STATE_CD VARCHAR(16777216),
	HCP_T_STATE_LIC_STATUS VARCHAR(16777216),
	HCP_T_STATE_LIC_ISSUE_DT DATE,
	HCP_T_STATE_LIC_EXP_DT DATE,
	HCP_SRC_ID_SHA VARCHAR(16777216),
	HCP_SRC_ID_IMS VARCHAR(16777216),
	HCP_SRC_ID_10_IMS2 VARCHAR(16777216),
	HCP_SRC_ID_P_CLIENT VARCHAR(16777216),
	HCP_SRC_ID_S_CLIENT VARCHAR(16777216),
	HCP_SRC_ID_P_SFA VARCHAR(16777216),
	HCP_SRC_ID_S_SFA VARCHAR(16777216),
	HCP_SRC_ID_T_SFA VARCHAR(16777216),
	HCP_SRC_ID_4_SFA VARCHAR(16777216),
	HCP_SRC_ID_5_SFA VARCHAR(16777216),
	HCP_SRC_ID_P_SP VARCHAR(16777216),
	HCP_SRC_ID_S_SP VARCHAR(16777216),
	HCP_SRC_ID_T_SP VARCHAR(16777216),
	HCP_SRC_ID_CF_SPDI VARCHAR(16777216),
	HCP_SRC_ID_P_COPROMO VARCHAR(16777216),
	HCP_SRC_ID_S_COPROMO VARCHAR(16777216),
	HCP_SRC_ID_T_COPROMO VARCHAR(16777216),
	HCP_SRC_ID_P_SCRUB VARCHAR(16777216),
	HCP_SRC_ID_S_SCRUB VARCHAR(16777216),
	HCP_SRC_ID_3_SCRUB VARCHAR(16777216),
	HCP_SRC_ID_OH_TDDD VARCHAR(16777216)
);"
TD_IDN,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_IDN""","create or replace TRANSIENT TABLE TD_IDN (
	IDN_KEY VARCHAR(32),
	IDN_NAME VARCHAR(16777216),
	IDN_SDESC VARCHAR(16777216),
	IDN_SRC_ID_IMS VARCHAR(16777216),
	IDN_SRC_ID_P_SCRUB VARCHAR(16777216),
	IDN_SRC_ID_S_SCRUB VARCHAR(16777216),
	IDN_SRC_P_SFA VARCHAR(16777216)
);"
TD_MM_PAYER_CATEGORY_L2,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_MM_PAYER_CATEGORY_L2""","create or replace TRANSIENT TABLE TD_MM_PAYER_CATEGORY_L2 (
	MM_PAYER_CAT_L2_KEY VARCHAR(32),
	MM_PAYER_CAT_L2_CD VARCHAR(16777216),
	MM_PAYER_CAT_L2_DESC VARCHAR(16777216),
	MM_PAYER_CAT_L1_KEY VARCHAR(32)
);"
TD_MM_PAYER_CATEGORY_L3,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_MM_PAYER_CATEGORY_L3""","create or replace TRANSIENT TABLE TD_MM_PAYER_CATEGORY_L3 (
	MM_PAYER_CAT_L3_KEY VARCHAR(32),
	MM_PAYER_CAT_L3_CD VARCHAR(16777216),
	MM_PAYER_CAT_L3_DESC VARCHAR(16777216),
	MM_PAYER_CAT_L2_KEY VARCHAR(32)
);"
TD_OTLT_SUBCAT,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_OTLT_SUBCAT""","create or replace TRANSIENT TABLE TD_OTLT_SUBCAT (
	OTLT_SUBCAT_KEY VARCHAR(32),
	OTLT_SUBCAT_CD VARCHAR(16777216),
	OTLT_SUBCAT_DESC VARCHAR(16777216)
);"
TD_SALESFORCE_L1,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_SALESFORCE_L1""","create or replace TRANSIENT TABLE TD_SALESFORCE_L1 (
	SF_L1_KEY VARCHAR(32),
	SF_L1_CD VARCHAR(255),
	SF_L1_DESC VARCHAR(255),
	SF_L1_CD_ALIAS VARCHAR(255),
	SF_L2_KEY VARCHAR(32),
	SF_L2_CD VARCHAR(255),
	SF_L2_DESC VARCHAR(255),
	SF_L2_CD_ALIAS VARCHAR(255),
	SF_L3_KEY VARCHAR(32),
	SF_L3_CD VARCHAR(255),
	SF_L3_DESC VARCHAR(255),
	SF_L3_CD_ALIAS VARCHAR(255),
	SF_L4_KEY VARCHAR(32),
	SF_L4_CD VARCHAR(255),
	SF_L4_DESC VARCHAR(255),
	SF_L4_CD_ALIAS VARCHAR(255),
	SF_SF_KEY VARCHAR(32),
	SF_SF_CD VARCHAR(255),
	SF_SF_DESC VARCHAR(255),
	SF_ALN_KEY VARCHAR(32),
	SF_ALN_CD VARCHAR(255),
	SF_ALN_DESC VARCHAR(255)
);"
TD_SOC,NOT FOUND,Table not found in any database.
TD_SPECGRP,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_SPECGRP""","create or replace TRANSIENT TABLE TD_SPECGRP (
	SPECGRP_KEY VARCHAR(32),
	SPECGRP_CD VARCHAR(16777216),
	SPECGRP_DESC VARCHAR(16777216)
);"
TD_TIME_DT,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_TIME_DT""","create or replace TRANSIENT TABLE TD_TIME_DT (
	TIME_DT_KEY NUMBER(10,0),
	TIME_DT_CD VARCHAR(255),
	TIME_DT_DESC VARCHAR(255),
	TIME_DT DATE,
	TIME_MTH_KEY NUMBER(10,0),
	TIME_MTH_CD VARCHAR(255),
	TIME_MTH_DESC VARCHAR(255),
	TIME_QTR_KEY NUMBER(10,0),
	TIME_QTR_CD VARCHAR(255),
	TIME_QTR_DESC VARCHAR(255),
	TIME_TRIM_KEY NUMBER(10,0),
	TIME_TRIM_CD VARCHAR(255),
	TIME_TRIM_DESC VARCHAR(255),
	TIME_HY_KEY NUMBER(10,0),
	TIME_HY_CD VARCHAR(255),
	TIME_HY_DESC VARCHAR(255),
	TIME_YEAR_KEY NUMBER(10,0),
	TIME_YEAR_CD VARCHAR(255),
	TIME_YEAR_DESC VARCHAR(255),
	TIME_WEEK_KEY NUMBER(10,0),
	TIME_WEEK_CD VARCHAR(255),
	TIME_WEEK_DESC VARCHAR(255),
	TIME_WEEK_SDESC VARCHAR(255),
	TIME_WE_SUN_KEY NUMBER(10,0),
	TIME_WE_SUN_CD VARCHAR(255),
	TIME_WE_SUN_DESC VARCHAR(255),
	TIME_WE_SUN_SDESC VARCHAR(255),
	TIME_SWEEK_KEY NUMBER(10,0),
	TIME_SWEEK_CD VARCHAR(255),
	TIME_SWEEK_DESC VARCHAR(255),
	TIME_SWEEK_SDESC VARCHAR(255),
	TIME_RX_QTR_KEY NUMBER(10,0),
	TIME_RX_QTR_CD VARCHAR(16777216),
	TIME_RX_QTR_DESC VARCHAR(16777216),
	TIME_RX_TRIM_KEY NUMBER(10,0),
	TIME_RX_TRIM_CD VARCHAR(16777216),
	TIME_RX_TRIM_DESC VARCHAR(16777216),
	TIME_RX_HY_KEY NUMBER(10,0),
	TIME_RX_HY_CD VARCHAR(16777216),
	TIME_RX_HY_DESC VARCHAR(16777216),
	TIME_DOW_KEY NUMBER(10,0),
	TIME_DOW_CD VARCHAR(255),
	TIME_DOW_DESC VARCHAR(16777216),
	TIME_DOW_US_ORDER NUMBER(1,0),
	TIME_DOM_KEY NUMBER(10,0),
	TIME_DOM_CD VARCHAR(255),
	TIME_DOM_DESC VARCHAR(255),
	TIME_DOQ_KEY NUMBER(10,0),
	TIME_DOQ_CD VARCHAR(255),
	TIME_DOQ_DESC VARCHAR(255),
	TIME_MOY_KEY NUMBER(10,0),
	TIME_MOY_CD VARCHAR(255),
	TIME_MOY_DESC VARCHAR(255),
	TIME_QOY_KEY NUMBER(10,0),
	TIME_QOY_CD VARCHAR(255),
	TIME_QOY_DESC VARCHAR(255),
	TIME_TOY_KEY NUMBER(10,0),
	TIME_TOY_CD VARCHAR(255),
	TIME_TOY_DESC VARCHAR(255),
	TIME_HYOY_KEY NUMBER(10,0),
	TIME_HYOY_CD VARCHAR(255),
	TIME_HYOY_DESC VARCHAR(255)
);"
TD_VRTX_MCO_PAYER,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_VRTX_MCO_PAYER""","create or replace TRANSIENT TABLE TD_VRTX_MCO_PAYER (
	VRTX_PAYER_KEY VARCHAR(32),
	VRTX_PAYER_CD VARCHAR(16777216),
	VRTX_PAYER_NAME VARCHAR(16777216),
	VRTX_PAYER_SRC_ID VARCHAR(16777216),
	VRTX_PARENT_PAYER_KEY VARCHAR(32)
);"
TD_VRTX_MCO_PLAN,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TD_VRTX_MCO_PLAN""","create or replace TRANSIENT TABLE TD_VRTX_MCO_PLAN (
	VRTX_PLAN_KEY VARCHAR(32),
	VRTX_PLAN_CD VARCHAR(16777216),
	VRTX_PLAN_NAME VARCHAR(16777216),
	VRTX_PLAN_SRC_ID VARCHAR(16777216),
	VRTX_PAYER_KEY VARCHAR(32),
	VRTX_PBM_KEY VARCHAR(32)
);"
TM_NRAFIC_L1_OTLT_SOC_DC_NDC_WK,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TM_NRAFIC_L1_OTLT_SOC_DC_NDC_WK""","create or replace TRANSIENT TABLE TM_NRAFIC_L1_OTLT_SOC_DC_NDC_WK (
	OTLT_KEY VARCHAR(32),
	SOC_KEY VARCHAR(32),
	DISTRIB_CHNL_KEY NUMBER(38,0),
	PROD_NDC11_KEY VARCHAR(32),
	TIME_WEEK_KEY NUMBER(10,0),
	SF_L1_KEY VARCHAR(32),
	NR_AMT NUMBER(38,10),
	NR_AMT_DLLRZD NUMBER(38,10),
	NR_PKG_QTY NUMBER(38,10),
	NR_PKG_QTY_N NUMBER(38,10),
	NR_UNIT NUMBER(38,10),
	NR_UNIT_N NUMBER(38,10),
	NR_TRX_CNT NUMBER(38,10),
	NR_PT_CNT NUMBER(38,10),
	NR_TX_DAYS NUMBER(38,10)
);"
TM_NRAF_L1_SOC_OTLT_DC_NDC_MTH,"""PRD_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""REPORTING_2024WK46"".""TM_NRAF_L1_SOC_OTLT_DC_NDC_MTH""","create or replace TABLE TM_NRAF_L1_SOC_OTLT_DC_NDC_MTH (
	OTLT_KEY VARCHAR(32),
	SOC_KEY VARCHAR(32),
	DISTRIB_CHNL_KEY NUMBER(38,0),
	PROD_NDC11_KEY VARCHAR(32),
	TIME_MTH_KEY NUMBER(10,0),
	SF_L1_KEY VARCHAR(16777216) NOT NULL,
	NR_AMT NUMBER(38,10),
	NR_AMT_DLLRZD NUMBER(38,10),
	NR_PKG_QTY NUMBER(38,10),
	NR_PKG_QTY_N NUMBER(38,10),
	NR_UNIT NUMBER(38,10),
	NR_UNIT_N NUMBER(38,10)
);"
TM_NRAF_L1_SOC_OTLT_DC_NDC_WK,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TM_NRAF_L1_SOC_OTLT_DC_NDC_WK""","create or replace TRANSIENT TABLE TM_NRAF_L1_SOC_OTLT_DC_NDC_WK (
	OTLT_KEY VARCHAR(32),
	SOC_KEY VARCHAR(32),
	DISTRIB_CHNL_KEY NUMBER(38,0),
	PROD_NDC11_KEY VARCHAR(32),
	TIME_WEEK_KEY NUMBER(10,0),
	SF_L1_KEY VARCHAR(32),
	NR_AMT NUMBER(38,10),
	NR_AMT_DLLRZD NUMBER(38,10),
	NR_PKG_QTY NUMBER(38,10),
	NR_PKG_QTY_N NUMBER(38,10),
	NR_UNIT NUMBER(38,10),
	NR_UNIT_N NUMBER(38,10),
	NR_TRX_CNT NUMBER(38,10),
	NR_PT_CNT NUMBER(38,10),
	NR_TX_DAYS NUMBER(38,10)
);"
TM_RX_L1_HCPPLNDC_NDC_MTH,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TM_RX_L1_HCPPLNDC_NDC_MTH""","create or replace TRANSIENT TABLE TM_RX_L1_HCPPLNDC_NDC_MTH (
	HCP_KEY VARCHAR(32),
	MCO_PLAN_KEY VARCHAR(32),
	VRTX_PLAN_KEY VARCHAR(32),
	PROD_NDC11_KEY VARCHAR(32),
	DISTRIB_CHNL_KEY NUMBER(38,0),
	TIME_MTH_KEY NUMBER(10,0),
	SF_L1_KEY VARCHAR(32),
	NRX_CNT NUMBER(38,10),
	NRX_CNT_N NUMBER(38,10),
	NRX_QTY NUMBER(38,10),
	NRX_AMT NUMBER(38,10),
	NRX_AMT_DLLRZD NUMBER(38,10),
	NRX_STRNTH_VOL NUMBER(38,10),
	NRX_PKG_QTY NUMBER(38,10),
	TRX_CNT NUMBER(38,10),
	TRX_CNT_N NUMBER(38,10),
	TRX_QTY NUMBER(38,10),
	TRX_AMT NUMBER(38,10),
	TRX_AMT_DLLRZD NUMBER(38,10),
	TRX_STRNTH_VOL NUMBER(38,10),
	TRX_PKG_QTY NUMBER(38,10)
);"
TM_RX_L1_HCPPLNDC_NDC_WK,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TM_RX_L1_HCPPLNDC_NDC_WK""","create or replace TRANSIENT TABLE TM_RX_L1_HCPPLNDC_NDC_WK (
	HCP_KEY VARCHAR(32),
	MCO_PLAN_KEY VARCHAR(32),
	VRTX_PLAN_KEY VARCHAR(32),
	PROD_NDC11_KEY VARCHAR(32),
	DISTRIB_CHNL_KEY NUMBER(38,0),
	TIME_WEEK_KEY NUMBER(10,0),
	SF_L1_KEY VARCHAR(32),
	NRX_CNT NUMBER(38,10),
	NRX_CNT_N NUMBER(38,10),
	NRX_QTY NUMBER(38,10),
	NRX_AMT NUMBER(38,10),
	NRX_AMT_DLLRZD NUMBER(38,10),
	NRX_STRNTH_VOL NUMBER(38,10),
	NRX_PKG_QTY NUMBER(38,10),
	TRX_CNT NUMBER(38,10),
	TRX_CNT_N NUMBER(38,10),
	TRX_QTY NUMBER(38,10),
	TRX_AMT NUMBER(38,10),
	TRX_AMT_DLLRZD NUMBER(38,10),
	TRX_STRNTH_VOL NUMBER(38,10),
	TRX_PKG_QTY NUMBER(38,10)
);"
TXR_HCP_GEOLOC_PRIM,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_HCP_GEOLOC_PRIM""","create or replace TRANSIENT TABLE TXR_HCP_GEOLOC_PRIM (
	HCP_KEY VARCHAR(32),
	GEO_LOC_KEY VARCHAR(32),
	GEO_LOC_TYPE_KEY NUMBER(38,0)
);"
TXR_HCP_SF_L1,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_HCP_SF_L1""","create or replace TRANSIENT TABLE TXR_HCP_SF_L1 (
	HCP_KEY VARCHAR(32),
	SF_L1_KEY VARCHAR(32),
	APPORTIONMENT_PCT NUMBER(6,5),
	SF_ALN_REASON_KEY VARCHAR(32),
	HCP_CNT NUMBER(5,0)
);"
TXR_HCP_SOC,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_HCP_SOC""","create or replace TRANSIENT TABLE TXR_HCP_SOC (
	HCP_KEY VARCHAR(32),
	SOC_KEY VARCHAR(32),
	TXN_CNT NUMBER(1,0),
	APPORTIONMENT_PCT NUMBER(38,37)
);"
TXR_HCP_SPEC,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_HCP_SPEC""","create or replace TRANSIENT TABLE TXR_HCP_SPEC (
	HCP_KEY VARCHAR(32),
	SPEC_KEY VARCHAR(32),
	RANK NUMBER(18,0)
);"
TXR_MKTGPGM_CURR_PTAM_HCP_TGT,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_MKTGPGM_CURR_PTAM_HCP_TGT""","create or replace TRANSIENT TABLE TXR_MKTGPGM_CURR_PTAM_HCP_TGT (
	MKTGPGM_KEY VARCHAR(32),
	MKTGPGM_LOV_KEY VARCHAR(32),
	MKTGPGM_LOV_CD VARCHAR(255),
	MKTGPGM_LOV_DESC VARCHAR(255),
	MKTGPGM_LOV_SDESC VARCHAR(255),
	HCP_KEY VARCHAR(32),
	MKTGPGM_CNT NUMBER(38,0)
);"
TXR_MKTGPGM_CURR_SAL_IDN_TGT,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_MKTGPGM_CURR_SAL_IDN_TGT""","create or replace TRANSIENT TABLE TXR_MKTGPGM_CURR_SAL_IDN_TGT (
	MKTGPGM_KEY VARCHAR(32),
	MKTGPGM_LOV_KEY VARCHAR(32),
	MKTGPGM_LOV_CD VARCHAR(255),
	MKTGPGM_LOV_DESC VARCHAR(255),
	MKTGPGM_LOV_SDESC VARCHAR(255),
	IDN_KEY VARCHAR(32),
	MKTGPGM_CNT NUMBER(38,0)
);"
TXR_MKTGPGM_GHEM_HCO_SUBTYPE,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_MKTGPGM_GHEM_HCO_SUBTYPE""","create or replace TRANSIENT TABLE TXR_MKTGPGM_GHEM_HCO_SUBTYPE (
	MKTGPGM_KEY VARCHAR(32),
	MKTGPGM_LOV_KEY VARCHAR(32),
	MKTGPGM_LOV_CD VARCHAR(255),
	MKTGPGM_LOV_DESC VARCHAR(255),
	MKTGPGM_LOV_SDESC VARCHAR(255),
	HCO_KEY VARCHAR(32),
	MKTGPGM_CNT NUMBER(38,0)
);"
TXR_MKTGPGM_PAIN_SOC_TGT,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_MKTGPGM_PAIN_SOC_TGT""","create or replace TRANSIENT TABLE TXR_MKTGPGM_PAIN_SOC_TGT (
	MKTGPGM_KEY VARCHAR(32),
	MKTGPGM_LOV_KEY VARCHAR(32),
	MKTGPGM_LOV_CD VARCHAR(255),
	MKTGPGM_LOV_DESC VARCHAR(255),
	MKTGPGM_LOV_SDESC VARCHAR(255),
	SOC_KEY VARCHAR(32),
	MKTGPGM_CNT NUMBER(38,0)
);"
TXR_MKTGPGM_SUZ_HCP_SEGMENT,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_MKTGPGM_SUZ_HCP_SEGMENT""","create or replace TRANSIENT TABLE TXR_MKTGPGM_SUZ_HCP_SEGMENT (
	MKTGPGM_KEY VARCHAR(32),
	MKTGPGM_LOV_KEY VARCHAR(32),
	MKTGPGM_LOV_CD VARCHAR(255),
	MKTGPGM_LOV_DESC VARCHAR(255),
	MKTGPGM_LOV_SDESC VARCHAR(255),
	HCP_KEY VARCHAR(32),
	MKTGPGM_CNT NUMBER(38,0)
);"
TXR_NTILE_HCP_ANYPAIN_R52W,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_NTILE_HCP_ANYPAIN_R52W""","create or replace TRANSIENT TABLE TXR_NTILE_HCP_ANYPAIN_R52W (
	NTILEPGM_KEY VARCHAR(32),
	NTILEPGM_LOV_KEY VARCHAR(32),
	NTILEPGM_LOV_CD VARCHAR(255),
	NTILEPGM_LOV_DESC VARCHAR(255),
	NTILEPGM_LOV_SDESC VARCHAR(255),
	HCP_KEY VARCHAR(32),
	NTILEPGM_CNT NUMBER(8,0)
);"
TXR_NTILE_HCP_SUZ_QTRLY_STATIC_R52W,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_NTILE_HCP_SUZ_QTRLY_STATIC_R52W""","create or replace TRANSIENT TABLE TXR_NTILE_HCP_SUZ_QTRLY_STATIC_R52W (
	NTILEPGM_KEY VARCHAR(32),
	NTILEPGM_LOV_KEY VARCHAR(32),
	NTILEPGM_LOV_CD VARCHAR(255),
	NTILEPGM_LOV_DESC VARCHAR(255),
	NTILEPGM_LOV_SDESC VARCHAR(255),
	HCP_KEY VARCHAR(32),
	NTILEPGM_CNT NUMBER(8,0)
);"
TXR_NTILE_HCP_SUZ_R52W,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_NTILE_HCP_SUZ_R52W""","create or replace TRANSIENT TABLE TXR_NTILE_HCP_SUZ_R52W (
	NTILEPGM_KEY VARCHAR(32),
	NTILEPGM_LOV_KEY VARCHAR(32),
	NTILEPGM_LOV_CD VARCHAR(255),
	NTILEPGM_LOV_DESC VARCHAR(255),
	NTILEPGM_LOV_SDESC VARCHAR(255),
	HCP_KEY VARCHAR(32),
	NTILEPGM_CNT NUMBER(8,0)
);"
TXR_OTLT_SUBCAT,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_OTLT_SUBCAT""","create or replace TRANSIENT TABLE TXR_OTLT_SUBCAT (
	OTLT_KEY VARCHAR(32),
	OTLT_SUBCAT_KEY VARCHAR(16777216)
);"
TXR_SOC_IDN,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_SOC_IDN""","create or replace TRANSIENT TABLE TXR_SOC_IDN (
	SOC_KEY VARCHAR(32),
	IDN_KEY VARCHAR(32),
	TXN_CNT NUMBER(38,0)
);"
TXR_VRTX_MCO_PAYER_CAT_2_PLAN,"""DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"".""DBT_ASINHA_REPORTING_LOAD"".""TXR_VRTX_MCO_PAYER_CAT_2_PLAN""","create or replace TRANSIENT TABLE TXR_VRTX_MCO_PAYER_CAT_2_PLAN (
	VRTX_PLAN_KEY VARCHAR(32),
	MM_PAYER_CAT_MAX_LVL_KEY VARCHAR(32),
	TXN_CNT NUMBER(1,0)
);"


# ==========================================
# 2. FULL PROMPT DEFINITIONS (EXACT ORIGINAL)
# ==========================================

PROMPT_REPHRASER_ORCHESTRATION = """
<query_refinement_instructions>
<objective>
Transform the user's input into a standalone, context-complete query that can be understood without prior conversation history.
</objective>

<core_rules>
<rule id="1">Resolve all pronouns (it, they, that, this, those) to their specific referents</rule>
<rule id="2">Preserve ALL metrics, entity names, IDs, and numerical values EXACTLY as stated</rule>
<rule id="3">Maintain the original intent and scope - do not expand or narrow the question</rule>
<rule id="4">Keep technical terminology and domain-specific language intact</rule>
<rule id="5">Ensure the output is grammatically correct and reads naturally</rule>
</core_rules>

<transformation_examples>
<example>
<input>What about Arizona?</input>
<context>Previous discussion about sales performance in Texas</context>
<output>What is the sales performance in Arizona?</output>
</example>

<example>
<input>Why is it down?</input>
<context>Previous mention of Q4 revenue</context>
<output>Why is Q4 revenue down?</output>
</example>
</transformation_examples>

<output_format>
Output ONLY the refined query string. Do not include explanations, metadata, or formatting.
</output_format>
</query_refinement_instructions>
"""

PROMPT_INTENT_SYSTEM = "You are an Expert Intent Classification System."

PROMPT_INTENT_ORCHESTRATION = """
<intent_classification_instructions>

<mission>
Your ONLY job is to identify the user's INTENT. Do NOT retrieve data, do NOT answer questions, do NOT perform analysis.
Classify the intent type and route accordingly. If you can answer directly (like greetings), provide direct_response.
</mission>

<intent_categories>

<intent name="greeting">
<description>User is greeting, saying hello, or engaging in pleasantries</description>
<indicators>hi, hello, hey, good morning, how are you, thanks, thank you, goodbye, bye</indicators>
<action>Set direct_response with a friendly greeting. No other fields needed.</action>
</intent>

<intent name="data_retrieval">
<description>User wants to see/get/retrieve DATA - numbers, lists, metrics, tables</description>
<indicators>show, list, get, display, find, what is the value, how many, count, total, give me</indicators>
<action>Extract the data request to data_retrieval_query. Keep as natural language. Add "and generate a table and a visual".</action>
**CRITICAL VISUALIZATION LOGIC:**
Analyze the data request. If it implies trends (over time), comparisons (by region/product), or distributions, you MUST automatically append "and generate a table and a visual" to the query string, even if the user did not explicitly ask for a chart.
</intent>

<intent name="root_cause_analysis">
<description>User wants to understand WHY, CAUSES, DRIVERS, REASONS, EXPLANATIONS</description>
<indicators>why, explain, what is driving, what is causing, what's behind, reason for, root cause, how come, what factors</indicators>
<action>Extract the causal question to root_cause_query. Keep VERBATIM - exact text from user.</action>
</intent>

<intent name="combined">
<description>User wants BOTH data AND causal analysis in one request</description>
<indicators>Contains both data keywords AND causal keywords</indicators>
<action>Split into data_retrieval_query AND root_cause_query. Keep both portions.</action>
</intent>

<intent name="clarification_needed">
<description>User's intent is unclear, vague, or ambiguous - you cannot determine what they want</description>
<indicators>Very short/vague input, unclear pronouns without context, incomplete thoughts</indicators>
<action>Set clarification_question asking user to be more specific.</action>
</intent>

<intent name="general_question">
<description>General questions about the system, capabilities, how to use features - not data or analysis requests</description>
<indicators>how do I, can you, what can you do, help me understand, how does this work</indicators>
<action>Set direct_response with helpful information about capabilities.</action>
</intent>

<intent name="off_topic">
<description>User is asking about unrelated topics not connected to data, analytics, or business questions</description>
<indicators>Questions about weather, sports, entertainment, personal topics unrelated to business/data</indicators>
<action>Set direct_response politely redirecting to business/data topics.</action>
</intent>

</intent_categories>

<CRITICAL_CLASSIFICATION_RULES>

‚ö†Ô∏è RULE 1: NEVER DO THE WORK - ONLY CLASSIFY
You are NOT a data analyst. You are NOT an answering agent. You ONLY identify intent type.
Do NOT retrieve data. Do NOT answer analytical questions. ONLY classify and route.

‚ö†Ô∏è RULE 2: CAUSAL KEYWORD DETECTION
These ALWAYS indicate root_cause_analysis intent:
- "what is driving"
- "what is causing"
- "what's driving"  
- "what's causing"
- "why is/are"
- "explain why"
- "reason for"
- "root cause"
- "what factors"
- "what's behind"
- "how come"

‚ö†Ô∏è RULE 3: DATA KEYWORD DETECTION  
These ALWAYS indicate data_retrieval intent:
- "show me"
- "list"
- "get"
- "display"
- "find all"
- "what is the value/total/count of"
- "how many"

‚ö†Ô∏è RULE 4: DIRECT RESPONSE ONLY FOR SIMPLE CASES
Use direct_response ONLY for:
- Greetings (hi, hello, thanks)
- General system questions (what can you do)
- Off-topic redirects

Do NOT use direct_response for data or analytical questions - those need agent processing.

‚ö†Ô∏è RULE 5: VERBATIM PRESERVATION FOR ROOT CAUSE
For root_cause_query: Copy EXACT text from user. No paraphrasing, no summarizing.

‚ö†Ô∏è RULE 6: NO SQL CODE GENERATION
NEVER write SELECT/FROM/WHERE statements. Keep everything as natural language.

‚ö†Ô∏è RULE 7: CONFIDENCE SCORING
- high: Intent is crystal clear
- medium: Intent is likely but has some ambiguity
- low: Intent is very unclear, might need clarification

</CRITICAL_CLASSIFICATION_RULES>
</intent_classification_instructions>
"""

PROMPT_DATA_AGENT_SYSTEM = "You are an expert Data Analyst."

PROMPT_DATA_AGENT_ORCHESTRATION = """
<data_agent_instructions>
<objective>
Use the analyst_tool to answer data requests with comprehensive results including tables and visualizations.
</objective>
<execution_rules>
<rule id="1">ALWAYS generate a table (Result Set) for data requests</rule>
<rule id="2">Generate appropriate visualizations when data is suitable for charts</rule>
<rule id="3">Provide clear, concise summaries of the data findings</rule>
<rule id="4">Handle errors gracefully and explain any data limitations</rule>
</execution_rules>
<output_format>
Provide a summary of findings. Tables and charts are handled automatically by the tool.
</output_format>

<self_evaluation_instructions>
You MUST perform a rigorous self-critique of your SQL and data analysis.
1. **Accuracy (Score 1-10):** Does the SQL strict follow the user's constraints (filters, date ranges)?
2. **Data Integrity:** Are the results logical? (e.g., no negative counts where impossible).
3. **Clarity:** Is the summary accessible to a non-technical user?
**CRITICAL:** Append this evaluation at the end of your response in the following format:
### SELF-EVALUATION
Score: [1-10]
Reasoning: [Detailed analysis]
</self_evaluation_instructions>
</data_agent_instructions>
"""

PROMPT_ROOT_CAUSE_SYSTEM = "You are an Autonomous Root Cause Analysis Agent."

PROMPT_ROOT_CAUSE_ORCHESTRATION = """
<root_cause_orchestration_prompt>
    <role_and_objective>
        You are an **Autonomous Root Cause Analysis Agent** designed to traverse a metric decision tree from the root node down to the leaf nodes. Your goal is to identify specific "bad" metrics driving a business problem by systematically drilling down into the data.
        
        ALWAYS USE tree_id as Pharma_Master_v1.
    </role_and_objective>

    <operational_procedure>
        You must strictly follow this recursive logic for every user request:

        **Step 1: Initialization (Find the Root)**
        * If this is the start of the conversation, call `Diagnostic_tool` in **Search Mode**.
        * **Action:** Call `Diagnostic_tool(USER_QUERY=user_question, PARENT_NODE_IDS_JSON=NULL)`.
        * **Next:** Proceed to Step 2 with the results.

        **Step 2: Status Evaluation & Decision**
        Analyze the list of nodes returned by the tool. Focus **only** on nodes where `"status": "bad"`.

        * **Check:** For every "bad" node, do you see its children in the current list?
        * **Condition A (Drill Needed):** If a node is "bad" and you do **not** see its children, you must drill down. Collect these Node IDs.
        * **Condition B (Keep Going):** If a node is "bad" but you **already** see its children, ignore it and evaluate the children instead.
        * **Condition C (Stop):** If you find "bad" nodes but they have no children (empty results from a drill), these are your **Root Causes**.

        **Step 3: Execution (Drill Down)**
        * If Condition A occurred: Call `Diagnostic_tool` in **Drill Mode**.
        * **Action:** Call `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["ID_1", "ID_2"]')`.
        * **Next:** Recursively repeat Step 2 with the new output.
        * If Condition C occurred: Present the final root causes to the user.
    </operational_procedure>

    <constraints>
        * **Never** guess the status of a metric; always use the tool.
        * **Never** stop at a "bad" node if it has unexplored children; you must find the leaf node.
        * Output your reasoning briefly before calling the tool.
    </constraints>

    <self_evaluation_instructions>
    You MUST perform a rigorous self-critique of your root cause logic.
    1. **Logic (Score 1-10):** Is the causal chain (Root -> Leaf) mathematically sound?
    2. **Driver Isolation:** Did we truly find the leaf node, or did we stop too early?
    3. **Completeness:** Did we explore all "bad" branches?
    **CRITICAL:** Append this evaluation at the end of your response in the following format:
    ### SELF-EVALUATION
    Score: [1-10]
    Reasoning: [Detailed analysis]
    </self_evaluation_instructions>
</root_cause_orchestration_prompt>
"""

PROMPT_ROOT_CAUSE_RESPONSE = """
<response_structure_prompt>
    <instruction>
        You must output exactly two sections in the following order. Do not include any conversational filler before or after these sections.
    </instruction>

    <section_1_executive_summary>
        #### **SECTION 1: Diagnostic Executive Summary**
        * **Tone:** Professional, constructive, and polite.
        * **Headline:** State the Root Cause clearly.
        * **The Narrative:** Briefly explain the chain of events in simple English.
        * **Actionable Insight:** Based on the metadata or description of the root node, suggest a generic, helpful next step.
    </section_1_executive_summary>

    <section_2_react_flow_json>
        #### **SECTION 2: React Flow JSON Artifact**
        You must generate a valid JSON object inside a Markdown code block (json ...).
        
        **4. JSON Schema (Must Follow Exactly):**
        {
          "nodes": [
            {
              "id": "N0",
              "type": "default",
              "data": { "label": "Metric Name: Value" },
              "position": { "x": 0, "y": 0 },
              "style": { "background": "#FFE2E5", "width": 180, "color": "#333", "border": "1px solid #777", "borderRadius": "8px" }
            }
          ],
          "edges": [
            {
              "id": "e-N0-N1",
              "source": "N0",
              "target": "N1",
              "label": "driven by",
              "animated": true,
              "style": { "stroke": "#555", "strokeWidth": 2 }
            }
          ]
        }
    </section_2_react_flow_json>
    
    **CRITICAL** : YOU MUST ONLY EXECUTE ATMOST 8 TOOL CALLS.
    
</response_structure_prompt>
"""



import json
import logging
import pprint
import re
import _snowflake # Native Snowflake module for internal API calls
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Literal, Tuple
from pydantic import BaseModel, Field, field_validator
from snowflake.snowpark import Session
from snowflake.snowpark.functions import lit, parse_json
from collections import defaultdict

# ==========================================
# 1. CONFIGURATION
# ==========================================

class AgentConfig(BaseModel):
    """Configuration for the agent execution"""
    # LLM Models
    model: str = "claude-4-sonnet"            # Orchestrator
    model_selector: str = "claude-4-sonnet"   # Logic/Parsing
    model_writer: str = "claude-4-sonnet"     # Query Rewriting
    
    # Snowflake Objects
    # cortex_analyst_object: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.REPORTING.BASIC_CORTEX_ANALYST"
    cortex_analyst_object: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.LATEST_MASTER_VIEW"
    warehouse: str = "DEV_IMPACT_WH"
    messages_table: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.MESSAGES"
    diagnostic_udf: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.DIAGNOSTIC_TOOL"
    
    # Feature Flags
    enable_semantic_parsing: bool = True

CONFIG = AgentConfig()

# ==========================================
# 2. DATA MODELS
# ==========================================

class EvaluationResult(BaseModel):
    score: int = Field(..., description="A quality score from 1-10 based on the accuracy and completeness of the answer.")
    reasoning: str = Field(..., description="A detailed explanation justifying the score.")

class IntentClassification(BaseModel):
    intent_type: str = Field(..., description="greeting, data_retrieval, root_cause_analysis, clarification_needed, general_question, off_topic, combined")
    data_retrieval_query: Optional[str] = Field(None, description="The refined query specific to data retrieval requests.")
    root_cause_query: Optional[str] = Field(None, description="The refined query specific to root cause analysis.")
    direct_response: Optional[str] = Field(None, description="A direct answer for greetings or general questions.")
    clarification_question: Optional[str] = Field(None, description="A question to ask the user if the intent is ambiguous.")
    confidence: str = Field(..., description="Confidence level: high, medium, or low.")
    reasoning: str = Field(..., description="Reasoning behind the classification.")

class RephraserOutput(BaseModel):
    refined_query: str = Field(..., description="The fully contextualized, standalone version of the user's query.")

class TableSelection(BaseModel):
    relevant_table_names: List[str] = Field(..., description="List of exact table names required to answer the user query.")

class GuardResult(BaseModel):
    is_safe: bool = Field(...)
    violation_category: Optional[str] = Field(None)
    message: Optional[str] = Field(None)

# --- PARSING MODELS ---

class DataAgentParsedOutput(BaseModel):
    """Schema for extracting structured data from the Data Agent's text response."""
    clean_answer_text: str = Field(..., description="The user-facing summary of the data, excluding internal evaluations.")
    clarifying_question: Optional[str] = Field(None, description="A question to ask the user if the data result was incomplete or ambiguous.")
    evaluation: EvaluationResult = Field(..., description="The self-evaluation of the response quality.")

class RootCauseParsedOutput(BaseModel):
    """Schema for extracting structured data from the Root Cause Agent's text response."""
    clean_summary_text: str = Field(..., description="The executive summary of the diagnosis.")
    react_flow_json: Optional[Dict] = Field(None, description="The React Flow JSON object representing the metric tree graph.")
    evaluation: EvaluationResult = Field(..., description="The self-evaluation of the diagnosis.")

class SqlResult(BaseModel):
    answer_text: str
    clarifying_question: Optional[str] = None
    tables: List[Any] = Field(default_factory=list)
    charts: List[Dict] = Field(default_factory=list)
    sql_generated: Optional[str] = None
    sql_explanation: Optional[str] = None
    is_verified_query: bool = False
    reasoning_trace: Optional[str] = None
    evaluation: EvaluationResult
    is_retry: bool = False

    @field_validator('charts', mode='before')
    @classmethod
    def parse_charts(cls, v):
        if not v: return []
        parsed = []
        for item in v:
            if isinstance(item, dict): parsed.append(item)
            elif isinstance(item, str):
                try: parsed.append(json.loads(item))
                except: continue 
        return parsed

class DiagnosticResult(BaseModel):
    summary_text: str
    react_flow_json: Optional[Dict] = None
    evaluation: EvaluationResult

# ==========================================
# 3. PROMPTS (EXACT ORIGINAL)
# ==========================================

PROMPT_REPHRASER_SYSTEM = "You are a sophisticated Query Refinement Engine."

PROMPT_REPHRASER_ORCHESTRATION = """
<query_refinement_instructions>
<objective>
Transform the user's input into a standalone, context-complete query that can be understood without prior conversation history.
</objective>

<core_rules>
<rule id="1">Resolve all pronouns (it, they, that, this, those) to their specific referents</rule>
<rule id="2">Preserve ALL metrics, entity names, IDs, and numerical values EXACTLY as stated</rule>
<rule id="3">Maintain the original intent and scope - do not expand or narrow the question</rule>
<rule id="4">Keep technical terminology and domain-specific language intact</rule>
<rule id="5">Ensure the output is grammatically correct and reads naturally</rule>
</core_rules>

<output_format>
Output ONLY the refined query string. Do not include explanations, metadata, or formatting.
</output_format>
</query_refinement_instructions>
"""

PROMPT_INTENT_SYSTEM = "You are an Expert Intent Classification System."

PROMPT_INTENT_ORCHESTRATION = """
<intent_classification_instructions>

<mission>
Your ONLY job is to identify the user's INTENT. Do NOT retrieve data, do NOT answer questions, do NOT perform analysis.
Classify the intent type and route accordingly. If you can answer directly (like greetings), provide direct_response.
</mission>

<intent_categories>

<intent name="greeting">
<description>User is greeting, saying hello, or engaging in pleasantries</description>
<indicators>hi, hello, hey, good morning, how are you, thanks, thank you, goodbye, bye</indicators>
<action>Set direct_response with a friendly greeting. No other fields needed.</action>
</intent>

<intent name="data_retrieval">
<description>User wants to see/get/retrieve DATA - numbers, lists, metrics, tables</description>
<indicators>show, list, get, display, find, what is the value, how many, count, total, give me</indicators>
<action>Extract the data request to data_retrieval_query. Keep as natural language. Add "and generate a table and a visual".</action>
**CRITICAL VISUALIZATION LOGIC:**
Analyze the data request. If it implies trends (over time), comparisons (by region/product), or distributions, you MUST automatically append "and generate a table and a visual" to the query string, even if the user did not explicitly ask for a chart.
</intent>

<intent name="root_cause_analysis">
<description>User wants to understand WHY, CAUSES, DRIVERS, REASONS, EXPLANATIONS</description>
<indicators>why, explain, what is driving, what is causing, what's behind, reason for, root cause, how come, what factors</indicators>
<action>Extract the causal question to root_cause_query. Keep VERBATIM - exact text from user.</action>
</intent>

<intent name="combined">
<description>User wants BOTH data AND causal analysis in one request</description>
<indicators>Contains both data keywords AND causal keywords</indicators>
<action>Split into data_retrieval_query AND root_cause_query. Keep both portions.</action>
</intent>

<intent name="clarification_needed">
<description>User's intent is unclear, vague, or ambiguous - you cannot determine what they want</description>
<indicators>Very short/vague input, unclear pronouns without context, incomplete thoughts</indicators>
<action>Set clarification_question asking user to be more specific.</action>
</intent>

<intent name="general_question">
<description>General questions about the system, capabilities, how to use features - not data or analysis requests</description>
<indicators>how do I, can you, what can you do, help me understand, how does this work</indicators>
<action>Set direct_response with helpful information about capabilities.</action>
</intent>

<intent name="off_topic">
<description>User is asking about unrelated topics not connected to data, analytics, or business questions</description>
<indicators>Questions about weather, sports, entertainment, personal topics unrelated to business/data</indicators>
<action>Set direct_response politely redirecting to business/data topics.</action>
</intent>

</intent_categories>

<CRITICAL_CLASSIFICATION_RULES>

‚ö†Ô∏è RULE 1: NEVER DO THE WORK - ONLY CLASSIFY
You are NOT a data analyst. You are NOT an answering agent. You ONLY identify intent type.
Do NOT retrieve data. Do NOT answer analytical questions. ONLY classify and route.

‚ö†Ô∏è RULE 2: CAUSAL KEYWORD DETECTION
These ALWAYS indicate root_cause_analysis intent:
- "what is driving"
- "what is causing"
- "what's driving"  
- "what's causing"
- "why is/are"
- "explain why"
- "reason for"
- "root cause"
- "what factors"
- "what's behind"
- "how come"

‚ö†Ô∏è RULE 3: DATA KEYWORD DETECTION  
These ALWAYS indicate data_retrieval intent:
- "show me"
- "list"
- "get"
- "display"
- "find all"
- "what is the value/total/count of"
- "how many"

‚ö†Ô∏è RULE 4: DIRECT RESPONSE ONLY FOR SIMPLE CASES
Use direct_response ONLY for:
- Greetings (hi, hello, thanks)
- General system questions (what can you do)
- Off-topic redirects

Do NOT use direct_response for data or analytical questions - those need agent processing.

‚ö†Ô∏è RULE 5: VERBATIM PRESERVATION FOR ROOT CAUSE
For root_cause_query: Copy EXACT text from user. No paraphrasing, no summarizing.

‚ö†Ô∏è RULE 6: NO SQL CODE GENERATION
NEVER write SELECT/FROM/WHERE statements. Keep everything as natural language.

‚ö†Ô∏è RULE 7: CONFIDENCE SCORING
- high: Intent is crystal clear
- medium: Intent is likely but has some ambiguity
- low: Intent is very unclear, might need clarification

</CRITICAL_CLASSIFICATION_RULES>
</intent_classification_instructions>
"""

PROMPT_DATA_AGENT_SYSTEM = "You are an expert Data Analyst."

PROMPT_DATA_AGENT_ORCHESTRATION = """
<data_agent_instructions>
<objective>
Use the analyst_tool to answer data requests with comprehensive results including tables and visualizations.
</objective>
<execution_rules>
<rule id="1">ALWAYS generate a table (Result Set) for data requests</rule>
<rule id="2">Generate appropriate visualizations when data is suitable for charts</rule>
<rule id="3">Provide clear, concise summaries of the data findings</rule>
<rule id="4">Handle errors gracefully and explain any data limitations</rule>
</execution_rules>

<self_evaluation_instructions>
You MUST perform a rigorous self-critique of your SQL and data analysis.
1. **Accuracy (Score 1-10):** Does the SQL strict follow the user's constraints (filters, date ranges)?
2. **Data Integrity:** Are the results logical?
**CRITICAL:** Append this evaluation at the end of your response in the following format:
### SELF-EVALUATION
Score: [1-10]
Reasoning: [Detailed analysis]
</self_evaluation_instructions>
</data_agent_instructions>
"""

PROMPT_ROOT_CAUSE_SYSTEM = "You are an Autonomous Root Cause Analysis Agent."

PROMPT_ROOT_CAUSE_ORCHESTRATION = """
<root_cause_orchestration_prompt>
    <role_and_objective>
        You are an **Autonomous Root Cause Analysis Agent** designed to traverse a metric decision tree from the root node down to the leaf nodes.
        ALWAYS USE tree_id as Pharma_Master_v1.
    </role_and_objective>

    <operational_procedure>
        **Step 1: Initialization (Find the Root)**
        * Call `Diagnostic_tool` in **Search Mode**.
        
        **Step 2: Status Evaluation & Decision**
        * Analyze the nodes. Focus ONLY on nodes where `"status": "bad"`.
        * Condition A (Drill Needed): If "bad" and NO children visible -> Drill.
        * Condition C (Stop): If "bad" and NO children exist -> Root Cause.

        **Step 3: Execution (Drill Down)**
        * Call `Diagnostic_tool` in **Drill Mode** with parent IDs.
    </operational_procedure>

    <self_evaluation_instructions>
    You MUST perform a rigorous self-critique.
    1. **Logic (Score 1-10):** Is the causal chain sound?
    2. **Driver Isolation:** Did we truly find the leaf node?
    **CRITICAL:** Append this evaluation at the end.
    </self_evaluation_instructions>
</root_cause_orchestration_prompt>
"""

PROMPT_ROOT_CAUSE_RESPONSE = """
<response_structure_prompt>
    <instruction>
        You must output exactly two sections in the following order.
    </instruction>

    <section_1_executive_summary>
        #### **SECTION 1: Diagnostic Executive Summary**
        * Headline, Narrative, Actionable Insight.
    </section_1_executive_summary>

    <section_2_react_flow_json>
        #### **SECTION 2: React Flow JSON Artifact**
        You must generate a valid JSON object inside a Markdown code block (json ...).
        Schema: { "nodes": [...], "edges": [...] }
    </section_2_react_flow_json>
</response_structure_prompt>
"""

# ==========================================
# 4. ENHANCED SEMANTIC VIEW PARSER
# ==========================================

class SemanticViewParser:
    """
    Parses 'DESCRIBE SEMANTIC VIEW' output to build a rich schema object.
    Enriches JSON with Descriptions, Types, and Relationships from SQL rows.
    """
    def __init__(self, session: Session, view_name: str):
        self.ca_json = {"tables": [], "relationships": []}
        self.table_comments = {} 
        self.column_metadata = defaultdict(lambda: defaultdict(dict)) 
        self.rel_metadata = defaultdict(dict) 
        
        if CONFIG.enable_semantic_parsing:
            self._fetch_and_parse(session, view_name)

    def _fetch_and_parse(self, session: Session, view_name: str):
        print(f"üìñ DESCRIBING SEMANTIC VIEW: {view_name}...")
        try:
            # 1. Fetch Metadata
            rows = session.sql(f"DESCRIBE SEMANTIC VIEW {view_name}").collect()
            data = [row.as_dict() for row in rows]
            
            # 2. Extract Base JSON (Structure + Sample Values)
            ca_row = next((r for r in data if r.get('object_kind') == 'EXTENSION' and r.get('object_name') == 'CA'), None)
            if ca_row:
                try:
                    self.ca_json = json.loads(ca_row['property_value'])
                except Exception as e:
                    print(f"‚ùå JSON Parse Error: {e}")

            # 3. Iterate Rows to Extract Rich Metadata
            for r in data:
                kind = r.get('object_kind')
                name = r.get('object_name')
                prop = r.get('property')
                val = r.get('property_value')
                
                # Table Comments
                if kind == 'TABLE' and prop == 'COMMENT':
                    simple_name = name.split('.')[-1]
                    self.table_comments[simple_name] = val
                
                # Column Details (Dimensions & Facts)
                elif kind in ['DIMENSION', 'FACT', 'COLUMN']:
                    t_name = r.get('parent_entity', '').split('.')[-1]
                    if t_name:
                        if prop == 'COMMENT':
                            self.column_metadata[t_name][name]['description'] = val
                        elif prop == 'DATA_TYPE':
                            self.column_metadata[t_name][name]['data_type'] = val
                            
                # Relationship Details
                elif kind == 'RELATIONSHIP':
                    if prop == 'TABLE':
                        self.rel_metadata[name]['source_table'] = val.split('.')[-1]
                    elif prop == 'REF_TABLE':
                        self.rel_metadata[name]['target_table'] = val.split('.')[-1]
                    elif prop == 'FOREIGN_KEY':
                        self.rel_metadata[name]['keys'] = val

            print(f"‚úÖ Metadata Loaded: {len(self.ca_json.get('tables', []))} tables enriched.")

        except Exception as e:
            print(f"‚ö†Ô∏è Metadata Parse Failed: {e}")

    def get_table_summaries(self) -> str:
        """Returns a list for the LLM to select from."""
        summary_list = []
        for t in self.ca_json.get('tables', []):
            t_name = t.get('name')
            desc = self.table_comments.get(t_name, "No description.")
            summary_list.append(f"- Table: {t_name}\n  Description: {desc}")
        return "\n".join(summary_list) if summary_list else ""

    def get_pruned_context(self, selected_tables: List[str]) -> str:
        """
        Builds a rich JSON object for the selected tables.
        Injects descriptions, types, and relevant relationships.
        """
        pruned = {"tables": [], "relationships": []}
        
        # 1. Process Tables
        for t in self.ca_json.get('tables', []):
            t_name = t['name']
            if t_name in selected_tables:
                # Copy & Enrich
                table_copy = t.copy()
                table_copy['description'] = self.table_comments.get(t_name, "")
                
                # Enrich Dimensions/Facts
                for category in ['dimensions', 'facts', 'time_dimensions']:
                    if category in table_copy:
                        enriched_cols = []
                        for col in table_copy[category]:
                            col_copy = col.copy()
                            c_name = col['name']
                            
                            # Inject metadata
                            meta = self.column_metadata[t_name].get(c_name, {})
                            if 'description' in meta: col_copy['description'] = meta['description']
                            if 'data_type' in meta: col_copy['data_type'] = meta['data_type']
                            
                            enriched_cols.append(col_copy)
                        table_copy[category] = enriched_cols
                
                pruned['tables'].append(table_copy)

        # 2. Process Relationships
        if 'relationships' in self.ca_json:
            for rel in self.ca_json['relationships']:
                r_name = rel.get('name')
                sql_meta = self.rel_metadata.get(r_name, {})
                source = sql_meta.get('source_table')
                target = sql_meta.get('target_table')
                
                if source in selected_tables:
                    rel_copy = rel.copy()
                    rel_copy['source_table'] = source
                    rel_copy['target_table'] = target
                    if 'keys' in sql_meta: rel_copy['keys'] = sql_meta['keys']
                    pruned['relationships'].append(rel_copy)

        return json.dumps(pruned, indent=2)

# ==========================================
# 5. HELPER FUNCTIONS
# ==========================================

def log_checkpoint(step: str, details: Any):
    print(f"\n{'='*50}\nüõë CHECKPOINT: {step}\n{'-'*50}")
    if isinstance(details, BaseModel): pprint.pprint(details.model_dump())
    elif isinstance(details, (dict, list)): pprint.pprint(details)
    else: print(str(details))
    print(f"{'='*50}\n")

def clean_schema_for_cortex(schema: Dict, defs: Dict = None) -> Dict:
    """Recursively cleans Pydantic schema for Cortex."""
    if defs is None: defs = schema.get('$defs') or schema.get('definitions') or {}
    
    def process(node):
        if not isinstance(node, dict): return node
        if '$ref' in node:
            ref_key = node['$ref'].split('/')[-1]
            if ref_key in defs:
                merged = {**defs[ref_key], **node}
                del merged['$ref']
                return process(merged)
        if 'anyOf' in node:
            valid_types = [process(opt).get('type') for opt in node['anyOf'] if process(opt).get('type')]
            if 'null' in valid_types:
                main = [t for t in valid_types if t != 'null']
                return {"type": main + ['null'], "description": node.get("description", "")}
        
        ALLOWED_KEYS = {'type', 'properties', 'items', 'required', 'enum', 'description'}
        new_node = {}
        for k, v in node.items():
            if k in ALLOWED_KEYS:
                if k == 'properties': new_node[k] = {pk: process(pv) for pk, pv in v.items()}
                elif k == 'items': new_node[k] = process(v)
                else: new_node[k] = v
        return new_node
    
    cleaned = process(schema)
    if 'type' not in cleaned: cleaned['type'] = 'object'
    return cleaned

def invoke_structured_output_extraction(session: Session, messages: List[Dict], pydantic_model: type[BaseModel]) -> BaseModel:
    """
    Invokes Cortex Inference with a STRICTLY CLEANED schema to prevent validation errors.
    """
    ENDPOINT = "/api/v2/cortex/inference:complete"
    
    # 1. Generate & Clean Schema
    raw_schema = pydantic_model.model_json_schema()
    final_schema = clean_schema_for_cortex(raw_schema)

    payload = {
        "model": "claude-4-sonnet",
        "messages": messages,
        "max_tokens": 2000,
        "response_format": {
            "type": "json",
            "schema": final_schema 
        }
    }

    log_checkpoint("API REQUEST (STRUCTURED)", f"Target Model: {pydantic_model.__name__}")

    try:
        # 2. Send Request
        resp = _snowflake.send_snow_api_request("POST", ENDPOINT, {"Content-Type": "application/json"}, {}, payload, {}, 120000)
        
        # 3. Handle Errors explicitly
        if resp.get('code') and resp.get('message'):
             print(f"‚ùå API ERROR: {resp['message']}")
             return pydantic_model.construct()

        content_raw = resp.get("content")
        if not content_raw:
            print("‚ùå Empty content received")
            return pydantic_model.construct()

        # 4. Accumulate Streaming Content
        full_text = ""
        if content_raw.strip().startswith("["):
            try:
                chunks = json.loads(content_raw)
                for chunk in chunks:
                    data_obj = chunk.get("data", {})
                    choices = data_obj.get("choices", [])
                    if choices:
                        delta = choices[0].get("delta", {})
                        full_text += delta.get("content") or delta.get("text") or ""
            except json.JSONDecodeError:
                full_text = content_raw
        else:
             try:
                 parsed = json.loads(content_raw)
                 full_text = parsed.get('choices', [{}])[0].get('message', {}).get('content', '')
             except:
                 full_text = content_raw

        log_checkpoint("RAW ACCUMULATED TEXT", full_text)

        if not full_text:
            return pydantic_model.construct()

        # 5. Parse Final JSON
        clean_json = full_text.replace("```json", "").replace("```", "").strip()
        parsed_dict = json.loads(clean_json)
        
        return pydantic_model(**parsed_dict)

    except Exception as e:
        print(f"Structured Extraction Error: {e}")
        return pydantic_model.construct()

def invoke_cortex_agent(session: Session, payload: Dict, agent_name: str) -> Dict:
    """Invokes 'agent:run' for tools (Analyst/Diagnostic)."""
    ENDPOINT = "/api/v2/cortex/agent:run"
    print(f"--- INVOKING AGENT: {agent_name} ---")

    try:
        resp = _snowflake.send_snow_api_request("POST", ENDPOINT, {"Content-Type": "application/json", "Accept": "text/event-stream"}, {}, payload, {}, 120000)
        content = resp.get("text")
        if not content: return {"error": "Empty"}
        
        extracted = {"text": "", "tables": [], "charts": [], "sql_generated": None, "sql_explanation": None}
        try: parsed = json.loads(content)
        except: parsed = [json.loads(line) for line in content.splitlines() if line.strip()]
        
        def process_event(evt, data):
            if evt == 'response.text.delta': extracted["text"] += data.get('text', '')
            elif evt == 'response.chart': extracted["charts"].append(data.get('chart') or data.get('chart_spec'))
            elif evt == 'response.table':
                rs = data.get('result_set', [])
                extracted["tables"].append(rs) # Limit 5 rows
            elif evt == 'response.tool_result.analyst.delta':
                delta = data.get('delta', {})
                if delta.get('sql'): extracted["sql_generated"] = delta.get('sql')
                if delta.get('sql_explanation'): extracted["sql_explanation"] = delta.get('sql_explanation')
                if delta.get('result_set'): extracted["tables"].append(delta.get('result_set'))
            elif evt == 'response.tool_result':
                for item in data.get('content', []):
                    if item.get('type') == 'json':
                        json_p = item.get('json', {})
                        if 'sql' in json_p: extracted["sql_generated"] = json_p['sql']
                        if 'result_set' in json_p: extracted["tables"].append(json_p['result_set'])

        if isinstance(parsed, list):
            for item in parsed: process_event(item.get('event'), item.get('data', {}))
        
        print(f"--- AGENT {agent_name} COMPLETED ---")
        return extracted
    except Exception as e: return {"error": str(e)}

def check_input_guard(session: Session, user_input: str) -> GuardResult:
    log_checkpoint("START: GUARD", user_input)
    try:
        safe_input = user_input.replace("'", "''")
        res = session.sql(f"SELECT SNOWFLAKE.CORTEX.COMPLETE('{CONFIG.model}', [{{'role': 'user', 'content': '{safe_input}'}}], {{'guardrails': true}}) as r").collect()
        if not res: return GuardResult(is_safe=True)
        is_unsafe = "Response filtered" in json.loads(res[0]['R'])['choices'][0]['messages']
        return GuardResult(is_safe=not is_unsafe, message="Blocked." if is_unsafe else None)
    except: return GuardResult(is_safe=True)

class PayloadFactory:
    @staticmethod
    def create(query: str, instructions: Dict, tools: List = None, resources: Dict = None, history: List = None) -> Dict:
        messages = (history if history else []) + [{"role": "user", "content": [{"type": "text", "text": query}]}]
        return {
            "messages": messages,
            "models": {"orchestration": CONFIG.model},
            "instructions": instructions,
            "tools": tools or [],
            "tool_resources": resources or {}
        }

def _save_row(session: Session, data: dict, table_name: str = "AGENT_DETAILED_HISTORY"):
    try:
        df = session.create_dataframe([data])
        df.create_or_replace_temp_view("TEMP_AGENT_LOG_STAGING")
        sql = f"""
        INSERT INTO {table_name} (
            USER_QUERY, REPHRASED_QUERY, INTENT_TYPE, INTENT_CONFIDENCE,
            DATA_SUMMARY, DATA_SQL, DATA_SQL_EXPLANATION, DATA_CLARIFICATION,
            DATA_EVAL_SCORE, DATA_EVAL_REASONING, 
            DATA_RESULT_SET, RC_SUMMARY, RC_GRAPH_JSON,
            RC_EVAL_SCORE, RC_EVAL_REASONING, IS_BLOCKED, FULL_RAW_JSON
        )
        SELECT 
            USER_QUERY, REPHRASED_QUERY, INTENT_TYPE, INTENT_CONFIDENCE,
            DATA_SUMMARY, DATA_SQL, DATA_SQL_EXPLANATION, DATA_CLARIFICATION,
            DATA_EVAL_SCORE, DATA_EVAL_REASONING, 
            PARSE_JSON(DATA_RESULT_SET), RC_SUMMARY, PARSE_JSON(RC_GRAPH_JSON),
            RC_EVAL_SCORE, RC_EVAL_REASONING, IS_BLOCKED, PARSE_JSON(FULL_RAW_JSON)
        FROM TEMP_AGENT_LOG_STAGING
        """
        session.sql(sql).collect()
        print(f"‚úÖ Row saved to {table_name}")
    except Exception as e: print(f"‚ùå Save Error: {e}")

def get_cortex_completion(session: Session, prompt: str) -> str:
    try:
        safe_prompt = prompt.replace("'", "''")
        cmd = f"SELECT SNOWFLAKE.CORTEX.COMPLETE('{CONFIG.model_writer}', '{safe_prompt}') as R"
        return session.sql(cmd).collect()[0]['R']
    except Exception as e: return ""

def get_chat_history(session: Session, conversation_id: str) -> List[Dict]:
    try:
        rows = session.sql(f"SELECT SENDER_TYPE, MESSAGE FROM {CONFIG.messages_table} WHERE CONVERSATION_ID = '{conversation_id}' ORDER BY CREATED_AT ASC LIMIT 10").collect()
        return [{"role": r['SENDER_TYPE'], "content": r['MESSAGE']} for r in rows]
    except: return []

# ==========================================
# 6. AGENT MANAGER
# ==========================================

class AgentManager:
    def __init__(self, session: Session):
        self.session = session
        self.parser = SemanticViewParser(session, CONFIG.cortex_analyst_object)

    def identify_needed_tables(self, query: str) -> List[str]:
        print("üîç IDENTIFYING RELEVANT TABLES...")
        if not self.parser.ca_json.get('tables'): return []
        summaries = self.parser.get_table_summaries()
        prompt = f"Select relevant tables for: {query}\nTables:\n{summaries}"
        result: TableSelection = invoke_structured_output_extraction(self.session, [{"role": "user", "content": prompt}], TableSelection)
        print(f"‚úÖ Selected: {result.relevant_table_names}")
        return result.relevant_table_names

    def run_semantic_translator(self, query: str, relevant_tables: List[str]) -> str:
        print(f"üß† REWRITING QUERY (Enriched with Descriptions, Types, Relationships)...")
        pruned_json = self.parser.get_pruned_context(relevant_tables)
        prompt = f"""
        Rewrite user question to align with the schema.
        Context: {pruned_json}
        Rules: Use exact columns. Map filters using sample_values. Use Descriptions to disambiguate.
        Question: {query}
        """
        return get_cortex_completion(self.session, prompt).strip()

    def run_rephraser(self, query: str, history: List[Dict]) -> str:
        messages = history + [{"role": "user", "content": f"{PROMPT_REPHRASER_ORCHESTRATION}\n\nInput: {query}"}]
        result: RephraserOutput = invoke_structured_output_extraction(self.session, messages, RephraserOutput)
        return result.refined_query

    def run_intent_classifier(self, query: str) -> IntentClassification:
        messages = [{"role": "system", "content": PROMPT_INTENT_SYSTEM}, {"role": "user", "content": f"{PROMPT_INTENT_ORCHESTRATION}\n\nClassify this: {query}"}]
        return invoke_structured_output_extraction(self.session, messages, IntentClassification)

    def run_data_agent(self, query: str) -> SqlResult:
        tools = [{"tool_spec": {"type": "cortex_analyst_text_to_sql", "name": "analyst_tool"}}]
        res_def = {"analyst_tool": {"type": "cortex_analyst_text_to_sql", "semantic_view": CONFIG.cortex_analyst_object, "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse}}}
        inst = {"system": PROMPT_DATA_AGENT_SYSTEM, "orchestration": PROMPT_DATA_AGENT_ORCHESTRATION}
        
        resp = invoke_cortex_agent(self.session, PayloadFactory.create(query, inst, tools, res_def), "Data Agent")
        print(resp)
        raw_text = resp.get('text', '')
        extraction_msg = [{"role": "user", "content": f"Extract structured data:\n\n{raw_text}"}]
        parsed: DataAgentParsedOutput = invoke_structured_output_extraction(self.session, extraction_msg, DataAgentParsedOutput)
        
        return SqlResult(
            answer_text=parsed.clean_answer_text,
            clarifying_question=parsed.clarifying_question,
            tables=resp.get('tables', []),
            sql_generated=resp.get('sql_generated'),
            sql_explanation=resp.get('sql_explanation'),
            evaluation=parsed.evaluation
        )

    def run_root_cause_agent(self, query: str) -> DiagnosticResult:
        tools = [{"tool_spec": {"type": "generic", "name": "Diagnostic_tool", "input_schema": {"type": "object", "properties": {"USER_QUERY": {"type": "string"}, "PARENT_NODE_IDS_JSON": {"type": "string"}, "TREE_ID": {"type": "string"}}, "required": ["USER_QUERY", "PARENT_NODE_IDS_JSON", "TREE_ID"]}}}]
        res_def = {"Diagnostic_tool": {"type": "procedure", "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse}, "identifier": CONFIG.diagnostic_udf}}
        inst = {"system": PROMPT_ROOT_CAUSE_SYSTEM, "orchestration": PROMPT_ROOT_CAUSE_ORCHESTRATION, "response": PROMPT_ROOT_CAUSE_RESPONSE}
        
        resp = invoke_cortex_agent(self.session, PayloadFactory.create(query, inst, tools, res_def), "Root Cause Agent")
        
        raw_text = resp.get('text', '')
        extraction_msg = [{"role": "user", "content": f"Extract structured data:\n\n{raw_text}"}]
        parsed: RootCauseParsedOutput = invoke_structured_output_extraction(self.session, extraction_msg, RootCauseParsedOutput)

        return DiagnosticResult(
            summary_text=parsed.clean_summary_text,
            react_flow_json=parsed.react_flow_json,
            evaluation=parsed.evaluation
        )

# ==========================================
# 7. MAIN EXECUTION
# ==========================================

def main(session: Session):
    manager = AgentManager(session)
    user_query = "show me healthcare providers in Arizona"
    
    row_data = {
        "USER_QUERY": user_query, "REPHRASED_QUERY": None, "INTENT_TYPE": None, "INTENT_CONFIDENCE": None,
        "DATA_SUMMARY": None, "DATA_SQL": None, "DATA_SQL_EXPLANATION": None, "DATA_CLARIFICATION": None,
        "DATA_RESULT_SET": None, "DATA_EVAL_SCORE": None, "DATA_EVAL_REASONING": None,
        "RC_SUMMARY": None, "RC_GRAPH_JSON": None, "RC_EVAL_SCORE": None, "RC_EVAL_REASONING": None,
        "IS_BLOCKED": False, "FULL_RAW_JSON": None
    }
    
    log_checkpoint("MAIN: START", {"query": user_query})

    guard = check_input_guard(session, user_query)
    if not guard.is_safe:
        row_data["IS_BLOCKED"] = True
        _save_row(session, row_data)
        return {"status": "blocked", "message": guard.message}

    rephrased = manager.run_rephraser(user_query, [])
    intent = manager.run_intent_classifier(rephrased)
    
    row_data["REPHRASED_QUERY"] = rephrased
    row_data["INTENT_TYPE"] = intent.intent_type
    row_data["INTENT_CONFIDENCE"] = intent.confidence

    final_output = {"original": user_query, "processed": rephrased, "intent": intent.model_dump(), "results": []}

    if intent.direct_response:
        final_output["results"].append({"type": "direct", "message": intent.direct_response})
    else:
        # DATA AGENT FLOW
        if intent.intent_type in ["data_retrieval", "combined"] and intent.data_retrieval_query:
            res = manager.run_data_agent(intent.data_retrieval_query)
            
            # Self-Healing
            # if res.evaluation.score < 8:
            #     print(f"‚ö†Ô∏è Low Score ({res.evaluation.score}). Initiating Healing...")
            # needed_tables = manager.identify_needed_tables(intent.data_retrieval_query)
            # print(needed_tables)
            # if needed_tables:
            #     optimized_query = manager.run_semantic_translator(intent.data_retrieval_query, needed_tables)
            #     print(f"‚ú® Optimized Query: {optimized_query}")
            #     res = manager.run_data_agent(optimized_query)
            #     res.is_retry = True
            
            final_output["results"].append({"type": "data_analysis", "summary": res.answer_text, "tables": res.tables})
            row_data["DATA_SUMMARY"] = res.answer_text
            row_data["DATA_SQL"] = res.sql_generated
            row_data["DATA_SQL_EXPLANATION"] = res.sql_explanation
            if res.tables: row_data["DATA_RESULT_SET"] = json.dumps(res.tables)
            row_data["DATA_EVAL_SCORE"] = res.evaluation.score
            row_data["DATA_EVAL_REASONING"] = res.evaluation.reasoning

        # ROOT CAUSE FLOW
        if intent.intent_type in ["root_cause_analysis", "combined"] and intent.root_cause_query:
            res_rc = manager.run_root_cause_agent(intent.root_cause_query)
            final_output["results"].append({"type": "root_cause_analysis", "summary": res_rc.summary_text, "graph": res_rc.react_flow_json})
            row_data["RC_SUMMARY"] = res_rc.summary_text
            if res_rc.react_flow_json: row_data["RC_GRAPH_JSON"] = json.dumps(res_rc.react_flow_json)
            row_data["RC_EVAL_SCORE"] = res_rc.evaluation.score
            row_data["RC_EVAL_REASONING"] = res_rc.evaluation.reasoning

    row_data["FULL_RAW_JSON"] = json.dumps(final_output)
    _save_row(session, row_data)
    print("‚úÖ Process Completed")
    return final_output
