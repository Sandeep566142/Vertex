DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
 / 
AI_POC
 / 
DIAGNOSTIC_TOOL(VARCHAR, VARCHAR, VARCHAR)

 
 
 # Note: Use POST, not GET
            response = _snowflake.send_snow_api_request(
                "POST", 
                "/api/v2/cortex/threads", 
                {"Content-Type": "application/json"}, 
                {}, 
                {"origin_application": "my_app"}, 
                None, 
                30000
            )

            print(response)



 # Note: Use POST, not GET
            response = _snowflake.send_snow_api_request(
                "POST", 
                "/api/v2/cortex/threads", 
                {"Content-Type": "application/json"}, 
                {}, 
                {"origin_application": "my_app"}, 
                None, 
                30000
            )

            print(response)
            
name: BASIC_CORTEX_ANALYST
tables:
  - name: ARD_GIACO_TM_HCP_ACUTE_MKT_WEEKLY_LAAD
    description: The table contains records of healthcare provider performance metrics in acute care settings, tracked on a weekly basis. Each record represents a healthcare provider's acute episode activity within specific territories and includes details about provider characteristics, geographic location, product categories, and territorial assignments.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: ARD_GIACO_TM_HCP_ACUTE_MKT_WEEKLY_LAAD
    dimensions:
      - name: BRANDED_FLAG
        description: Indicates whether the product is a branded medication.
        expr: BRANDED_FLAG
        data_type: VARCHAR(16777216)
        sample_values:
          - 'N'
          - 'Y'
      - name: GOLDEN_ID
        description: A unique identifier used to link and track healthcare providers across different systems and databases.
        expr: GOLDEN_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - GLD004700470057009305
          - GLD000900090008008206
          - GLD999909691076060401
      - name: HCP_ID
        description: Unique identifier for healthcare providers in the system.
        expr: HCP_ID
        data_type: VARCHAR(32)
        sample_values:
          - 2bb17ea9ca26bb191254a298da5e5b31
          - e78d13c51088736b6ad0abcfb0db5496
          - 232e71209a7c50de00969cff0164f64f
      - name: HCP_SPECIALTY
        description: Healthcare provider medical specialty or professional designation.
        expr: HCP_SPECIALTY
        data_type: VARCHAR(16777216)
        sample_values:
          - GERIATRIC MEDICINE (INTERNAL MEDICINE)
          - COLON & RECTAL SURGERY
          - GENERAL SURGERY
      - name: HCP_SPECIALTY_GROUP
        description: Healthcare provider specialty groups categorizing medical practice areas.
        expr: HCP_SPECIALTY_GROUP
        data_type: VARCHAR(16777216)
        sample_values:
          - SURGERY
          - MEDICINE
          - ALL OTHER
      - name: NPI
        description: National Provider Identifier numbers for healthcare providers.
        expr: NPI
        data_type: VARCHAR(16777216)
        sample_values:
          - '1568438992'
          - '1093197857'
          - '1710410949'
      - name: ONEKEY_HCP_ID
        description: Unique identifier for healthcare professionals in the OneKey system.
        expr: ONEKEY_HCP_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - WUSM00274580
          - WUSR03862562
          - WUSM01303609
      - name: PRACTICE_SETTING
        description: The type of healthcare practice setting where medical services are provided.
        expr: PRACTICE_SETTING
        data_type: VARCHAR(256)
        sample_values:
          - RETAIL
          - DISCHARGE
      - name: PROCEDURE_GROUP
        description: Medical procedure categories or groupings used for classification purposes.
        expr: PROCEDURE_GROUP
        data_type: VARCHAR(256)
        sample_values:
          - LAPAROSCOPIC UROLOGIC PROCEDURES
          - OB/GYN PROCEDURES
          - JOINT REPAIR
      - name: PRODUCT_CATEGORY
        description: Categories of pharmaceutical pain management products used in acute care markets.
        expr: PRODUCT_CATEGORY
        data_type: VARCHAR(256)
        sample_values:
          - ANALGESICS-SEDATIVE COMBINATION
          - DERMATOLOGICAL LOCAL TOPICAL ANESTHETIC
          - MODERATE OPIOID COMBINATIONS
      - name: PRODUCT_FAMILY_GROUP
        description: Pharmaceutical product family groups used for pain management and treatment.
        expr: PRODUCT_FAMILY_GROUP
        data_type: VARCHAR(256)
        sample_values:
          - IBUPROFEN
          - DICLOFENAC
          - MELOXICAM
      - name: PTAM_TERRITORY_ID
        description: Unique identifier for a PTAM (Pain Territory Account Manager) territory.
        expr: PTAM_TERRITORY_ID
        data_type: VARCHAR(255)
        sample_values:
          - V1NAUSA-5733506
          - V1NAUSA-5733204
          - V1NAUSA-5733403
      - name: ROA
        description: Route of administration for the medication or treatment.
        expr: ROA
        data_type: VARCHAR(256)
        sample_values:
          - ORAL
          - Sales territory identifier used to designate specific geographicTOPICAL
          - TOPICAL-PATCH
      - name: SAL_TERRITORY_ID
        description:  or organizational sales regions.
        expr: SAL_TERRITORY_ID
        data_type: VARCHAR(255)
        sample_values:
          - V1NAUSA-5720008
          - V1NAUSA-5720014
          - V1NAUSA-5720013
      - name: SITE_OF_CARE_ZIP_CODE
        description: ZIP codes for healthcare facility locations.
        expr: SITE_OF_CARE_ZIP_CODE
        data_type: VARCHAR(16777216)
        sample_values:
          - '80911'
          - '76903'
          - '32771'
      - name: VERTEX_ID
        description: Unique identifier for a vertex in the healthcare provider network.
        expr: VERTEX_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - '14811523'
          - '14821592'
          - '17503007'
      - name: ZIP_CODE
        description: Postal ZIP codes for geographic locations.
        expr: ZIP_CODE
        data_type: VARCHAR(16777216)
        sample_values:
          - '14621'
          - '01805'
          - '95337'
    time_dimensions:
      - name: DATE_KEY
        description: A date used as a key identifier in the healthcare provider acute market weekly reporting data.
        expr: DATE_KEY
        data_type: DATE
        sample_values:
          - '2025-02-14'
          - '2024-11-15'
          - '2022-11-11'
    facts:
      - name: ACUTE_CALENDAR_DAYS_LAAD
        description: The number of calendar days in the acute care period for look-alike-alike drug analysis.
        expr: ACUTE_CALENDAR_DAYS_LAAD
        data_type: NUMBER(38,5)
        access_modifier: public_access
        sample_values:
          - '1.00000'
          - '3.00000'
          - '10.00000'
      - name: ACUTE_EPISODES_LAAD
        description: The number of acute episodes within the look-ahead period.
        expr: ACUTE_EPISODES_LAAD
        data_type: NUMBER(18,0)
        access_modifier: public_access
        sample_values:
          - '1'
          - '4'
          - '2'
      - name: ACUTE_PROCEDURES_LAAD
        description: The number of acute procedures performed using laparoscopic-assisted abdominal delivery techniques.
        expr: ACUTE_PROCEDURES_LAAD
        data_type: NUMBER(18,0)
        access_modifier: public_access
        sample_values:
          - '1'
          - '3'
          - '2'
      - name: ACUTE_REFILLS_LAAD
        description: The number of acute medication refills within the look-ahead period.
        expr: ACUTE_REFILLS_LAAD
        data_type: NUMBER(13,0)
        access_modifier: public_access
        sample_values:
          - '2'
          - '4'
          - '1'
      - name: ACUTE_TRX_LAAD
        description: The number of acute transactions for the last-as-advertised date period.
        expr: ACUTE_TRX_LAAD
        data_type: NUMBER(18,0)
        access_modifier: public_access
        sample_values:
          - '5'
          - '1'
          - '3'
      - name: ACUTE_TX_DAYS_LAAD
        description: The number of days for acute treatment based on LAAD (Last Available Actual Data).
        expr: ACUTE_TX_DAYS_LAAD
        data_type: NUMBER(38,5)
        access_modifier: public_access
        sample_values:
          - '5.00000'
          - '10.00000'
          - '30.00000'
      - name: ACUTE_UNITS_LAAD
        description: Acute care units measured as a look-ahead average demand.
        expr: ACUTE_UNITS_LAAD
        data_type: NUMBER(38,5)
        access_modifier: public_access
        sample_values:
          - '9.00000'
          - '20.00000'
          - '90.00000'
      - name: WEEK_DT
        description: The date representing the week in the healthcare provider acute market reporting period.
        expr: WEEK_DT
        data_type: NUMBER(10,0)
        access_modifier: public_access
        sample_values:
          - '20240524'
          - '20250418'
          - '20250606'
  - name: TD_GEO_CITY
    description: The table contains records of cities and their geographic information. Each record represents a single city and includes hierarchical location details such as state and country associations, along with precise coordinate data.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_GEO_CITY
    dimensions:
      - name: GEO_CITY_KEY
        description: A unique identifier key for geographic city records.
        expr: GEO_CITY_KEY
        data_type: VARCHAR(32)
        sample_values:
          - cb004e0c1c429625db774a086579bd3b
          - 3de4110deff030eca4c2da9e8e85ae39
          - 6b5b3307241b5195dd6bfef7630d834e
      - name: GEO_CITY_NAME
        description: The name of a geographic city location.
        expr: GEO_CITY_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - BELLEVUE
          - MATTAPAN
          - TALLAHASSEE FALLS
      - name: GEO_COUNTRY_CD
        description: Geographic country code representing the country where the city is located.
        expr: GEO_COUNTRY_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - US
      - name: GEO_COUNTRY_NAME
        description: The name of the country in a geographic location.
        expr: GEO_COUNTRY_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - UNITED STATES
      - name: GEO_STATE_CD
        description: State codes for geographic locations.
        expr: GEO_STATE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - XX
          - VT
          - FL
      - name: GEO_STATE_NAME
        description: The name of the state within the United States.
        expr: GEO_STATE_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - SOUTH CAROLINA
          - TENNESSEE
          - NEBRASKA
    facts:
      - name: GEO_COUNTRY_KEY
        description: A unique identifier for countries in the geographic data structure.
        expr: GEO_COUNTRY_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '1'
      - name: GEO_LATITUDE
        description: Geographic latitude coordinate in decimal degrees.
        expr: GEO_LATITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
      - name: GEO_LONGITUDE
        description: Geographic longitude coordinate expressed in decimal degrees.
        expr: GEO_LONGITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
      - name: GEO_STATE_KEY
        description: Numeric identifier for geographic states.
        expr: GEO_STATE_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '21'
          - '22'
          - '53'
  - name: TD_GEO_LOC
    description: The table contains records of geographic locations with their complete address and coordinate information. Each record represents a specific location and includes hierarchical geographic details from address lines through city, state, postal code, and country levels, as well as precise latitude and longitude coordinates.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_GEO_LOC
    dimensions:
      - name: GEO_CITY_KEY
        description: A unique identifier key for geographic cities.
        expr: GEO_CITY_KEY
        data_type: VARCHAR(32)
        sample_values:
          - e190c59f3d88823cb626962e558314df
          - 21bb6ea0074b193b6643355e6b83aec2
          - a6a0645ce9ac1995532b4bb9f28ce04c
      - name: GEO_CITY_NAME
        description: Geographic city names.
        expr: GEO_CITY_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - HARRISBURG
          - SAINT LOUIS
          - DEER LODGE
      - name: GEO_COUNTRY_CD
        description: Geographic country code representing the country location.
        expr: GEO_COUNTRY_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - US
      - name: GEO_COUNTRY_NAME
        description: The name of the country in a geographic location.
        expr: GEO_COUNTRY_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - UNITED STATES
      - name: GEO_LOC_KEY
        description: A unique identifier for geographic location records.
        expr: GEO_LOC_KEY
        data_type: VARCHAR(32)
        sample_values:
          - 96a44f5985fb18551e9c45db3d75f237
          - 40665982fb20e15ee0b1e0bde8b43330
          - 512d454f090f878a6e8175cad706cecc
      - name: GEO_POSTAL_CD
        description: Postal codes for geographic locations.
        expr: GEO_POSTAL_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - '45459'
          - '92037'
          - '33175'
      - name: GEO_POSTAL_CD_EXT
        description: A column holding data of type VARCHAR(16777216).
        expr: GEO_POSTAL_CD_EXT
        data_type: VARCHAR(16777216)
      - name: GEO_POSTAL_KEY
        description: A unique identifier key for postal geographic locations.
        expr: GEO_POSTAL_KEY
        data_type: VARCHAR(32)
        sample_values:
          - c3195f6d307dd319bf1275b8f98dca51
          - 1a1c1e45309875960a48106f0b73a0b9
          - f8ca2b7549985685fb5a1275bfcced43
      - name: GEO_STATE_CD
        description: State codes for geographic locations.
        expr: GEO_STATE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - FL
          - MI
          - MA
      - name: GEO_STATE_NAME
        description: The name of the state within the United States.
        expr: GEO_STATE_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - SOUTH CAROLINA
          - MONTANA
          - PENNSYLVANIA
      - name: LOC_ADDR_LINE_1
        description: Street addresses including house numbers and street names.
        expr: LOC_ADDR_LINE_1
        data_type: VARCHAR(16777216)
        sample_values:
          - 12110 NE 64TH PL
          - 1920 LENLAND AVE
          - 210 BAXTER ST
      - name: LOC_ADDR_LINE_2
        description: Secondary address information such as suite numbers, apartment numbers, or room designations.
        expr: LOC_ADDR_LINE_2
        data_type: VARCHAR(16777216)
        sample_values:
          - UNIT B
          - APT 206
      - name: LOC_ADDR_LINE_3
        description: The third line of a location's address.
        expr: LOC_ADDR_LINE_3
        data_type: VARCHAR(16777216)
      - name: LOC_ADDR_LINE_4
        description: The fourth line of a location's address.
        expr: LOC_ADDR_LINE_4
        data_type: VARCHAR(16777216)
    facts:
      - name: GEO_COUNTRY_KEY
        description: A unique identifier for countries in the geographic location dimension.
        expr: GEO_COUNTRY_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '1'
      - name: GEO_LATITUDE
        description: Geographic latitude coordinate expressed in decimal degrees.
        expr: GEO_LATITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
      - name: GEO_LONGITUDE
        description: Geographic longitude coordinate expressed in decimal degrees.
        expr: GEO_LONGITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
      - name: GEO_STATE_KEY
        description: Unique identifier for geographic states.
        expr: GEO_STATE_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '59'
          - '22'
          - '30'
  - name: TD_GEO_POSTAL
    description: The table contains records of geographic postal code areas with their associated location hierarchies. Each record represents a postal code and includes details about the corresponding state and country jurisdictions as well as precise geographic coordinates.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_GEO_POSTAL
    dimensions:
      - name: GEO_COUNTRY_CD
        description: Geographic country code representing the country associated with a postal location.
        expr: GEO_COUNTRY_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - US
      - name: GEO_COUNTRY_KEY
        description: A unique identifier for geographic countries.
        expr: GEO_COUNTRY_KEY
        data_type: NUMBER(38,0)
        sample_values:
          - '1'
      - name: GEO_COUNTRY_NAME
        description: The name of the country in a geographic location.
        expr: GEO_COUNTRY_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - UNITED STATES
      - name: GEO_POSTAL_CD
        description: Postal codes for geographic locations.
        expr: GEO_POSTAL_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - '25205'
          - '62545'
          - '78675'
      - name: GEO_POSTAL_KEY
        description: A unique identifier key for geographic postal code records.
        expr: GEO_POSTAL_KEY
        data_type: VARCHAR(32)
        sample_values:
          - 794d1a9eee30d774236a5186a2b55bf9
          - 0eec7bab53a4751845b83ff0ce77c5a1
          - 728b5b967aeb1d257948dc6e622bbc95
      - name: GEO_STATE_CD
        description: State codes for geographic locations.
        expr: GEO_STATE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - TX
          - WV
          - MS
      - name: GEO_STATE_KEY
        description: Unique identifier for geographic states within the system.
        expr: GEO_STATE_KEY
        data_type: NUMBER(38,0)
        sample_values:
          - '75'
          - '59'
          - '28'
      - name: GEO_STATE_NAME
        description: The name of the geographic state or territory.
        expr: GEO_STATE_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - GEORGIA
          - NEW JERSEY
          - TEXAS
    facts:
      - name: GEO_LATITUDE
        description: Geographic latitude coordinate expressed in decimal degrees.
        expr: GEO_LATITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
      - name: GEO_LONGITUDE
        description: Geographic longitude coordinate expressed in decimal degrees.
        expr: GEO_LONGITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
  - name: TD_GEO_STATE
    description: The table contains records of geographic states and their associated countries. Each record includes location identifiers, geographic coordinates, and hierarchical relationships between states and countries.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_GEO_STATE
    dimensions:
      - name: GEO_COUNTRY_CD
        description: Geographic country code.
        expr: GEO_COUNTRY_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - US
      - name: GEO_COUNTRY_NAME
        description: The name of the country in a geographic location.
        expr: GEO_COUNTRY_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - UNITED STATES
      - name: GEO_STATE_CD
        description: Geographic state codes used to identify states or territories.
        expr: GEO_STATE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - NC
          - XX
          - DE
      - name: GEO_STATE_NAME
        description: Names of US states and territories.
        expr: GEO_STATE_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - SOUTH CAROLINA
          - DELAWARE
          - NEBRASKA
    facts:
      - name: GEO_COUNTRY_KEY
        description: Geographic country key used as a unique identifier for countries.
        expr: GEO_COUNTRY_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '1'
      - name: GEO_LATITUDE
        description: Geographic latitude coordinates for state locations.
        expr: GEO_LATITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
        sample_values:
          - '39.00040000'
          - '34.00040000'
      - name: GEO_LONGITUDE
        description: Geographic longitude coordinates for state locations.
        expr: GEO_LONGITUDE
        data_type: NUMBER(11,8)
        access_modifier: public_access
        sample_values:
          - '-86.25030000'
          - '-71.49980000'
          - '-123.04600000'
      - name: GEO_STATE_KEY
        description: Unique identifier for geographic state records.
        expr: GEO_STATE_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '21'
          - '261'
          - '22'
  - name: TD_HCO
    description: The table contains records of healthcare organizations with their identifying information and regulatory details. Each record includes organizational names, business identifiers, operational status, and licensing information including DEA registrations and HIN numbers across multiple categories.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_HCO
    dimensions:
      - name: HCO_BUSINESS_NAME
        description: Healthcare organization business name.
        expr: HCO_BUSINESS_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - CSC COMMUNITY PHARMACY
          - ENDOSCOPY AND SURGICAL CARE SUITE
      - name: HCO_CD
        description: Healthcare organization codes used to identify specific healthcare entities.
        expr: HCO_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - '1438'
          - '22740441'
          - '30819949'
      - name: HCO_KEY
        description: A unique identifier key for healthcare organizations.
        expr: HCO_KEY
        data_type: VARCHAR(32)
        sample_values:
          - f40f52d91c6e993b6c7c5af4716c359b
          - 27e97cbb80be61e0fb616254104d33b3
          - a9322f73290eae0c74ca2d7b98379369
      - name: HCO_LEGAL_NAME
        description: The legal name of the healthcare organization.
        expr: HCO_LEGAL_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - BINUR, NIR, OFFICE
          - ALTAMED PHARMACY BOYLE HEIGHTS
          - ONE HEALTH GASTROENTEROLOGY AND HEPATOLOGY
      - name: HCO_NAME
        description: Names of healthcare organizations.
        expr: HCO_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - MYCARE PHARMACY
          - FERRER, OLGA M, OFFICE
          - COSHOCTON REGIONAL MEDICAL CENTER INTENSIVE CARE UNIT
      - name: HCO_P_DEA_LIC_STATUS_CD
        description: Healthcare organization Drug Enforcement Administration license status code.
        expr: HCO_P_DEA_LIC_STATUS_CD
        data_type: VARCHAR(16777216)
      - name: HCO_P_DEA_NUM
        description: Drug Enforcement Administration number for healthcare organizations.
        expr: HCO_P_DEA_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - FV5496244
          - FG5718347
      - name: HCO_P_GLN_NUM
        description: Healthcare organization primary Global Location Number (GLN).
        expr: HCO_P_GLN_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_P_HIN_NUM
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_P_HIN_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - L89YW0W00
          - 38G06QNF1
          - 8N302YY00
      - name: HCO_P_NCPDP_NUM
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_P_NCPDP_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_P_NPI_NUM
        description: Healthcare organization primary National Provider Identifier number.
        expr: HCO_P_NPI_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_S_DEA_LIC_STATUS_CD
        description: Healthcare organization Drug Enforcement Administration license status code.
        expr: HCO_S_DEA_LIC_STATUS_CD
        data_type: VARCHAR(16777216)
      - name: HCO_S_DEA_NUM
        description: Drug Enforcement Administration number for healthcare organizations.
        expr: HCO_S_DEA_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - FM3749629
          - FP5150862
      - name: HCO_S_GLN_NUM
        description: Healthcare organization Global Location Number (GLN) used for unique identification in supply chain and healthcare transactions.
        expr: HCO_S_GLN_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_S_HIN_NUM
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_S_HIN_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_S_NCPDP_NUM
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_S_NCPDP_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_S_NPI_NUM
        description: National Provider Identifier number for healthcare organizations.
        expr: HCO_S_NPI_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_SDESC
        description: Names of healthcare organizations.
        expr: HCO_SDESC
        data_type: VARCHAR(16777216)
        sample_values:
          - CASPER COLLEGE STUDENT HEALTH SERVICES
          - ROYAL SUPPLIES
          - AVERA SURGICAL ASSOCIATES ENDOSCOPY
      - name: HCO_SRC_ID_IMS
        description: Healthcare organization source identifier from IMS data system.
        expr: HCO_SRC_ID_IMS
        data_type: VARCHAR(16777216)
        sample_values:
          - WUSH00158767
          - '2705090'
          - '22740441'
      - name: HCO_SRC_ID_P_SCRUB
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_SRC_ID_P_SCRUB
        data_type: VARCHAR(16777216)
      - name: HCO_SRC_ID_P_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_SRC_ID_P_SFA
        data_type: VARCHAR(16777216)
      - name: HCO_SRC_ID_S_SCRUB
        description: Healthcare organization source identifier after data scrubbing processes.
        expr: HCO_SRC_ID_S_SCRUB
        data_type: VARCHAR(16777216)
        sample_values:
          - WUSF00046723
          - WUSE01155990
          - WUSF00100048
      - name: HCO_SRC_ID_S_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_SRC_ID_S_SFA
        data_type: VARCHAR(16777216)
      - name: HCO_SRC_ID_T_SCRUB
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_SRC_ID_T_SCRUB
        data_type: VARCHAR(16777216)
      - name: HCO_SRC_ID_T_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_SRC_ID_T_SFA
        data_type: VARCHAR(16777216)
      - name: HCO_SRC_ID_WKH
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_SRC_ID_WKH
        data_type: VARCHAR(16777216)
      - name: HCO_T_DEA_LIC_STATUS_CD
        description: Healthcare organization Drug Enforcement Administration license status code.
        expr: HCO_T_DEA_LIC_STATUS_CD
        data_type: VARCHAR(16777216)
      - name: HCO_T_DEA_NUM
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_T_DEA_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - HV00789
          - RM0314790
      - name: HCO_T_GLN_NUM
        description: Healthcare organization trading partner Global Location Number (GLN).
        expr: HCO_T_GLN_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_T_HIN_NUM
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_T_HIN_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_T_NCPDP_NUM
        description: A column holding data of type VARCHAR(16777216).
        expr: HCO_T_NCPDP_NUM
        data_type: VARCHAR(16777216)
      - name: HCO_T_NPI_NUM
        description: Healthcare organization National Provider Identifier number.
        expr: HCO_T_NPI_NUM
        data_type: VARCHAR(16777216)
    time_dimensions:
      - name: HCO_P_DEA_EXP_DT
        description: The expiration date of the healthcare organization's Drug Enforcement Administration registration.
        expr: HCO_P_DEA_EXP_DT
        data_type: DATE
      - name: HCO_S_DEA_EXP_DT
        description: The expiration date of the healthcare organization's Drug Enforcement Administration registration.
        expr: HCO_S_DEA_EXP_DT
        data_type: DATE
      - name: HCO_T_DEA_EXP_DT
        description: The expiration date of the healthcare organization's Drug Enforcement Administration registration.
        expr: HCO_T_DEA_EXP_DT
        data_type: DATE
    facts:
      - name: HCO_STATUS_KEY
        description: Healthcare organization status identifier key.
        expr: HCO_STATUS_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '1'
      - name: HCO_SUBTYPE_KEY
        description: The unique identifier for the healthcare organization subtype.
        expr: HCO_SUBTYPE_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '35'
          - '30'
          - '39'
  - name: TD_HCO_SUBTYPE
    description: The table contains records of healthcare organization subtypes and their corresponding parent types. Each record represents a specific subtype classification with descriptive information and links to broader organizational type categories.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_HCO_SUBTYPE
    dimensions:
      - name: HCO_SUBTYPE_CD
        description: Healthcare organization subtype codes that classify different types of medical entities and payer organizations.
        expr: HCO_SUBTYPE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - VA CMOP
          - PAYER_REG
          - PBM
      - name: HCO_SUBTYPE_DESC
        description: Healthcare organization subtype descriptions that categorize different types of medical entities and payers.
        expr: HCO_SUBTYPE_DESC
        data_type: VARCHAR(16777216)
        sample_values:
          - MCO PBM
          - IDN
          - MCO NATIONAL PAYER
      - name: HCO_SUBTYPE_KEY
        description: Unique identifier for healthcare organization subtypes.
        expr: HCO_SUBTYPE_KEY
        data_type: NUMBER(38,0)
        sample_values:
          - '1'
          - '2'
          - '3'
      - name: HCO_TYPE_CD
        description: Healthcare organization type code indicating the classification of the healthcare organization.
        expr: HCO_TYPE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - ORG
          - MCO
      - name: HCO_TYPE_DESC
        description: Healthcare organization type description indicating the category or classification of the healthcare entity.
        expr: HCO_TYPE_DESC
        data_type: VARCHAR(16777216)
        sample_values:
          - ORGANIZATION
          - MANAGED CARE ORGANIZATION
      - name: HCO_TYPE_KEY
        description: A unique identifier for healthcare organization types.
        expr: HCO_TYPE_KEY
        data_type: NUMBER(38,0)
        sample_values:
          - '1'
          - '2'
    primary_key:
      columns:
        - HCO_SUBTYPE_KEY
  - name: TD_HCP
    description: The table contains records of healthcare professionals and their personal and professional information. Each record represents an individual healthcare provider and includes demographic details, contact information, educational background, and various status indicators related to communication preferences and professional classifications.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_HCP
    dimensions:
      - name: AMA_NO_CONTACT
        description: American Medical Association no contact indicator.
        expr: AMA_NO_CONTACT
        data_type: VARCHAR(1)
        sample_values:
          - 'N'
          - 'Y'
      - name: EMAIL_ADDR_P
        description: Email addresses for healthcare professionals.
        expr: EMAIL_ADDR_P
        data_type: VARCHAR(16777216)
        sample_values:
          - ATELIS1150730@WM.PROVIDENCEDIRECT.ORG
          - KATYDM7@HOTMAIL.COM
          - GRANTJSULLIVAN@ATT.NET
      - name: EMAIL_ADDR_S
        description: Email addresses of healthcare providers.
        expr: EMAIL_ADDR_S
        data_type: VARCHAR(16777216)
        sample_values:
          - T.THOMAS@ARCHILDRENS.ORG
          - EHUANG@POST.HARVARD.EDU
          - CGOODWIN@UMC.EDU
      - name: EMAIL_ADDR_T
        description: Email addresses for healthcare providers.
        expr: EMAIL_ADDR_T
        data_type: VARCHAR(16777216)
      - name: HCP_DEGREE
        description: The degree or credential held by the healthcare professional.
        expr: HCP_DEGREE
        data_type: VARCHAR(16777216)
        sample_values:
          - PHT
          - RPH
          - NP
      - name: HCP_FIRST_NAME
        description: The first name of the healthcare provider.
        expr: HCP_FIRST_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - SHANE
          - ROYAL
          - CASIANNE
      - name: HCP_GENDER
        description: The gender of the healthcare provider.
        expr: HCP_GENDER
        data_type: VARCHAR(255)
        sample_values:
          - FEMALE
          - UNKNOWN
      - name: HCP_GRAD_YR
        description: The year when the healthcare provider graduated from their medical program.
        expr: HCP_GRAD_YR
        data_type: VARCHAR(16777216)
        sample_values:
          - '1967'
          - '2019'
          - '1965'
      - name: HCP_KEY
        description: A unique identifier for healthcare providers in the system.
        expr: HCP_KEY
        data_type: VARCHAR(32)
        sample_values:
          - 10f535a03919512d15df722bdf50f8fe
          - 59c026c2a4da8798332fd8140066c866
          - cce70a032d441fc7cf17357da57ffa3e
      - name: HCP_LAST_NAME
        description: Healthcare provider last names.
        expr: HCP_LAST_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - FULP
          - GERBER
          - DIGGINS
      - name: HCP_ME_NUM
        description: Healthcare provider medical education number used for identification purposes.
        expr: HCP_ME_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - '4950901017'
          - '0230178074'
      - name: HCP_MID_NAME
        description: Middle names of healthcare providers.
        expr: HCP_MID_NAME
        data_type: VARCHAR(16777216)
        sample_values:
          - ANN
          - DARLENE
          - TUCKER
      - name: HCP_P_DEA_BUSINESS_ACTIVITY_CD
        description: Healthcare provider Drug Enforcement Administration business activity code.
        expr: HCP_P_DEA_BUSINESS_ACTIVITY_CD
        data_type: VARCHAR(16777216)
      - name: HCP_P_DEA_DRUG_SCHEDULE
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_P_DEA_DRUG_SCHEDULE
        data_type: VARCHAR(16777216)
      - name: HCP_P_DEA_LIC_STATUS_CD
        description: The status code indicating whether a healthcare provider's Drug Enforcement Administration license is active or inactive.
        expr: HCP_P_DEA_LIC_STATUS_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - ACTIVE
      - name: HCP_P_DEA_NUM
        description: Drug Enforcement Administration number assigned to healthcare providers for prescribing controlled substances.
        expr: HCP_P_DEA_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - AB7255272
          - BB3188833
      - name: HCP_P_DEA_STATE_OF_LICENSURE
        description: The state where the healthcare provider's Drug Enforcement Administration license is issued.
        expr: HCP_P_DEA_STATE_OF_LICENSURE
        data_type: VARCHAR(16777216)
        sample_values:
          - TX
          - MA
          - LA
      - name: HCP_P_HIN
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_P_HIN
        data_type: VARCHAR(16777216)
      - name: HCP_P_MA_ID
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_P_MA_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - '216513'
          - '166691'
          - '169135'
      - name: HCP_P_NPI_NUM
        description: National Provider Identifier (NPI) numbers for healthcare providers.
        expr: HCP_P_NPI_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - '1134821382'
          - '1235525957'
          - '1558956375'
      - name: HCP_P_STATE_LIC_NUM
        description: Healthcare provider state license numbers.
        expr: HCP_P_STATE_LIC_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - PA2022-0048
          - '107955'
          - '238052'
      - name: HCP_P_STATE_LIC_STATE_CD
        description: State code where the healthcare provider holds their professional license.
        expr: HCP_P_STATE_LIC_STATE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - AR
          - NV
          - TX
      - name: HCP_P_STATE_LIC_STATUS
        description: The status of a healthcare provider's professional state license.
        expr: HCP_P_STATE_LIC_STATUS
        data_type: VARCHAR(16777216)
        sample_values:
          - INACTIVE
          - ACTIVE
      - name: HCP_PDRP_OPTOUT_IND
        description: Healthcare provider Patient Data Request Program opt-out indicator.
        expr: HCP_PDRP_OPTOUT_IND
        data_type: VARCHAR(16777216)
        sample_values:
          - 'Y'
          - 'N'
      - name: HCP_PHONE_HOME_P
        description: Healthcare provider's home phone number.
        expr: HCP_PHONE_HOME_P
        data_type: VARCHAR(16777216)
      - name: HCP_PHONE_MOBILE_P
        description: Mobile phone number for the healthcare provider.
        expr: HCP_PHONE_MOBILE_P
        data_type: VARCHAR(16777216)
      - name: HCP_PHONE_WORK_P
        description: Work phone numbers for healthcare providers.
        expr: HCP_PHONE_WORK_P
        data_type: VARCHAR(16777216)
        sample_values:
          - '3604862900'
          - '2183486888'
          - '9155323937'
      - name: HCP_S_DEA_BUSINESS_ACTIVITY_CD
        description: Drug Enforcement Administration business activity code for healthcare providers.
        expr: HCP_S_DEA_BUSINESS_ACTIVITY_CD
        data_type: VARCHAR(16777216)
      - name: HCP_S_DEA_DRUG_SCHEDULE
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_S_DEA_DRUG_SCHEDULE
        data_type: VARCHAR(16777216)
      - name: HCP_S_DEA_LIC_STATUS_CD
        description: Healthcare provider Drug Enforcement Administration license status code.
        expr: HCP_S_DEA_LIC_STATUS_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - ACTIVE
      - name: HCP_S_DEA_NUM
        description: Drug Enforcement Administration number assigned to healthcare providers for prescribing controlled substances.
        expr: HCP_S_DEA_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - FG6124919
          - FF8289161
      - name: HCP_S_DEA_STATE_OF_LICENSURE
        description: The state where the healthcare provider's Drug Enforcement Administration license was issued.
        expr: HCP_S_DEA_STATE_OF_LICENSURE
        data_type: VARCHAR(16777216)
        sample_values:
          - VA
          - NE
      - name: HCP_S_HIN
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_S_HIN
        data_type: VARCHAR(16777216)
      - name: HCP_S_MA_ID
        description: Healthcare provider master agreement identifier.
        expr: HCP_S_MA_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - '381767'
          - '808700'
          - '469231'
      - name: HCP_S_NPI_NUM
        description: National Provider Identifier number for healthcare providers.
        expr: HCP_S_NPI_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - '1003443490'
          - '1396321816'
          - '1295437895'
      - name: HCP_S_STATE_LIC_NUM
        description: State license numbers for healthcare providers.
        expr: HCP_S_STATE_LIC_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - RN528086L
          - 053978-23
          - '22566'
      - name: HCP_S_STATE_LIC_STATE_CD
        description: State code where the healthcare provider holds their professional license.
        expr: HCP_S_STATE_LIC_STATE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - CT
          - WI
      - name: HCP_S_STATE_LIC_STATUS
        description: The status of a healthcare provider's state license.
        expr: HCP_S_STATE_LIC_STATUS
        data_type: VARCHAR(16777216)
        sample_values:
          - PROBATION
          - ACTIVE
      - name: HCP_SRC_ID_10_IMS2
        description: Healthcare provider source identifier from IMS database system.
        expr: HCP_SRC_ID_10_IMS2
        data_type: VARCHAR(16777216)
        sample_values:
          - '7692967'
          - '24968143'
          - '8098879'
      - name: HCP_SRC_ID_3_SCRUB
        description: Healthcare provider source identifier from the third data source after data scrubbing processes.
        expr: HCP_SRC_ID_3_SCRUB
        data_type: VARCHAR(16777216)
        sample_values:
          - WUSM01621661
          - WUSR06017841
          - WUSP00548392
      - name: HCP_SRC_ID_4_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_4_SFA
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_5_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_5_SFA
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_CF_SPDI
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_CF_SPDI
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_IMS
        description: Healthcare provider source identifier from IMS data system.
        expr: HCP_SRC_ID_IMS
        data_type: VARCHAR(16777216)
        sample_values:
          - '7182700'
          - '1292510'
          - '2851684'
      - name: HCP_SRC_ID_OH_TDDD
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_OH_TDDD
        data_type: VARCHAR(16777216)
        sample_values:
          - '020363900'
          - '021366000'
      - name: HCP_SRC_ID_P_CLIENT
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_P_CLIENT
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_P_COPROMO
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_P_COPROMO
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_P_SCRUB
        description: Healthcare provider source identifier after data scrubbing processes.
        expr: HCP_SRC_ID_P_SCRUB
        data_type: VARCHAR(16777216)
        sample_values:
          - '13322122'
          - '15733078'
      - name: HCP_SRC_ID_P_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_P_SFA
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_P_SP
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_P_SP
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_S_CLIENT
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_S_CLIENT
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_S_COPROMO
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_S_COPROMO
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_S_SCRUB
        description: Healthcare provider source identifier after data scrubbing processes.
        expr: HCP_SRC_ID_S_SCRUB
        data_type: VARCHAR(16777216)
        sample_values:
          - WUSM05200310
          - WUSM04669326
          - WUSM00017540
      - name: HCP_SRC_ID_S_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_S_SFA
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_S_SP
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_S_SP
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_SHA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_SHA
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_T_COPROMO
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_T_COPROMO
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_T_SFA
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_T_SFA
        data_type: VARCHAR(16777216)
      - name: HCP_SRC_ID_T_SP
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_SRC_ID_T_SP
        data_type: VARCHAR(16777216)
      - name: HCP_SUFFIX
        description: Healthcare provider name suffix such as senior or junior designations.
        expr: HCP_SUFFIX
        data_type: VARCHAR(16777216)
        sample_values:
          - JR
          - II
          - SR
      - name: HCP_T_DEA_BUSINESS_ACTIVITY_CD
        description: Drug Enforcement Administration business activity code for healthcare providers.
        expr: HCP_T_DEA_BUSINESS_ACTIVITY_CD
        data_type: VARCHAR(16777216)
      - name: HCP_T_DEA_DRUG_SCHEDULE
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_T_DEA_DRUG_SCHEDULE
        data_type: VARCHAR(16777216)
      - name: HCP_T_DEA_LIC_STATUS_CD
        description: Healthcare provider Drug Enforcement Administration license status code.
        expr: HCP_T_DEA_LIC_STATUS_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - ACTIVE
      - name: HCP_T_DEA_NUM
        description: Drug Enforcement Administration number for healthcare providers.
        expr: HCP_T_DEA_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - FK9444655
          - FB3199975
      - name: HCP_T_DEA_STATE_OF_LICENSURE
        description: The state where the healthcare provider's Drug Enforcement Administration license was issued.
        expr: HCP_T_DEA_STATE_OF_LICENSURE
        data_type: VARCHAR(16777216)
        sample_values:
          - SD
          - TN
      - name: HCP_T_HIN
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_T_HIN
        data_type: VARCHAR(16777216)
      - name: HCP_T_MA_ID
        description: A column holding data of type VARCHAR(16777216).
        expr: HCP_T_MA_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - '634359'
          - '526408'
      - name: HCP_T_NPI_NUM
        description: National Provider Identifier number for healthcare providers.
        expr: HCP_T_NPI_NUM
        data_type: VARCHAR(16777216)
      - name: HCP_T_STATE_LIC_NUM
        description: Healthcare provider state license numbers.
        expr: HCP_T_STATE_LIC_NUM
        data_type: VARCHAR(16777216)
        sample_values:
          - '0810007329'
          - '23410'
      - name: HCP_T_STATE_LIC_STATE_CD
        description: The state code where the healthcare provider holds their license.
        expr: HCP_T_STATE_LIC_STATE_CD
        data_type: VARCHAR(16777216)
        sample_values:
          - MD
          - FL
      - name: HCP_T_STATE_LIC_STATUS
        description: The status of a healthcare provider's state license.
        expr: HCP_T_STATE_LIC_STATUS
        data_type: VARCHAR(16777216)
        sample_values:
          - INACTIVE
          - ACTIVE
      - name: HCP_TITLE
        description: Healthcare provider professional title or designation.
        expr: HCP_TITLE
        data_type: VARCHAR(16777216)
      - name: KOL_IND
        description: Key opinion leader indicator flag.
        expr: KOL_IND
        data_type: VARCHAR(16777216)
      - name: NO_SEE_IND
        description: A column holding data of type VARCHAR(16777216).
        expr: NO_SEE_IND
        data_type: VARCHAR(16777216)
      - name: SAMPLEABILITY_IND
        description: An indicator of whether the healthcare provider can be sampled.
        expr: SAMPLEABILITY_IND
        data_type: VARCHAR(16777216)
    time_dimensions:
      - name: HCP_BIRTH_DT
        description: Healthcare provider birth dates.
        expr: HCP_BIRTH_DT
        data_type: DATE
        sample_values:
          - '1968-01-01'
          - '1971-01-01'
          - '1978-01-01'
      - name: HCP_P_DEA_EXP_DT
        description: The expiration date of the healthcare provider's Drug Enforcement Administration registration.
        expr: HCP_P_DEA_EXP_DT
        data_type: DATE
        sample_values:
          - '2028-01-31'
          - '2028-05-31'
          - '2027-02-28'
      - name: HCP_P_STATE_LIC_EXP_DT
        description: The expiration date of the healthcare provider's state license.
        expr: HCP_P_STATE_LIC_EXP_DT
        data_type: DATE
        sample_values:
          - '2027-08-31'
          - '2028-08-31'
          - '2025-11-30'
      - name: HCP_P_STATE_LIC_ISSUE_DT
        description: The date when the healthcare provider's state license was issued.
        expr: HCP_P_STATE_LIC_ISSUE_DT
        data_type: DATE
        sample_values:
          - '2022-07-01'
          - '2020-02-13'
          - '2024-06-23'
      - name: HCP_PDRP_OPTOUT_DT
        description: The date when a healthcare provider opted out of the Prescription Drug Reporting Program.
        expr: HCP_PDRP_OPTOUT_DT
        data_type: DATE
        sample_values:
          - '2008-12-04'
          - '2008-11-27'
      - name: HCP_S_DEA_EXP_DT
        description: The expiration date of the healthcare provider's Drug Enforcement Administration registration.
        expr: HCP_S_DEA_EXP_DT
        data_type: DATE
        sample_values:
          - '2026-07-31'
          - '2026-12-31'
          - '2026-03-31'
      - name: HCP_S_STATE_LIC_EXP_DT
        description: The expiration date of the healthcare provider's state license.
        expr: HCP_S_STATE_LIC_EXP_DT
        data_type: DATE
        sample_values:
          - '2026-04-05'
          - '2026-02-04'
          - '2027-03-01'
      - name: HCP_S_STATE_LIC_ISSUE_DT
        description: The date when the healthcare provider's state license was issued.
        expr: HCP_S_STATE_LIC_ISSUE_DT
        data_type: DATE
        sample_values:
          - '2019-04-23'
          - '2021-05-14'
      - name: HCP_T_DEA_EXP_DT
        description: The expiration date of the healthcare provider's Drug Enforcement Administration registration.
        expr: HCP_T_DEA_EXP_DT
        data_type: DATE
        sample_values:
          - '2028-04-30'
          - '2026-08-31'
          - '2027-10-31'
      - name: HCP_T_STATE_LIC_EXP_DT
        description: The expiration date of the healthcare provider's state license.
        expr: HCP_T_STATE_LIC_EXP_DT
        data_type: DATE
        sample_values:
          - '2026-10-31'
          - '2025-12-31'
          - '2026-09-30'
      - name: HCP_T_STATE_LIC_ISSUE_DT
        description: The date when the healthcare provider's state license was issued.
        expr: HCP_T_STATE_LIC_ISSUE_DT
        data_type: DATE
        sample_values:
          - '2002-05-08'
          - '2022-02-18'
      - name: SAMPLEABILITY_DT
        description: The date when sampleability status was determined or last updated.
        expr: SAMPLEABILITY_DT
        data_type: DATE
    facts:
      - name: HCP_STATUS_KEY
        description: Healthcare provider status identifier used as a foreign key reference.
        expr: HCP_STATUS_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '1'
      - name: HCP_SUBTYPE_KEY
        description: Healthcare provider subtype identifier key.
        expr: HCP_SUBTYPE_KEY
        data_type: NUMBER(38,0)
        access_modifier: public_access
        sample_values:
          - '9'
          - '10'
          - '13'
    primary_key:
      columns:
        - HCP_KEY
  - name: TD_PROD_BRAND
    description: The table contains records of product brand and categorization information used for sales performance reporting. Each record represents a product hierarchy structure that includes market segments, category groupings, individual categories, family groupings, and product families with their corresponding identifiers and descriptions.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TD_PROD_BRAND
    dimensions:
      - name: PROD_BRAND_CD
        description: Product brand codes for pharmaceutical and medical products.
        expr: PROD_BRAND_CD
        data_type: VARCHAR(256)
        sample_values:
          - ANOLOR 300
          - CHILDRENS Q-PAP
          - TENCON
      - name: PROD_BRAND_DESC
        description: Brand names of pharmaceutical pain management products along with their associated market categories.
        expr: PROD_BRAND_DESC
        data_type: VARCHAR(256)
        sample_values:
          - BUFFERED ASPIRIN EXTRA S BRAND (VERTEX ACUTE MARKET)
          - LIDOPRIL XR BRAND (ACUTE HEOR MARKET)
          - RA ORASOL BRAND (VERTEX ACUTE MARKET)
      - name: PROD_BRAND_KEY
        description: A unique identifier key for product brands.
        expr: PROD_BRAND_KEY
        data_type: VARCHAR(32)
        sample_values:
          - a96f921f9adcf6357022ae1d77e0036e
          - 77238a2664c2da30f3763020cc9f64a1
          - a3013ea55958b130a8be56287a23ac84
      - name: PROD_BRAND_SDESC
        description: Product brand short description or name.
        expr: PROD_BRAND_SDESC
        data_type: VARCHAR(256)
        sample_values:
          - RECTASMOOTHE
          - LIDOVIX L
          - NUVAKAAN-II
      - name: PROD_BRAND_UCD
        description: Brand names of pharmaceutical and over-the-counter medical products.
        expr: PROD_BRAND_UCD
        data_type: VARCHAR(256)
        sample_values:
          - CAPSAICIN TOPICAL PAIN P
          - PAIN-EZE PLUS
          - PRILO PATCH II
      - name: PROD_CAT_CD
        description: Pharmaceutical product category codes indicating the therapeutic classification of medical products.
        expr: PROD_CAT_CD
        data_type: VARCHAR(256)
        sample_values:
          - DERMATOLOGICAL TOPICAL ANESTHETIC COMBINATIONS
          - LIDOCAINE - ANESTHETIC
          - AMIDES
      - name: PROD_CAT_DESC
        description: Product category descriptions for pharmaceutical pain management treatments.
        expr: PROD_CAT_DESC
        data_type: VARCHAR(256)
        sample_values:
          - ANALGESICS-SEDATIVE COMBINATION CATEGORY (ANY PAIN MARKET)
          - LIDOCAINE - ANESTHETIC CATEGORY (VERTEX ACUTE MARKET)
          - DENTAL LOCAL ANESTHETIC CATEGORY (ANY PAIN MARKET)
      - name: PROD_CAT_KEY
        description: A unique identifier key for product categories.
        expr: PROD_CAT_KEY
        data_type: VARCHAR(32)
        sample_values:
          - 12bc423a463b94db26c110a7b5f35c2b
          - 51d89477c08fab581269b5cd07cc9249
          - 6cdf4a9156745f9ceab6a9d55493f62f
      - name: PROD_CAT_SDESC
        description: Product category short descriptions for pharmaceutical and medical products.
        expr: PROD_CAT_SDESC
        data_type: VARCHAR(256)
        sample_values:
          - RHEUMATOID ARTHRITIS
          - DERMATOLOGICAL TOPICAL ANESTHETIC
          - SSRI
      - name: PROD_CATGRP_CD
        description: Product category group codes representing different pharmaceutical and medical product classifications.
        expr: PROD_CATGRP_CD
        data_type: VARCHAR(256)
        sample_values:
          - OSTEOARTHRITIS
          - ANESTHETIC
          - DENTALS
      - name: PROD_CATGRP_DESC
        description: Product category group descriptions for pharmaceutical products in acute care markets.
        expr: PROD_CATGRP_DESC
        data_type: VARCHAR(256)
        sample_values:
          - OTHER CATEGORY GROUP (ANY PAIN MARKET)
          - DERMATOLOGICAL CATEGORY GROUP (VERTEX ACUTE MARKET)
          - NON-NARCOTIC ANALGESIC CATEGORY GROUP (VERTEX ACUTE MARKET)
      - name: PROD_CATGRP_KEY
        description: A unique identifier key for product category groups.
        expr: PROD_CATGRP_KEY
        data_type: VARCHAR(32)
        sample_values:
          - ae4d74c1255ef3b525bf3ee4914ed273
          - a9946aba572bb08c111e94c3c36c84f7
          - d8cad3b14468ba5121e210028e133b00
      - name: PROD_CATGRP_SDESC
        description: Product category group short description indicating the therapeutic classification or type of pharmaceutical product.
        expr: PROD_CATGRP_SDESC
        data_type: VARCHAR(256)
        sample_values:
          - ANESTHETIC
          - DENTALS
          - NON-NARCOTIC ANALGESIC
      - name: PROD_FAM_CD
        description: Product family codes used to categorize and group related products.
        expr: PROD_FAM_CD
        data_type: VARCHAR(256)
        sample_values:
          - HYCOGESIC
          - PAMPRIN
          - ENDOLOR
      - name: PROD_FAM_DESC
        description: Product family descriptions for pain relief medications and treatments.
        expr: PROD_FAM_DESC
        data_type: VARCHAR(256)
        sample_values:
          - EFFEXOR XR FAMILY (ACUTE HEOR MARKET)
          - ENDOLOR FAMILY (ANY PAIN MARKET)
          - DYNAMIC FAMILY (ANY PAIN MARKET)
      - name: PROD_FAM_KEY
        description: A unique identifier key for product families.
        expr: PROD_FAM_KEY
        data_type: VARCHAR(32)
        sample_values:
          - 10738f1513e6f471c10da189c7b6db7c
          - 5e208a4effc1cd0786bed71851c6964f
          - 1ab075c96c0468e9796a665631ad472e
      - name: PROD_FAM_SDESC
        description: Pharmaceutical product family short descriptions or names.
        expr: PROD_FAM_SDESC
        data_type: VARCHAR(256)
        sample_values:
          - CHLORASEPTIC SR THRT
          - CHILDRENS Q-PAP
          - SYNOFLEX
      - name: PROD_FAMGRP_CD
        description: Active pharmaceutical ingredients or drug compounds used in pain management products.
        expr: PROD_FAMGRP_CD
        data_type: VARCHAR(256)
        sample_values:
          - CAPSAICIN-MENTHOL - DERMATOLOGICAL TOPICAL ANESTHETIC COMBINATIONS
          - HYDROCODONE
          - BENZOCAINE-ZINC CHLORIDE
      - name: PROD_FAMGRP_DESC
        description: Pharmaceutical product family group descriptions indicating drug types and their associated pain market categories.
        expr: PROD_FAMGRP_DESC
        data_type: VARCHAR(256)
        sample_values:
          - CAPSAICIN FAMILY GROUP (ANY PAIN MARKET)
          - BENZOCAINE-ZINC CHLORIDE FAMILY GROUP (ANY PAIN MARKET)
          - OXYCODONE COMBINATIONS FAMILY GROUP (ACUTE HEOR MARKET)
      - name: PROD_FAMGRP_KEY
        description: A unique identifier key for product family groups represented as a hash value.
        expr: PROD_FAMGRP_KEY
        data_type: VARCHAR(32)
        sample_values:
          - e86d9067df720cbe38d87f1d1e6ba4bb
          - 2d8609830dfd91355e0f57b69a124265
          - 2c25349bf27be50f2527bb0084f0f4d2
      - name: PROD_FAMGRP_SDESC
        description: Pharmaceutical product family group short descriptions indicating the active ingredient or drug category.
        expr: PROD_FAMGRP_SDESC
        data_type: VARCHAR(256)
        sample_values:
          - DESIPRAMINE
          - BENZOCAINE - DERMATOLOGICAL TOPICAL ANESTHETIC COMBINATIONS
          - LIDOCAINE - DERMATOLOGICAL TOPICAL ANESTHETIC COMBINATIONS
      - name: PROD_MKT_CD
        description: Product marketing code identifying the specific marketing category or segment for the product brand.
        expr: PROD_MKT_CD
        data_type: VARCHAR(256)
        sample_values:
          - ACUTE HEOR
          - ANY PAIN
          - VERTEX ACUTE
      - name: PROD_MKT_DESC
        description: Product market description indicating the specific market segment or category for the product.
        expr: PROD_MKT_DESC
        data_type: VARCHAR(256)
        sample_values:
          - VERTEX ACUTE MARKET
          - ACUTE HEOR MARKET
          - ANY PAIN MARKET
      - name: PROD_MKT_KEY
        description: A unique marketing key identifier for products.
        expr: PROD_MKT_KEY
        data_type: VARCHAR(32)
        sample_values:
          - da0dacac9831661226335a3eb0b04274
          - 89cf3596760b4a79df67736f2272a1b7
          - c8c606d1c69b2144eea5bbe3157289b6
      - name: PROD_MKT_SDESC
        description: Product marketing short description for pharmaceutical pain management brands.
        expr: PROD_MKT_SDESC
        data_type: VARCHAR(256)
        sample_values:
          - ACUTE HEOR
          - ANY PAIN
          - VERTEX ACUTE
    primary_key:
      columns:
        - PROD_BRAND_KEY
  - name: TM_ARD_HCP_ACUTE_MKT_WEEKLY_XPO
    description: The table contains records of healthcare provider prescription activity in acute care settings, tracked on a weekly basis. Each record represents prescription exposure data for a specific healthcare provider and includes provider identifiers, specialty information, territorial assignments, and prescription metrics for acute care treatments.
    base_table:
      database: DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
      schema: REPORTING
      table: TM_ARD_HCP_ACUTE_MKT_WEEKLY_XPO
    dimensions:
      - name: BRANDED_FLAG
        description: A flag indicating whether the product is branded or not.
        expr: BRANDED_FLAG
        data_type: VARCHAR(16777216)
        sample_values:
          - 'N'
          - 'Y'
      - name: GOLDEN_ID
        description: A unique identifier used to link and track healthcare providers across different systems and databases.
        expr: GOLDEN_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - GLD000600060001002101
          - GLD999900180020002401
          - GLD021402140287037101
      - name: HCP_ID
        description: Unique identifier for healthcare providers.
        expr: HCP_ID
        data_type: VARCHAR(32)
        sample_values:
          - b53fb88ce1b284c55206728afb2a9da6
          - e32c570ee80606057196f62cf5b6e502
          - 132c4377c0b6eee2172aefb7d046b30e
      - name: HCP_SPECIALTY
        description: Medical specialty or practice area of the healthcare provider.
        expr: HCP_SPECIALTY
        data_type: VARCHAR(16777216)
        sample_values:
          - ADULT RECONSTRUCTIVE ORTHOPEDICS
          - EMERGENCY MEDICINE
          - DENTIST
      - name: HCP_SPECIALTY_GROUP
        description: Healthcare provider specialty groups categorizing medical professionals by their area of practice.
        expr: HCP_SPECIALTY_GROUP
        data_type: VARCHAR(16777216)
        sample_values:
          - MEDICINE
          - PHYSICIAN EXTENDERS
          - ORTHOPEDICS AND SPORTS MEDICINE
      - name: NPI
        description: National Provider Identifier numbers for healthcare providers.
        expr: NPI
        data_type: VARCHAR(16777216)
        sample_values:
          - '1194281006'
          - '1659465573'
          - '1396203352'
      - name: ONEKEY_HCP_ID
        description: Unique identifier for healthcare providers in the OneKey system.
        expr: ONEKEY_HCP_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - WUSD00201756
          - WUSM00530772
          - WUSM00311233
      - name: PRODUCT_CATEGORY
        description: Categories of pharmaceutical products used for pain management and treatment.
        expr: PRODUCT_CATEGORY
        data_type: VARCHAR(256)
        sample_values:
          - NSAID
          - STRONG OPIOID AGONIST
          - ANTICONVULSANTS MISC.
      - name: PRODUCT_FAMILY_GROUP
        description: Pharmaceutical product family groups categorizing related medications and therapeutic compounds.
        expr: PRODUCT_FAMILY_GROUP
        data_type: VARCHAR(256)
        sample_values:
          - LIDOCAINE - DERMATOLOGICAL LOCAL TOPICAL ANESTHETIC
          - MELOXICAM
          - HYDROCODONE COMBINATIONS
      - name: PTAM_TERRITORY_ID
        description: Unique identifier for a PTAM (Pain Territory Account Manager) territory.
        expr: PTAM_TERRITORY_ID
        data_type: VARCHAR(255)
        sample_values:
          - V1NAUSA-5733101
          - V1NAUSA-5733602
          - V1NAUSA-5733604
      - name: ROA
        description: Route of administration for pharmaceutical products.
        expr: ROA
        data_type: VARCHAR(256)
        sample_values:
          - ORAL
          - TOPICAL
          - TOPICAL-PATCH
      - name: SAL_TERRITORY_ID
        description: Sales territory identifier used to designate specific geographic or organizational sales regions.
        expr: SAL_TERRITORY_ID
        data_type: VARCHAR(255)
        sample_values:
          - V1NAUSA-5720008
          - V1NAUSA-5720014
      - name: SITE_OF_CARE_ZIP_CODE
        description: ZIP codes for healthcare facility locations.
        expr: SITE_OF_CARE_ZIP_CODE
        data_type: VARCHAR(16777216)
        sample_values:
          - '73118'
          - '43228'
          - '37027'
      - name: VERTEX_ID
        description: Unique identifier for a vertex in the healthcare provider network.
        expr: VERTEX_ID
        data_type: VARCHAR(16777216)
        sample_values:
          - '15670811'
          - '14499271'
      - name: ZIP_CODE
        description: Postal ZIP codes for geographic locations.
        expr: ZIP_CODE
        data_type: VARCHAR(16777216)
        sample_values:
          - '20850'
          - '33912'
          - '90262'
    time_dimensions:
      - name: DATE_KEY
        description: The date associated with each record in the healthcare provider acute market weekly exposure data.
        expr: DATE_KEY
        data_type: DATE
        sample_values:
          - '2024-05-17'
          - '2024-12-06'
          - '2025-02-07'
    facts:
      - name: ACUTE_NRX_XPO
        description: Acute new prescription exposure index for healthcare providers in the market.
        expr: ACUTE_NRX_XPO
        data_type: NUMBER(38,10)
        access_modifier: public_access
        sample_values:
          - '2.0320000000'
          - '0.9650000000'
          - '1.0420000000'
      - name: ACUTE_TRX_XPO
        description: Acute transaction exposure metric for healthcare providers in the market.
        expr: ACUTE_TRX_XPO
        data_type: NUMBER(38,10)
        access_modifier: public_access
        sample_values:
          - '1.6080000000'
          - '1.0300000000'
          - '0.9630000000'
      - name: ACUTE_TX_DAYS_XPO
        description: The number of days of acute treatment exposure.
        expr: ACUTE_TX_DAYS_XPO
        data_type: NUMBER(38,10)
        access_modifier: public_access
        sample_values:
          - '4.2033678514'
          - '3.2592136056'
          - '24.4437444312'
      - name: ACUTE_UNITS_XPO
        description: Acute care units exposure metric.
        expr: ACUTE_UNITS_XPO
        data_type: NUMBER(38,10)
        access_modifier: public_access
        sample_values:
          - '61.1900000000'
          - '160.0900000000'
          - '30.0000000000'
      - name: WEEK_DT
        description: The date representing a specific week in the healthcare provider acute market reporting period.
        expr: WEEK_DT
        data_type: NUMBER(10,0)
        access_modifier: public_access
        sample_values:
          - '20251024'
          - '20241220'
          - '20250321'
relationships:
  # Relationship between Weekly LAAD Facts and HCP Dimension
  - name: laad_weekly_to_hcp
    left_table: ARD_GIACO_TM_HCP_ACUTE_MKT_WEEKLY_LAAD
    right_table: TD_HCP
    relationship_columns:
      - left_column: HCP_ID
        right_column: HCP_KEY
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Weekly Exposure (XPO) Facts and HCP Dimension
  - name: xpo_weekly_to_hcp
    left_table: TM_ARD_HCP_ACUTE_MKT_WEEKLY_XPO
    right_table: TD_HCP
    relationship_columns:
      - left_column: HCP_ID
        right_column: HCP_KEY
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Weekly LAAD Facts and Postal Geography
  # Connecting via ZIP Code as the common identifier
  - name: laad_weekly_to_postal
    left_table: ARD_GIACO_TM_HCP_ACUTE_MKT_WEEKLY_LAAD
    right_table: TD_GEO_POSTAL
    relationship_columns:
      - left_column: ZIP_CODE
        right_column: GEO_POSTAL_CD
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Weekly Exposure (XPO) Facts and Postal Geography
  # Connecting via ZIP Code as the common identifier
  - name: xpo_weekly_to_postal
    left_table: TM_ARD_HCP_ACUTE_MKT_WEEKLY_XPO
    right_table: TD_GEO_POSTAL
    relationship_columns:
      - left_column: ZIP_CODE
        right_column: GEO_POSTAL_CD
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Cities and States (Geographic Hierarchy)
  - name: city_to_state
    left_table: TD_GEO_CITY
    right_table: TD_GEO_STATE
    relationship_columns:
      - left_column: GEO_STATE_KEY
        right_column: GEO_STATE_KEY
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Postal Codes and States (Geographic Hierarchy)
  - name: postal_to_state
    left_table: TD_GEO_POSTAL
    right_table: TD_GEO_STATE
    relationship_columns:
      - left_column: GEO_STATE_KEY
        right_column: GEO_STATE_KEY
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Specific Locations and Cities (Geographic Hierarchy)
  - name: location_to_city
    left_table: TD_GEO_LOC
    right_table: TD_GEO_CITY
    relationship_columns:
      - left_column: GEO_CITY_KEY
        right_column: GEO_CITY_KEY
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Specific Locations and Postal Codes (Geographic Hierarchy)
  - name: location_to_postal
    left_table: TD_GEO_LOC
    right_table: TD_GEO_POSTAL
    relationship_columns:
      - left_column: GEO_POSTAL_KEY
        right_column: GEO_POSTAL_KEY
    join_type: left_outer
    relationship_type: many_to_one

  # Relationship between Specific Locations and States (Geographic Hierarchy)
  - name: location_to_state
    left_table: TD_GEO_LOC
    right_table: TD_GEO_STATE
    relationship_columns:
      - left_column: GEO_STATE_KEY
        right_column: GEO_STATE_KEY
    join_type: left_outer
    relationship_type: many_to_one
def main(session: Session, user_query: str, parent_node_ids_json: str, tree_id: str = ''Pharma_Master_v1''):
    try:
        log_debug(session, "MAIN_ENTRY", f"TreeID: {tree_id}, Query: {user_query}")

        # 1. Load Decision Tree Structure
        tree_row = session.sql("SELECT GRAPH_JSON FROM DECISION_TREE_STORE_NEW WHERE TREE_ID = ?", params=[tree_id]).collect()
        
        if not tree_row: 
            return json.dumps({"error": f"Tree ID not found: {tree_id}"})
        
        full_json = json.loads(tree_row[0][''GRAPH_JSON''])
        graph = full_json.get(''graph'', full_json)
        
        # Build Lookup Maps
        all_nodes_map = {n[''id'']: n for n in graph.get(''nodes'', [])}
        name_to_node_map = {n[''metric'']: n for n in graph.get(''nodes'', [])}
        all_edges = graph.get(''edges'', [])

        # 2. Determine Strategy (Search vs Drill)
        targets_phase_1 = []
        
        # Robust Null Check
        is_drill_mode = (
            parent_node_ids_json is not None and 
            parent_node_ids_json != '''' and 
            parent_node_ids_json.upper() != ''NULL'' and 
            parent_node_ids_json != ''[]''
        )

        if not is_drill_mode and user_query:
            log_debug(session, "STRATEGY", "Vector Search (No parent IDs provided)")
            # Vector Search for Entry Point
            vec_sql = """
            SELECT METRIC_NAME FROM KPI_KNOWLEDGE_BASE_NEW 
            ORDER BY VECTOR_L2_DISTANCE(SNOWFLAKE.CORTEX.EMBED_TEXT_768(''snowflake-arctic-embed-m-v1.5'', ?), KPI_EMBEDDING) ASC 
            LIMIT 1
            """
            kb_row = session.sql(vec_sql, params=[user_query]).collect()
            if kb_row and kb_row[0][''METRIC_NAME''] in name_to_node_map:
                found_metric = kb_row[0][''METRIC_NAME'']
                targets_phase_1.append(name_to_node_map[found_metric])
                log_debug(session, "ANCHOR_FOUND", f"Matched: {found_metric}")

        elif is_drill_mode:
            log_debug(session, "STRATEGY", "Drill Down (Parent IDs provided)")
            try:
                p_ids = json.loads(parent_node_ids_json)
                if not isinstance(p_ids, list): p_ids = [p_ids]
                parent_set = set(p_ids)
                
                # Find children of provided parents
                for edge in all_edges:
                    if edge[''source''] in all_nodes_map and edge[''target''] in parent_set:
                         # Note: Depending on edge direction (source->target), adjust accordingly.
                         # Assuming Source=Parent, Target=Child? Or Source=Metric, Target=Root?
                         # Usually standard DAG is Parent -> Child. 
                         # If input is ''Parent Nodes'', we want their children.
                         # CHECK: Your previous code checked ''target'' in parent_set. 
                         # If Edge is Source(Parent)->Target(Child), then Target in ParentSet means we are looking for parents?
                         # Let''s assume the previous logic was correct: "Find nodes connected to these parents"
                         if edge[''target''] in parent_set:
                             targets_phase_1.append(all_nodes_map[edge[''source'']])
            except Exception as e:
                log_debug(session, "DRILL_ERROR", str(e))

        # 3. Execute Phase 1 (Entry Nodes)
        results_phase_1 = execute_batch(session, targets_phase_1, user_query)

        # 4. Execute Phase 2 (Greedy Traversal of "Bad" Nodes)
        bad_ids = {r[''id''] for r in results_phase_1 if r.get(''status'') == ''bad''}
        targets_phase_2 = []
        
        if bad_ids:
            log_debug(session, "PHASE_2", f"Expanding bad nodes: {list(bad_ids)}")
            for edge in all_edges:
                # If a node was bad, check its relationships
                if edge[''target''] in bad_ids and edge[''source''] in all_nodes_map:
                    targets_phase_2.append(all_nodes_map[edge[''source'']])
        
        results_phase_2 = execute_batch(session, targets_phase_2, user_query)

        # 5. Final Output
        return json.dumps({
            "nodes_analyzed": results_phase_1 + results_phase_2,
            "edges_traversed": [e for e in all_edges if e[''target''] in bad_ids or e[''source''] in bad_ids]
        })

    except Exception as e:
        log_debug(session, "MAIN_CRASH", str(e))
        return json.dumps({"error": "Critical Procedure Error", "details": str(e), "trace": traceback.format_exc()})
';
    left_table: TD_HCO
    right_table: TD_HCO_SUBTYPE
    relationship_columns:
      - left_column: HCO_SUBTYPE_KEY
        right_column: HCO_SUBTYPE_KEY
    join_type: left_outer
    relationship_type: many_to_one
-- 1. Create the Raw Load Table
CREATE OR REPLACE TABLE RAW_PHARMA_SALES (
    ONEKEY_ID VARCHAR(50),
    HCP_ID VARCHAR(50),
    HCP_ZIP_CODE VARCHAR(20),
    TERRITORY_ID VARCHAR(50),
    HCO_NAME VARCHAR(255),
    HCP_NAME VARCHAR(255),
    PRIMARY_SPECIALTY_METRIC VARCHAR(255),
    HCP_STATE VARCHAR(50),
    HCP_CITY VARCHAR(100),
    ID VARCHAR(50),
    GEO_STATE_CD VARCHAR(10),
    GEO_CITY_NAME VARCHAR(100),
    LOC_ADDR_LINE_1 VARCHAR(255),
    LOC_ADDR_LINE_2 VARCHAR(255),
    LOC_ADDR_LINE_3 VARCHAR(255),
    PTAM_TERRITORY_ID VARCHAR(50),
    PROD_FAMGRP_CD VARCHAR(50),
    PROD_CATGRP_CD VARCHAR(50),
    PRODUCT_FAMILY_GROUP VARCHAR(100),
    PRODUCT_CATEGORY VARCHAR(100),
    ACUTE_TRX_LAAD NUMBER,
    ACUTE_NRX_XPO NUMBER,
    ACUTE_TRX_XPO NUMBER,
    HCO_NAME_2 VARCHAR(255),
    HCO_SUBTYPE_CD VARCHAR(50)
);
COPY INTO RAW_PHARMA_SALES
FROM'@"DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB"."AI_POC"."MY_EXCEL_STAGE"/data_sample.csv'
FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
ON_ERROR = 'CONTINUE'; -- Keeps loading even if a few rows have errors
-- 2. Create the Requested View
CREATE OR REPLACE VIEW MASTER_DB_SAMPLE_DATA AS 
SELECT * FROM RAW_PHARMA_SALES;

-- 3. Clear Previous Knowledge Base Data
TRUNCATE TABLE KPI_KNOWLEDGE_BASE_NEW;
TRUNCATE TABLE DECISION_TREE_STORE_NEW;

INSERT INTO KPI_KNOWLEDGE_BASE_NEW (KPI_ID, METRIC_NAME, DESCRIPTION, SQL_QUERY, FEW_SHOT_EXAMPLES, KPI_EMBEDDING)

-- LEVEL 1: Market & Totals
SELECT 'K1', 'Total Market Volume (TRX)', 'Total volume of all acute prescriptions across the entire market.', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Total Market Volume TRX all sales')

UNION ALL
SELECT 'K2', 'Total New Starts (NRX)', 'Total volume of new-to-therapy prescriptions, indicating market growth.', 
'SELECT SUM(ACUTE_NRX_XPO) FROM MASTER_DB_SAMPLE_DATA', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Total New Starts NRX growth')

-- LEVEL 2: Categories
UNION ALL
SELECT 'K3', 'Opioid Category Volume', 'Total prescriptions for the Opioid drug class (e.g., Tramadol).', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_CATEGORY = ''OPIOID''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Opioid Category Volume pain meds')

UNION ALL
SELECT 'K4', 'NSAID Category Volume', 'Total prescriptions for the NSAID drug class (e.g., Diclofenac).', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_CATEGORY = ''NSAID''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'NSAID Category Volume anti inflammatory')

-- LEVEL 3: Products
UNION ALL
SELECT 'K5', 'Tramadol Brand Volume', 'Total prescriptions specifically for the Tramadol product family.', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_FAMILY_GROUP = ''TRAMADOL''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Tramadol Brand Volume product specific')

UNION ALL
SELECT 'K6', 'Diclofenac Brand Volume', 'Total prescriptions specifically for the Diclofenac product family.', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_FAMILY_GROUP = ''DICLOFENAC''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Diclofenac Brand Volume product specific')

-- LEVEL 4: Product Performance Metrics (New vs Refill)
UNION ALL
SELECT 'K7', 'Tramadol New Growth (NRX)', 'New patient prescriptions for Tramadol. Key indicator of sales force effectiveness.', 
'SELECT SUM(ACUTE_NRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_FAMILY_GROUP = ''TRAMADOL''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Tramadol New Growth NRX new patients')

UNION ALL
SELECT 'K8', 'Tramadol Retention (Refills)', 'Volume of Tramadol refills (TRX minus NRX), indicating patient retention.', 
'SELECT SUM(ACUTE_TRX_XPO) - SUM(ACUTE_NRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_FAMILY_GROUP = ''TRAMADOL''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Tramadol Retention Refills loyal patients')

UNION ALL
SELECT 'K9', 'Diclofenac New Growth (NRX)', 'New patient prescriptions for Diclofenac.', 
'SELECT SUM(ACUTE_NRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_FAMILY_GROUP = ''DICLOFENAC''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Diclofenac New Growth NRX')

-- LEVEL 5: Specialist & Geography Segments
UNION ALL
SELECT 'K10', 'Orthopedic Tramadol Starts', 'New Tramadol prescriptions written specifically by Orthopedic Surgeons.', 
'SELECT SUM(ACUTE_NRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_FAMILY_GROUP = ''TRAMADOL'' AND PRIMARY_SPECIALTY_METRIC LIKE ''%Orthopedic%''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'Orthopedic Tramadol Starts surgeon new rx')

UNION ALL
SELECT 'K11', 'PCP Tramadol Starts', 'New Tramadol prescriptions written by Primary Care/Internal Medicine.', 
'SELECT SUM(ACUTE_NRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRODUCT_FAMILY_GROUP = ''TRAMADOL'' AND PRIMARY_SPECIALTY_METRIC IN (''Other PCP Specialties'', ''Internal Medicine'')', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'PCP Tramadol Starts primary care')

UNION ALL
SELECT 'K12', 'NC Market Volume', 'Total prescription volume in North Carolina, a key territory.', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE HCP_STATE = ''NC''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'NC Market Volume north carolina state')

UNION ALL
SELECT 'K13', 'AR Market Volume', 'Total prescription volume in Arkansas.', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE HCP_STATE = ''AR''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'AR Market Volume arkansas state')

UNION ALL
SELECT 'K14', 'High Volume Writers (>10)', 'Prescriptions coming from HCPs with more than 10 total scripts.', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE ACUTE_TRX_XPO > 10', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'High Volume Writers top prescribers')

UNION ALL
SELECT 'K15', 'NP/PA Segment Contribution', 'Volume generated by Nurse Practitioners and Physician Assistants.', 
'SELECT SUM(ACUTE_TRX_XPO) FROM MASTER_DB_SAMPLE_DATA WHERE PRIMARY_SPECIALTY_METRIC = ''NP/PA''', 
PARSE_JSON('[]'), SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', 'NP PA Segment Contribution nurse practitioner');


INSERT INTO DECISION_TREE_STORE_NEW (TREE_ID, GRAPH_JSON) 
SELECT 'Pharma_Master_v1', PARSE_JSON($$
{
    "graph": {
        "nodes": [
            { "id": "K1", "metric": "Total Market Volume (TRX)" },
            { "id": "K2", "metric": "Total New Starts (NRX)" },
            { "id": "K3", "metric": "Opioid Category Volume" },
            { "id": "K4", "metric": "NSAID Category Volume" },
            { "id": "K5", "metric": "Tramadol Brand Volume" },
            { "id": "K6", "metric": "Diclofenac Brand Volume" },
            { "id": "K7", "metric": "Tramadol New Growth (NRX)" },
            { "id": "K8", "metric": "Tramadol Retention (Refills)" },
            { "id": "K9", "metric": "Diclofenac New Growth (NRX)" },
            { "id": "K10", "metric": "Orthopedic Tramadol Starts" },
            { "id": "K11", "metric": "PCP Tramadol Starts" },
            { "id": "K12", "metric": "NC Market Volume" },
            { "id": "K13", "metric": "AR Market Volume" },
            { "id": "K14", "metric": "High Volume Writers (>10)" },
            { "id": "K15", "metric": "NP/PA Segment Contribution" }
        ],
        "edges": [
            { "source": "K3", "target": "K1" }, 
            { "source": "K4", "target": "K1" }, 
            { "source": "K5", "target": "K3" }, 
            { "source": "K6", "target": "K4" }, 
            { "source": "K7", "target": "K5" }, 
            { "source": "K8", "target": "K5" }, 
            { "source": "K9", "target": "K6" }, 
            { "source": "K10", "target": "K7" }, 
            { "source": "K11", "target": "K7" }, 
            { "source": "K15", "target": "K4" }
        ]




  import _snowflake
import json
import time
import snowflake.snowpark as snowpark
from snowflake.snowpark.exceptions import SnowparkSQLException

# ==========================================
# HELPER 1: FAST FEEDBACK SUBMISSION (SQL)
# ==========================================
def submit_feedback_sql(session: snowpark.Session, user_id: str, text: str, is_positive: bool):
    try:
        # Simple SQL-safe string escape
        clean_text = text.replace("'", "''")
        query = f"""
            INSERT INTO feedback_staging 
            (user_id, feedback_text, feedback_rating)
            VALUES ('{user_id}', '{clean_text}', {is_positive})
        """
        session.sql(query).collect()
        print(f" Feedback submitted for: {user_id}")
    except Exception as e:
        print(f" Submit Error: {e}")

# ==========================================
# HELPER 2: CALL AGENT VIA INTERNAL API
# ==========================================
def call_cortex_agent_internal(session: snowpark.Session, user_id: str, user_query: str):
    print(f"\n--- Calling Agent for {user_id} ---")
    
    # --- CONFIGURATION ---
    # Endpoint for stateless agent execution
    AGENT_ENDPOINT_URL = f"/api/v2/databases/''DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB''/schemas/''AI_POC''/agent/''FEEDBACK_TEST'':run"
    TIMEOUT_MS = 180_000 # 3 minutes

    # 1. Retrieve Memory (Fast Lookup)
    try:
        mem_df = session.sql(f"SELECT current_summary FROM user_memory WHERE user_id = '{user_id}'").collect()
        retrieved_memory = mem_df[0]['CURRENT_SUMMARY'] if mem_df and mem_df[0]['CURRENT_SUMMARY'] else "No prior context."
        print(f" Memory Found: {len(retrieved_memory) > 20} (Length: {len(retrieved_memory)})")
    except:
         retrieved_memory = "No prior context."

    # 2. Define Instructions
    # Logic rules for the "brain"
    orchestration_rules = """
    You are a helpful corporate analyst agent.
    - Never fabricate data.
    - If you use a tool to get data, rely ONLY on that data.
    - CRITICAL: You must adhere to the USER CONTEXT provided below in your logic.
    """
    # Style rules for the "voice"
    response_rules = """
    - Be professional and concise.
    - Use Markdown for formatting.
    """

    # 3. Construct the Payload (using AgentInstruction schema)
    # We inject the retrieved memory directly into the orchestration block.
    payload = {
        "model": "claude-3-5-sonnet", # Or your preferred supported model
        "instructions": {
            "orchestration": f"{orchestration_rules}\n\n### USER CONTEXT ###\n{retrieved_memory}",
            "response": response_rules
        },
        "messages": [
            {"role": "user", "content": [{"type": "text", "text": user_query}]}
        ],
        "stream": False
    }

    # Headers for internal API call
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }

    # 4. Make the internal API request
    try:
        response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {}, # query params
            json.dumps(payload),
            {}, # cookies
            TIMEOUT_MS
        )

        # Parse the response body
        response_content = response_dict.get("content", "")
        if not response_content:
             return "Error: Empty response from agent API."
             
        parsed_response = json.loads(response_content)

        # Extract the final text response.
        # Handle standard blocking response format
        if isinstance(parsed_response, dict) and 'message' in parsed_response:
            content_items = parsed_response['message'].get('content', [])
            for item in reversed(content_items):
                if isinstance(item, dict) and item.get("type") == "text":
                    return item.get("text", "No text found in response.")
        
        # Handle potential streaming-style chunks in blocking response
        if isinstance(parsed_response, list) and len(parsed_response) > 0:
             item = parsed_response[-1] # Get last chunk
             if 'message' in item and 'content' in item['message']:
                 content_block = item['message']['content'][-1]
                 if content_block.get('type') == 'text':
                     return content_block.get('text')

        return f"Could not parse response. Raw: {str(parsed_response)[:200]}..."

    except Exception as e:
        return f"An error occurred calling the agent API: {str(e)}"

# ==========================================
# MAIN HANDLER FUNCTION
# ==========================================
def main(session: snowpark.Session):
    # Generate a unique user ID for this test run
    test_user_id = f"worksheet_user_{int(time.time())}"
    test_query = "When does the fiscal year start? Please list the months of Q1."

    print(f"Starting Test Run for User: {test_user_id}")

    # --- PART 1: BEFORE FEEDBACK ---
    print("\n PART 1: BASELINE (Before Feedback) ")
    # The agent should give a generic answer.
    response_before = call_cortex_agent_internal(session, test_user_id, test_query)
    print(f"\n[AGENT REPLY BEFORE]:\n{response_before}\n")


    # --- PART 2: SUBMIT FEEDBACK & WAIT ---
    print("\n PART 2: TEACHING THE SYSTEM ")
    fb_text = "Global Rule: Our fiscal year begins on February 1st. User Preference: Use bullet points for lists."
    print(f"Submitting Feedback: '{fb_text}'")
    # Submit negative feedback to initiate the correction loop
    submit_feedback_sql(session, test_user_id, fb_text, is_positive=False)

    print("\n--- Waiting 130 seconds for background tasks to process... ---")
    # Wait for Classifier Task (1 min) + Summarizer Task (1 min) to finish cycles.
    # This allows data to flow from Staging -> Logs -> Memory.
    time.sleep(130) 
    print("--- Wait complete. Memory should be updated. ---")


    # --- PART 3: AFTER FEEDBACK ---
    print("\n PART 3: VERIFICATION (After Feedback) ")
    # Run the exact same query. The agent should now use the new memory.
    response_after = call_cortex_agent_internal(session, test_user_id, test_query)
    print(f"\n[AGENT REPLY AFTER]:\n{response_after}\n")

    return "Test Complete. Check output panel for results."
    }
}
$$);
Could not parse response. Raw: {'status': 500, 'content': '{\n\t"code":\t"300014",\n\t"message":\t"Cannot construct instance of `com.snowflake.snowapi.cortexliteagent.model.AgentRunRequest` (although at least one Creator exists): n...
[AGENT REPLY BEFORE]:
{'status': 500, 'content': '{\n\t"code":\t"300014",\n\t"message":\t"Cannot construct instance of `com.snowflake.snowapi.cortexdataagent.model.DataAgentRunRequest` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value (\'{\"model\": \"claude-3-5-sonnet\", \"instructions\": {\"orchestration\": \"\\n    You are a helpful corporate analyst agent.\\n    - Never fabricate data.\\n    - If you use a tool to get data, rely ONLY on that data.\\n    - CRITICAL: You must adhere to the USER CONTEXT provided below in your logic.\\n    \\n\\n### USER CONTEXT ###\\nNo prior context.\", \"response\": \"\\n    - Be professional and concise.\\n    - Use Markdown for formatting.\\n    \"}, \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"When does the fiscal year start? Please list the months of Q1.\"}]}], \"stream\": false}\')\n at [Source: (org.eclipse.jetty.server.HttpInputOverHTTP); line: 1, column: 1]",\n\t"request_id":\t"e1c38c66-6845-47f6-8f80-51c409533c1d",\n\t"error_code":\t"300014"\n}', 'headers': {'\nContent-Length': '1033', '\nDate': 'Thu, 08 Jan 2026 18:40:37 GMT', '\nX-Snowflake-Request-ID': 'e1c38c66-6845-47f6-8f80-51c409533c1d', '\nContent-Type': 'application/json'}}
 PART 2: TEACHING THE SYSTEM 
Submitting Feedback: 'Global Rule: Our fiscal year begins on February 1st. User Preference: Use bullet points for lists.'
 Feedback submitted for: worksheet_user_1767897637
--- Waiting 130 seconds for background tasks to process... ---
--- Wait complete. Memory should be updated. ---
 PART 3: VERIFICATION (After Feedback) 
--- Calling Agent for worksheet_user_1767897637 ---
 Memory Found: False (Length: 17)
[AGENT REPLY AFTER]:
{'status': 500, 'content': '{\n\t"code":\t"300014",\n\t"message":\t"Cannot construct instance of `com.snowflake.snowapi.cortexdataagent.model.DataAgentRunRequest` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value (\'{\"model\": \"claude-3-5-sonnet\", \"instructions\": {\"orchestration\": \"\\n    You are a helpful corporate analyst agent.\\n    - Never fabricate data.\\n    - If you use a tool to get data, rely ONLY on that data.\\n    - CRITICAL: You must adhere to the USER CONTEXT provided below in your logic.\\n    \\n\\n### USER CONTEXT ###\\nNo prior context.\", \"response\": \"\\n    - Be professional and concise.\\n    - Use Markdown for formatting.\\n    \"}, \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"When does the fiscal year start? Please list the months of Q1.\"}]}], \"stream\": false}\')\n at [Source: (org.eclipse.jetty.server.HttpInputOverHTTP); line: 1, column: 1]",\n\t"request_id":\t"2f23c5b9-1174-402f-ba88-64f165fd5e2a",\n\t"error_code":\t"300014"\n}', 'headers': {'\nContent-Type': 'application/json', '\nContent-Length': '1033', '\nDate': 'Thu, 08 Jan 2026 18:42:48 GMT', '\nX-Snowflake-Request-ID': '2f23c5b9-1174-402f-ba88-64f165fd5e2a'}}


Skip to main content
Skip to editor
Skip to results
Site





Worksheets
Selection deleted
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
    
    # --- CONFIGURATION ---
    # Endpoint for stateless agent execution - FIX THE QUOTES
    AGENT_ENDPOINT_URL = "/api/v2/databases/DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB/schemas/AI_POC/agents/FEEDBACK_NEW:run"
    TIMEOUT_MS = 180_000  # 3 minutes

    # 1. Retrieve Memory (Fast Lookup)
    try:
        mem_df = session.sql(f"SELECT current_summary FROM user_memory WHERE user_id = '{user_id}'").collect()
        retrieved_memory = mem_df[0]['CURRENT_SUMMARY'] if mem_df and mem_df[0]['CURRENT_SUMMARY'] else "No prior context."
        print(f" Memory Found: {len(retrieved_memory) > 20} (Length: {len(retrieved_memory)})")
    except:
        retrieved_memory = "No prior context."

    # 2. Define Instructions
    # Logic rules for the "brain"
    orchestration_rules = """
    You are a helpful corporate analyst agent.
    - Never fabricate data.
    - If you use a tool to get data, rely ONLY on that data.
    - CRITICAL: You must adhere to the USER CONTEXT provided below in your logic.
    """
    # Style rules for the "voice"
    response_rules = """
    - Be professional and concise.
    - Use Markdown for formatting.
    """

    # 3. Construct the Payload (using AgentInstruction schema)
    # We inject the retrieved memory directly into the orchestration block.
    payload = {
        "model": "claude-3-5-sonnet",  # Or your preferred supported model
        "instructions": {
            "orchestration": f"{orchestration_rules}\n\n### USER CONTEXT ###\n{retrieved_memory}",
            "response": response_rules
        },
        "messages": [
            {"role": "user", "content": [{"type": "text", "text": user_query}]}
        ],
        "stream": False
    }

    # Headers for internal API call
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }

    # 4. Make the internal API request
    try:
        response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {},  # query params
            payload,
            {},  # cookies
            TIMEOUT_MS
        )
        print("Hi")
        # Parse the response body
        response_content = response_dict.get("content")
        # if not response_content:
        #     return "Error: Empty response from agent API."
        print(response_dict)    
        parsed_response = json.loads(response_content)

        # Extract the final text response.
        # Handle standard blocking response format
        if isinstance(parsed_response, dict) and 'message' in parsed_response:
            content_items = parsed_response['message'].get('content', [])
            for item in reversed(content_items):
                if isinstance(item, dict) and item.get("type") == "text":
                    return item.get("text", "No text found in response.")
        
        # Handle potential streaming-style chunks in blocking response
        if isinstance(parsed_response, list) and len(parsed_response) > 0:
            item = parsed_response[-1]  # Get last chunk
            if 'message' in item and 'content' in item['message']:
                content_block = item['message']['content'][-1]
                if content_block.get('type') == 'text':
                    return content_block.get('text')

        return f"Could not parse response. Raw: {str(parsed_response)[:200]}..."

    except Exception as e:
        return f"An error occurred calling the agent API: {str(e)}"
Starting Test Run for User: worksheet_user_1767899167
 PART 1: BASELINE (Before Feedback) 
--- Calling Agent for worksheet_user_1767899167 ---
 Memory Found: False (Length: 17)
Hi
{'status': 200, 'content': '[{\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"The"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" fiscal year start"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" date varies"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" depending"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" on the organization"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" and country:"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"\n\n**Most"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" common"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" fiscal"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" year starts"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t":**\n- **October"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" 1** (U"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t".S. federal"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" government)\n- **April"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" 1** (many"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" countries including"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" Japan"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t", India"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t", UK"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t")"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"\n- **January 1** ("\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"calendar"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" year -"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" many"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" businesses"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t")"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"\n- **July 1** (some"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" states"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" an"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"d organizations"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t")\n\n**Q"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"1 months"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" for"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" each"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t":**"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"\n\n-"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" **October"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" start"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t":** Q"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"1 ="\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" October,"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" November"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t", December\n- **April start:**"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" Q1 = April, May,"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" June  "\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"\n- **January start:** Q1"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" = January, February, March"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"\n- **July start:** Q1 "\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"= July, August, September\n\nIf"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" you\'re asking about a specific organization"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" or country, I"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"\'d be happy to provide"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t" more targete"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text.delta",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"d information!"\n\t\t}\n\t}, {\n\t\t"event":\t"response.text",\n\t\t"data":\t{\n\t\t\t"content_index":\t0,\n\t\t\t"text":\t"The fiscal year start date varies depending on the organization and country:\n\n**Most common fiscal year starts:**\n- **October 1** (U.S. federal government)\n- **April 1** (many countries including Japan, India, UK)\n- **January 1** (calendar year - many businesses)\n- **July 1** (some states and organizations)\n\n**Q1 months for each:**\n\n- **October start:** Q1 = October, November, December\n- **April start:** Q1 = April, May, June  \n- **January start:** Q1 = January, February, March\n- **July start:** Q1 = July, August, September\n\nIf you\'re asking about a specific organization or country, I\'d be happy to provide more targeted information!"\n\t\t}\n\t}, {\n\t\t"event":\t"response",\n\t\t"data":\t{\n\t\t\t"content":\t[{\n\t\t\t\t\t"text":\t"The fiscal year start date varies depending on the organization and country:\n\n**Most common fiscal year starts:**\n- **October 1** (U.S. federal government)\n- **April 1** (many countries including Japan, India, UK)\n- **January 1** (calendar year - many businesses)\n- **July 1** (some states and organizations)\n\n**Q1 months for each:**\n\n- **October start:** Q1 = October, November, December\n- **April start:** Q1 = April, May, June  \n- **January start:** Q1 = January, February, March\n- **July start:** Q1 = July, August, September\n\nIf you\'re asking about a specific organization or country, I\'d be happy to provide more targeted information!",\n\t\t\t\t\t"type":\t"text"\n\t\t\t\t}],\n\t\t\t"metadata":\t{\n\t\t\t\t"usage":\t{\n\t\t\t\t\t"tokens_consumed":\t[{\n\t\t\t\t\t\t\t"context_window":\t200000,\n\t\t\t\t\t\t\t"input_tokens":\t{\n\t\t\t\t\t\t\t\t"cache_read":\t0,\n\t\t\t\t\t\t\t\t"cache_write":\t0,\n\t\t\t\t\t\t\t\t"total":\t78,\n\t\t\t\t\t\t\t\t"uncached":\t78\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t"model_name":\t"claude-4-sonnet",\n\t\t\t\t\t\t\t"output_tokens":\t{\n\t\t\t\t\t\t\t\t"total":\t175\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t},\n\t\t\t"role":\t"assistant",\n\t\t\t"schema_version":\t"v2"\n\t\t}\n\t}, {\n\t\t"event":\t"done",\n\t\t"data":\t"[DONE]"\n\t}]', 'headers': {'\nDate': 'Thu, 08 Jan 2026 19:06:08 GMT', '\nX-Snowflake-Request-ID': '32911316-3d54-4627-a217-a074a4a3fd52', '\nContent-Type': 'application/json', '\nTransfer-Encoding': 'chunked'}}
[AGENT REPLY BEFORE]:
Could not parse response. Raw: [{'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'The'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' fiscal year start'}}, {'event': 'response.text.d...

Starting Test Run for User: worksheet_user_1767900217


import _snowflake
import json
import time
import snowflake.snowpark as snowpark
from snowflake.snowpark.exceptions import SnowparkSQLException

# ==========================================
# HELPER 1: FAST FEEDBACK SUBMISSION (SQL)
# ==========================================
def submit_feedback_sql(session: snowpark.Session, user_id: str, text: str, is_positive: bool):
    try:
        # Simple SQL-safe string escape
        clean_text = text.replace("'", "''")
        query = f"""
            INSERT INTO feedback_staging 
            (user_id, feedback_text, feedback_rating)
            VALUES ('{user_id}', '{clean_text}', {is_positive})
        """
        session.sql(query).collect()
        print(f" Feedback submitted for: {user_id}")
    except Exception as e:
        print(f" Submit Error: {e}")

# ==========================================
# HELPER 2: CALL AGENT VIA INTERNAL API
# ==========================================
def call_cortex_agent_internal(session: snowpark.Session, user_id: str, user_query: str):
    print(f"\n--- Calling Agent for {user_id} ---")
    
    # --- CONFIGURATION ---
    # Endpoint for stateless agent execution - FIX THE QUOTES
    AGENT_ENDPOINT_URL = "/api/v2/databases/DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB/schemas/AI_POC/agents/FEEDBACK_NEW:run"
    TIMEOUT_MS = 180_000  # 3 minutes

    # 1. Retrieve Memory (Fast Lookup)
    try:
        mem_df = session.sql(f"SELECT current_summary FROM user_memory WHERE user_id = '{user_id}'").collect()
        retrieved_memory = mem_df[0]['CURRENT_SUMMARY'] if mem_df and mem_df[0]['CURRENT_SUMMARY'] else "No prior context."
        print(f" Memory Found: {len(retrieved_memory) > 20} (Length: {len(retrieved_memory)})")
    except:
        retrieved_memory = "No prior context."

    # 2. Define Instructions
    # Logic rules for the "brain"
    orchestration_rules = """
    You are a helpful corporate analyst agent.
    - Never fabricate data.
    - If you use a tool to get data, rely ONLY on that data.
    - CRITICAL: You must adhere to the USER CONTEXT provided below in your logic.
    """
    # Style rules for the "voice"
    response_rules = """
    - Be professional and concise.
    - Use Markdown for formatting.
    """

    # 3. Construct the Payload (using AgentInstruction schema)
    # We inject the retrieved memory directly into the orchestration block.
    payload = {
        "model": "claude-3-5-sonnet",  # Or your preferred supported model
        "instructions": {
            "orchestration": f"{orchestration_rules}\n\n### USER CONTEXT ###\n{retrieved_memory}",
            "response": response_rules
        },
        "messages": [
            {"role": "user", "content": [{"type": "text", "text": user_query}]}
        ],
        "stream": False
    }

    # Headers for internal API call
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }

    # 4. Make the internal API request
    try:
        response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {},  # query params
            payload,
            {},  # cookies
            TIMEOUT_MS
        )

        # Parse the response body
        response_content = response_dict.get("content", "")
        if not response_content:
            return "Error: Empty response from agent API."
             
        parsed_response = json.loads(response_content)

        # Extract the final text response.
        # Handle standard blocking response format
        if isinstance(parsed_response, dict) and 'message' in parsed_response:
            content_items = parsed_response['message'].get('content', [])
            for item in reversed(content_items):
                if isinstance(item, dict) and item.get("type") == "text":
                    return item.get("text", "No text found in response.")
        
        # Handle potential streaming-style chunks in blocking response
        if isinstance(parsed_response, list) and len(parsed_response) > 0:
            item=parsed_response
            print(parsed_response)
            if 'event' in item and 'response' in item['event']:
                content_block = item['data']['content'][-1]
                if content_block.get('type') == 'text':
                    return content_block.get('text')

        return f"Could not parse response. Raw: {str(parsed_response)[:200]}..."

    except Exception as e:
        return f"An error occurred calling the agent API: {str(e)}"

# ==========================================
# MAIN HANDLER FUNCTION
# ==========================================
def main(session: snowpark.Session):
    # Generate a unique user ID for this test run
    test_user_id = f"worksheet_user_{int(time.time())}"
    test_query = "When does the fiscal year start? Please list the months of Q1."

    print(f"Starting Test Run for User: {test_user_id}")

    # --- PART 1: BEFORE FEEDBACK ---
    print("\n PART 1: BASELINE (Before Feedback) ")
    # The agent should give a generic answer.
    response_before = call_cortex_agent_internal(session, test_user_id, test_query)
    print(f"\n[AGENT REPLY BEFORE]:\n{response_before}\n")

    # # --- PART 2: SUBMIT FEEDBACK & WAIT ---
    # print("\n PART 2: TEACHING THE SYSTEM ")
    # fb_text = "Global Rule: Our fiscal year begins on February 1st. User Preference: Use bullet points for lists."
    # print(f"Submitting Feedback: '{fb_text}'")
    # # Submit negative feedback to initiate the correction loop
    # submit_feedback_sql(session, test_user_id, fb_text, is_positive=False)

    # print("\n--- Waiting 130 seconds for background tasks to process... ---")
    # # Wait for Classifier Task (1 min) + Summarizer Task (1 min) to finish cycles.
    # # This allows data to flow from Staging -> Logs -> Memory.
    # time.sleep(130) 
    # print("--- Wait complete. Memory should be updated. ---")

    # # --- PART 3: AFTER FEEDBACK ---
    # print("\n PART 3: VERIFICATION (After Feedback) ")
    # # Run the exact same query. The agent should now use the new memory.
    # response_after = call_cortex_agent_internal(session, test_user_id, test_query)
    # print(f"\n[AGENT REPLY AFTER]:\n{response_after}\n")

    # return "Test Complete. Check output panel for results."

 PART 1: BASELINE (Before Feedback) 
--- Calling Agent for worksheet_user_1767900217 ---
 Memory Found: False (Length: 17)
[{'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'The'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' fiscal year start'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' date varies'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' depending'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' on the organization'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' or'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' country:'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '\n\n**Common'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' fiscal year start dates:**\n- **'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'October'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' 1** (U'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '.S. federal government)\n-'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' **April'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' 1** (many'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' countries including'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' Japan'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ', India'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ', UK'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ')'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '\n- **January 1** ('}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'calendar'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' year, use'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'd by many'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' businesses'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ')\n- **July 1**'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' (some'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' states'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' an'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'd organizations'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ')\n\n**Q'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '1 months'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' depen'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'd on the'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' fiscal'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' year start'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ':**'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '\n\n-'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' If'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' fiscal'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' year starts **'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'October 1**:'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' Q1 ='}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' October,'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' November'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ', December\n- If fiscal year starts'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' **April 1**: Q1 '}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '= April, May, June  '}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '\n- If fiscal year starts **January'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' 1**: Q1 = January'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ', February, March\n- If fiscal'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' year starts **July 1**: Q'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '1 = July, August, September'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': '\n\nThe'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' most'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' common fiscal'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' year for'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' the'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' U.S. federal'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' government starts'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' October'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' 1,'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' so'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' Q1 woul'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'd be October, November, an'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'd December.'}}, {'event': 'response.text', 'data': {'content_index': 0, 'text': 'The fiscal year start date varies depending on the organization or country:\n\n**Common fiscal year start dates:**\n- **October 1** (U.S. federal government)\n- **April 1** (many countries including Japan, India, UK)\n- **January 1** (calendar year, used by many businesses)\n- **July 1** (some states and organizations)\n\n**Q1 months depend on the fiscal year start:**\n\n- If fiscal year starts **October 1**: Q1 = October, November, December\n- If fiscal year starts **April 1**: Q1 = April, May, June  \n- If fiscal year starts **January 1**: Q1 = January, February, March\n- If fiscal year starts **July 1**: Q1 = July, August, September\n\nThe most common fiscal year for the U.S. federal government starts October 1, so Q1 would be October, November, and December.'}}, {'event': 'response', 'data': {'content': [{'text': 'The fiscal year start date varies depending on the organization or country:\n\n**Common fiscal year start dates:**\n- **October 1** (U.S. federal government)\n- **April 1** (many countries including Japan, India, UK)\n- **January 1** (calendar year, used by many businesses)\n- **July 1** (some states and organizations)\n\n**Q1 months depend on the fiscal year start:**\n\n- If fiscal year starts **October 1**: Q1 = October, November, December\n- If fiscal year starts **April 1**: Q1 = April, May, June  \n- If fiscal year starts **January 1**: Q1 = January, February, March\n- If fiscal year starts **July 1**: Q1 = July, August, September\n\nThe most common fiscal year for the U.S. federal government starts October 1, so Q1 would be October, November, and December.', 'type': 'text'}], 'metadata': {'usage': {'tokens_consumed': [{'context_window': 200000, 'input_tokens': {'cache_read': 0, 'cache_write': 0, 'total': 78, 'uncached': 78}, 'model_name': 'claude-4-sonnet', 'output_tokens': {'total': 211}}]}}, 'role': 'assistant', 'schema_version': 'v2'}}, {'event': 'done', 'data': '[DONE]'}]
[AGENT REPLY BEFORE]:
Could not parse response. Raw: [{'event': 'response.text.delta', 'data': {'content_index': 0, 'text': 'The'}}, {'event': 'response.text.delta', 'data': {'content_index': 0, 'text': ' fiscal year start'}}, {'event': 'response.text.d...
Checking Task History (Last 5 runs)...
    [INCREMENTAL_MEMORY_TASK] State: SCHEDULED
    [BACKEND_CLASSIFIER_TASK] State: FAILED
       Error: Cannot execute task , USAGE privilege on the task's warehouse must be granted to owner role
    [INCREMENTAL_MEMORY_TASK] State: FAILED
       Error: Cannot execute task , USAGE privilege on the task's warehouse must be granted to owner role
    [BACKEND_CLASSIFIER_TASK] State: FAILED
       Error: Cannot execute task , USAGE privilege on the task's warehouse must be granted to owner role
GRANT USAGE ON WAREHOUSE DEV_PAIN_SALES_PERFORMANCE_B_WH TO ROLE FR_PRJ_DEV_PAIN_SALES_RW;
print(grant_permission)

-- 3. Grant permission to execute tasks (just in case)
GRANT EXECUTE TASK ON ACCOUNT TO ROLE SYSADMIN;


# # import _snowflake
# # import json
# # import time
# # import snowflake.snowpark as snowpark
# # from snowflake.snowpark.exceptions import SnowparkSQLException

# # # ==========================================
# # # HELPER 1: FAST FEEDBACK SUBMISSION (SQL)
# # # ==========================================
# # def submit_feedback_sql(session: snowpark.Session, user_id: str, text: str, is_positive: bool):
# #     try:
# #         # Simple SQL-safe string escape
# #         clean_text = text.replace("'", "''")
# #         query = f"""
# #             INSERT INTO feedback_staging 
# #             (user_id, feedback_text, feedback_rating)
# #             VALUES ('{user_id}', '{clean_text}', {is_positive})
# #         """
# #         session.sql(query).collect()
# #         print(f" Feedback submitted for: {user_id}")
# #     except Exception as e:
# #         print(f" Submit Error: {e}")

# # # ==========================================
# # # HELPER 2: CALL AGENT VIA INTERNAL API
# # # ==========================================
# # def call_cortex_agent_internal(session: snowpark.Session, user_id: str, user_query: str):
# #     print(f"\n--- Calling Agent for {user_id} ---")
    
# #     # --- CONFIGURATION ---
# #     # Endpoint for stateless agent execution
# #     AGENT_ENDPOINT_URL = "/api/v2/databases/DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB/schemas/AI_POC/agents/FEEDBACK_NEW:run"
# #     TIMEOUT_MS = 180_000  # 3 minutes

# #     # 1. Retrieve Memory (Fast Lookup)
# #     try:
# #         mem_df = session.sql(f"SELECT current_summary FROM user_memory WHERE user_id = '{user_id}'").collect()
# #         print(mem_df)
# #         retrieved_memory = mem_df[0]['CURRENT_SUMMARY'] if mem_df and mem_df[0]['CURRENT_SUMMARY'] else "No prior context."
# #         print(f" Memory Found: {len(retrieved_memory) > 20} (Length: {len(retrieved_memory)})")
# #     except:
# #         retrieved_memory = "No prior context."

# #     # 2. Define Instructions
# #     orchestration_rules = """
# #     You are a helpful corporate analyst agent.
# #     - Never fabricate data.
# #     - If you use a tool to get data, rely ONLY on that data.
# #     - CRITICAL: You must adhere to the USER CONTEXT provided below in your logic.
# #     """
# #     response_rules = """
# #     - Be professional and concise.
# #     - Use Markdown for formatting.
# #     """

# #     # 3. Construct the Payload
# #     payload = {
# #         "model": "claude-3-5-sonnet", 
# #         "instructions": {
# #             "orchestration": f"{orchestration_rules}\n\n### USER CONTEXT ###\n{retrieved_memory}",
# #             "response": response_rules
# #         },
# #         "messages": [
# #             {"role": "user", "content": [{"type": "text", "text": user_query}]}
# #         ],
# #         "stream": False
# #     }

# #     headers = {
# #         "Content-Type": "application/json",
# #         "Accept": "application/json"
# #     }

# #     # 4. Make the internal API request
# #     try:
# #         response_dict = _snowflake.send_snow_api_request(
# #             "POST",
# #             AGENT_ENDPOINT_URL,
# #             headers,
# #             {}, 
# #             payload,
# #             {}, 
# #             TIMEOUT_MS
# #         )

# #         response_content = response_dict.get("content", "")
# #         if not response_content:
# #             return "Error: Empty response from agent API."
            
# #         parsed_response = json.loads(response_content)

# #         # --- PARSING LOGIC START ---
        
# #         # Case A: Standard Dictionary Response (Blocking)
# #         if isinstance(parsed_response, dict) and 'message' in parsed_response:
# #             content_items = parsed_response['message'].get('content', [])
# #             for item in reversed(content_items):
# #                 if isinstance(item, dict) and item.get("type") == "text":
# #                     return item.get("text", "No text found in response.")
        
# #         # Case B: List of Events (Streaming/SSE captured as list)
# #         # This is the format matching your raw output log
# #         elif isinstance(parsed_response, list):
            
# #             # Priority 1: Look for the final 'response' event which contains the complete answer
# #             for item in parsed_response:
# #                 if item.get('event') == 'response':
# #                     try:
# #                         # Extract content list from data
# #                         content_list = item.get('data', {}).get('content', [])
# #                         # Look for the text part
# #                         for part in content_list:
# #                             if part.get('type') == 'text':
# #                                 return part.get('text')
# #                     except Exception:
# #                         continue

# #             # Priority 2: Look for 'response.text' event (aggregated text)
# #             for item in parsed_response:
# #                 if item.get('event') == 'response.text':
# #                     return item.get('data', {}).get('text')

# #             # Priority 3: Reconstruct from 'response.text.delta' events
# #             chunks = []
# #             for item in parsed_response:
# #                 if item.get('event') == 'response.text.delta':
# #                     chunks.append(item.get('data', {}).get('text', ''))
            
# #             if chunks:
# #                 return "".join(chunks)

# #         return f"Could not parse response. Raw structure unexpected."
# #         # --- PARSING LOGIC END ---

# #     except Exception as e:
# #         return f"An error occurred calling the agent API: {str(e)}"

# # # ==========================================
# # # MAIN HANDLER FUNCTION
# # # ==========================================
# # def main(session: snowpark.Session):
# #     # Generate a unique user ID for this test run
# #     test_user_id = f"worksheet_user_{int(time.time())}"
# #     test_query = "When does the fiscal year start? Please list the months of Q1."

# #     print(f"Starting Test Run for User: {test_user_id}")

# #     # --- PART 1: BEFORE FEEDBACK ---
# #     print("\n PART 1: BASELINE (Before Feedback) ")
# #     # The agent should give a generic answer.
# #     response_before = call_cortex_agent_internal(session, test_user_id, test_query)
# #     print(f"\n[AGENT REPLY BEFORE]:\n{response_before}\n")

# #     # --- PART 2: SUBMIT FEEDBACK & WAIT ---
# #     print("\n PART 2: TEACHING THE SYSTEM ")
# #     fb_text = "Global Rule: Our fiscal year begins on February 1st. User Preference: Use bullet points for lists."
# #     print(f"Submitting Feedback: '{fb_text}'")
# #     # Submit negative feedback to initiate the correction loop
# #     submit_feedback_sql(session, test_user_id, fb_text, is_positive=False)

# #     # print("\n--- Waiting 130 seconds for background tasks to process... ---")
# #     # # Wait for Classifier Task (1 min) + Summarizer Task (1 min) to finish cycles.
# #     # # This allows data to flow from Staging -> Logs -> Memory.
# #     # time.sleep(130) 
# #     # print("--- Wait complete. Memory should be updated. ---")

# #     # # --- PART 3: AFTER FEEDBACK ---
# #     # print("\n PART 3: VERIFICATION (After Feedback) ")
# #     # # Run the exact same query. The agent should now use the new memory.
# #     # response_after = call_cortex_agent_internal(session, test_user_id, test_query)
# #     # print(f"\n[AGENT REPLY AFTER]:\n{response_after}\n")

# #     # return "Test Complete. Check output panel for results."

I don't have access to information about your organization's fiscal year start date. Fiscal years vary significantly between different companies, organizations, and countries. 

To find out when your fiscal year starts, you could:
- Check with your finance/accounting department
- Look at recent financial statements or annual reports
- Review your employee handbook or internal documentation
- Ask your HR department or manager

Is there anything else I can help you with regarding fiscal year planning or budgeting concepts in general?



-- ================================================
-- SELF-CORRECTING AGENT: BACKEND SETUP CREATE
-- ================================================

-- ================================================
-- 2. STORAGE LAYER (TABLES)
-- ================================================

-- A. The "Fast Inbox" (Staging)
-- Your backend inserts here instantly. No processing happens on insert.
CREATE OR REPLACE TABLE feedback_staging (
    stage_id STRING DEFAULT UUID_STRING(),
    user_id STRING NOT NULL,
    feedback_text STRING,
    feedback_rating BOOLEAN, -- True=Positive, False=Negative (needs correction)
    created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

-- B. The "Raw Journal" (Permanent Logs)
-- Data moves here AFTER the AI procedure classifies the scope.
CREATE OR REPLACE TABLE feedback_logs (
    log_id STRING DEFAULT UUID_STRING(),
    user_id STRING,
    feedback_text STRING,
    feedback_rating BOOLEAN,
    scope STRING,            -- 'USER' or 'GLOBAL' (Determined by AI)
    created_at TIMESTAMP_NTZ,
    processed_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

-- C. The "Active Memory" (Hybrid Table)
-- Optimized for ultra-fast, single-row lookups during agent runtime.
-- Requires an account enabled for Hybrid Tables.
CREATE OR REPLACE HYBRID TABLE user_memory (
    user_id STRING PRIMARY KEY,
    current_summary STRING,
    last_updated TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

-- ================================================
-- 3. CHANGE DETECTION (STREAMS)
-- ================================================

-- Stream tracking new insertions into the Inbox
CREATE OR REPLACE STREAM feedback_staging_stream ON TABLE feedback_staging;

-- Stream tracking new validated entries in the Journal
CREATE OR REPLACE STREAM feedback_logs_stream ON TABLE feedback_logs;

-- ================================================
-- 4. PROCESSING LAYER (AI STORED PROCEDURES)
-- ================================================

-- PROCEDURE 1: CLASSIFIER (Staging -> Logs)
-- Reads new staging data, uses small LLM to classify scope, inserts into logs.
CREATE OR REPLACE PROCEDURE process_staging_to_logs()
RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.10'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'run_classification'
AS
$$
import pandas as pd

def run_classification(session):
    # 1. Read new INSERT data from the staging stream
    raw_df = session.sql("""
        SELECT user_id, feedback_text, feedback_rating, created_at
        FROM feedback_staging_stream
        WHERE metadata$action = 'INSERT'
    """).to_pandas()
    
    if raw_df.empty:
        return "Skipping: No new staging data detected."

    processed_rows = []
    
    # 2. Iterate and classify
    for _, row in raw_df.iterrows():
        text = row['FEEDBACK_TEXT']
        rating = row['FEEDBACK_RATING']
        user_id = row['USER_ID']
        created_at = row['CREATED_AT']
        
        detected_scope = 'USER' # Default assumption

        # Only execute AI classification for negative, textual feedback
        if rating == False and text and text.strip():
             # Escape for SQL safety in prompt
             safe_text_prompt = text.replace("'", "\\'").replace('"', '\\"')
             
             prompt = f"""
             <snowflake_prompt_trigger>
             Task: Classify feedback scope for an AI Agent.
             Definitions:
             - GLOBAL: Factual errors, wrong math, hallucinations, business logic bugs. (Applies to everyone).
             - USER: Tone preferences, formatting requests, language style. (Applies only to this user).
             
             Feedback: "{safe_text_prompt}"
             
             Output: Reply ONLY with the single word 'GLOBAL' or 'USER'.
             </snowflake_prompt_trigger>
             """
             
             try:
                # Use a fast, cheap model for simple classification
                cmd = "SELECT SNOWFLAKE.CORTEX.COMPLETE('llama3-8b', ?)"
                result_df = session.sql(cmd, params=[prompt]).collect()
                ai_response = result_df[0][0].strip().upper()
                
                if ai_response in ['GLOBAL', 'USER']:
                    detected_scope = ai_response
                # Else keep default 'USER' if LLM response is weird
             except Exception as e:
                 print(f"Classification error for user {user_id}: {e}")
                 # Keep default 'USER' on error

        # Prepare string for bulk insert
        safe_text_insert = text.replace("'", "''") if text else ""
        processed_rows.append(
            f"('{user_id}', '{safe_text_insert}', {rating}, '{detected_scope}', '{created_at}')"
        )

    # 3. Bulk Insert into Final Logs
    if processed_rows:
        values_str = ",".join(processed_rows)
        insert_sql = f"""
            INSERT INTO feedback_logs (user_id, feedback_text, feedback_rating, scope, created_at)
            VALUES {values_str}
        """
        session.sql(insert_sql).collect()

    return f"Success: Classified and moved {len(processed_rows)} rows to logs."
$$;

-- PROCEDURE 2: SUMMARIZER (Logs -> Active Memory)
-- Reads new logs, aggregates by user, merges with existing memory, summarizes using large LLM.
CREATE OR REPLACE PROCEDURE process_incremental_batch()
RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.10'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'run_summarization'
AS
$$
from snowflake.snowpark.functions import col, lit, listagg
import pandas as pd

def run_summarization(session):
    # 1. Aggregate new logs by User ID into a single text block
    # We limit the aggregated string length to avoid overflowing LLM context windows unnecessarily.
    new_feedback_df = session.sql("""
        SELECT 
            user_id, 
            SUBSTR(LISTAGG('[' || scope || '] ' || feedback_text, ' | SELECT NEW_LINE'), 0, 20000) as new_notes 
        FROM feedback_logs_stream 
        WHERE metadata$action = 'INSERT' 
        GROUP BY user_id
    """)
    
    count_df = new_feedback_df.count()
    if count_df == 0:
         return "Skipping: No new logs to summarize."

    # 2. Join with Current Memory (Hybrid Table) to get the "Old State"
    # We use a LEFT JOIN to handle new users who don't have memory yet.
    batch_df = new_feedback_df.join(
        session.table("user_memory"), 
        new_feedback_df.col("user_id") == session.table("user_memory").col("user_id"), 
        "left"
    ).select(
        new_feedback_df.col("user_id"),
        new_feedback_df.col("new_notes"),
        col("current_summary")
    )

    # 3. Vectorized AI Call (SQL-based parallel execution)
    # We construct the prompt within SQL and pass it to Cortex in bulk.
    # Using a larger model (llama3-70b) for complex summarization.
    prompt_template = """
    You are an expert memory manager for an AI agent.
    Task: Update the user's profile based on new feedback.
    
    ### OLD PROFILE:
    ' || NVL(current_summary, 'No prior profile.') || '
    
    ### NEW FEEDBACK TO INTEGRATE:
    ' || new_notes || '
    
    ### INSTRUCTIONS:
    1. Consolidate the new feedback into the old profile.
    2. Resolve conflicts: New feedback overrides old profile data.
    3. Categorize clearly between "GLOBAL RULES" (facts/logic) and "USER PREFERENCES" (style/tone).
    4. Be concise. Output ONLY the updated profile text.
    """
    
    source_query = f"""
    SELECT 
        user_id,
        SNOWFLAKE.CORTEX.COMPLETE('llama3-70b', '{prompt_template}') as new_sum
    FROM ({batch_df.queries['queries'][0]})
    """

    # 4. Execute MERGE (Upsert) into Hybrid Table
    merge_sql = f"""
        MERGE INTO user_memory AS target
        USING ({source_query}) AS source
        ON target.user_id = source.user_id
        WHEN MATCHED THEN 
            UPDATE SET current_summary = source.new_sum, last_updated = CURRENT_TIMESTAMP()
        WHEN NOT MATCHED THEN 
            INSERT (user_id, current_summary) VALUES (source.user_id, source.new_sum)
    """
    
    session.sql(merge_sql).collect()
    return f"Success: Updated memory for {count_df} users."
$$;

-- ================================================
-- 5. AUTOMATION (TASKS)
-- ================================================

-- Task 1: The Classifier (Checks staging stream every minute)
CREATE OR REPLACE TASK backend_classifier_task
    WAREHOUSE = 'DEV_IMPACT_WH' -- Replace with your warehouse
    SCHEDULE = '1 MINUTE'
    WHEN SYSTEM$STREAM_HAS_DATA('feedback_staging_stream')
AS
    CALL process_staging_to_logs();

-- Task 2: The Summarizer (Checks logs stream every minute)
CREATE OR REPLACE TASK incremental_memory_task
    WAREHOUSE = 'DEV_IMPACT_WH' -- Replace with your warehouse
    SCHEDULE = '1 MINUTE'
    WHEN SYSTEM$STREAM_HAS_DATA('feedback_logs_stream')
AS
    CALL process_incremental_batch();

-- Start the automation tasks
ALTER TASK backend_classifier_task RESUME;
ALTER TASK incremental_memory_task RESUME;


 # Note: Use POST, not GET
            response = _snowflake.send_snow_api_request(
                "POST", 
                "/api/v2/cortex/threads", 
                {"Content-Type": "application/json"}, 
                {}, 
                {"origin_application": "my_app"}, 
                None, 
                30000
            )

            print(response)
            
-- ================================================
-- SETUP COMPLETE
-- ================================================


 # Note: Use POST, not GET
            response = _snowflake.send_snow_api_request(
                "POST", 
                "/api/v2/cortex/threads", 
                {"Content-Type": "application/json"}, 
                {}, 
                {"origin_application": "my_app"}, 
                None, 
                30000
            )

            print(response)


CREATE OR REPLACE PROCEDURE DIAGNOSTIC_TOOL(
    USER_QUERY VARCHAR,
    PARENT_NODE_IDS_JSON VARCHAR,
    TREE_ID VARCHAR
)
RETURNS VARCHAR
LANGUAGE PYTHON
RUNTIME_VERSION = '3.10'
PACKAGES = ('snowflake-snowpark-python', 'pandas', 'simplejson', 'pydantic')
HANDLER = 'main'
EXECUTE AS OWNER
AS $$
import simplejson as json
import concurrent.futures
import traceback
from pydantic import BaseModel, Field
from snowflake.snowpark import Session

# ==========================================
# 1. DATA MODELS
# ==========================================
class SqlGenerationResponse(BaseModel):
    sql_query: str = Field(description="The executable Snowflake SQL query")
    filters_applied: list[str] = Field(description="List of specific filters applied from the user request")

class StatusCheckResponse(BaseModel):
    is_bad: bool = Field(description="True if the metric value indicates a negative business outcome")
    reason: str = Field(description="Concise business reason (max 10 words)")

# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================
def log_debug(session: Session, step_name: str, message: str):
    """Logs debug information to the database securely using Dollar Quoting."""
    try:
        DQ = "$" + "$"
        safe_msg = str(message)[:100000]
        safe_step = str(step_name)[:50]
        query = (
            "INSERT INTO DIAGNOSTIC_TOOL_DB_VERTEX.DIAGNOSTIC.DEBUG_LOGS_NEW "
            "(STEP_NAME, LOG_MESSAGE) VALUES (" + DQ + safe_step + DQ + ", " + DQ + safe_msg + DQ + ")"
        )
        session.sql(query).collect()
    except Exception:
        pass # Fail silently to strictly avoid interrupting main flow

# ==========================================
# 3. CORE LOGIC: PROCESS SINGLE NODE
# ==========================================
def process_node(session: Session, node: dict, user_query: str, meta_map: dict) -> dict:
    metric_name = node['metric']
    node_id = node['id']
    polarity = node.get('polarity', 0)
    DQ = "$" + "$"

    # --- Step 0: Validation ---
    log_debug(session, f"NODE_START_{node_id}", f"Processing Metric: {metric_name}")
    
    meta = meta_map.get(metric_name)
    if not meta:
        error_msg = f"Metadata missing for metric: {metric_name}"
        log_debug(session, f"NODE_ERROR_{node_id}", error_msg)
        return {"id": node_id, "status": "error", "error": error_msg}

    static_query = meta['SQL_QUERY']
    final_query = static_query
    query_type = "static"
    executed_sql = static_query

    # --- Step 1: Dynamic SQL Generation (GPT-4o) ---
    if user_query and len(user_query) > 5:
        try:
            log_debug(session, f"GEN_SQL_START_{node_id}", "Initiating prompt construction...")
            
            # Build Few-Shot Examples
            examples_text = ""
            ex_json = meta.get('FEW_SHOT_EXAMPLES')
            if ex_json:
                ex_list = json.loads(ex_json) if isinstance(ex_json, str) else ex_json
                for i, ex in enumerate(ex_list):
                    examples_text += "\nExample {}: User Request='{}' -> SQL='{}'".format(i+1, ex['question'], ex['sql'])

            # Expert Prompt Engineering
            prompt_content = """
            You are an expert Snowflake SQL Architect. 
            Your goal is to adapt a Standard SQL Logic to answer a specific User Request.

            **Context:**
            - Metric Name: {}
            - Standard Logic: {}
            - User Request: "{}"

            **Guidelines:**
            1. Keep the base logic of the Standard Logic (tables, joins, core calculations).
            2. Apply filters (WHERE clauses) based on the User Request (e.g., date ranges, regions, segments).
            3. Ensure the syntax is valid Snowflake SQL.
            4. If the User Request is irrelevant to this metric, return the Standard Logic unchanged.

            **Reference Examples:**
            {}
            """.format(metric_name, static_query, user_query, examples_text)

            # Sanitize Prompt
            safe_prompt = prompt_content.replace(DQ, "")
            
            # Define Output Schema
            options_str = json.dumps({
                "type": "json",
                "schema": {
                    "type": "object",
                    "properties": {
                        "sql_query": { "type": "string" },
                        "filters_applied": { "type": "array", "items": { "type": "string" } }
                    },
                    "required": ["sql_query", "filters_applied"],
                    "additionalProperties": False  # <--- ADD THIS LINE
                }
            })

            # Call Cortex (openai-gpt-4.1)
            cmd = (
                "SELECT AI_COMPLETE("
                "model => 'openai-gpt-4.1', "
                "prompt => " + DQ + safe_prompt + DQ + ", "
                "response_format => PARSE_JSON(" + DQ + options_str + DQ + ")"
                ")"
            )
            
            resp = session.sql(cmd).collect()
            raw_result = resp[0][0]
            log_debug(session, f"GEN_SQL_RESPONSE_{node_id}", str(raw_result))

            # Parse Response
            if isinstance(raw_result, str):
                parsed = SqlGenerationResponse.model_validate_json(raw_result)
            else:
                parsed = SqlGenerationResponse.model_validate(raw_result)
                
            final_query = parsed.sql_query
            query_type = "dynamic"
            
        except Exception as e:
            log_debug(session, f"GEN_SQL_FAIL_{node_id}", f"Error: {str(e)}. Fallback to static.")
            final_query = static_query
            query_type = "static_fallback"

    # --- Step 2: Query Execution ---
    val = "No Data"
    executed_sql = final_query
    
    try:
        log_debug(session, f"EXEC_START_{node_id}", f"Running SQL: {final_query}")
        df = session.sql(final_query).to_pandas()
        val = df.iloc[0,0] if not df.empty else "No Data"
        log_debug(session, f"EXEC_SUCCESS_{node_id}", f"Result Value: {val}")
        
    except Exception as e:
        log_debug(session, f"EXEC_FAIL_{node_id}", f"Primary SQL failed. Attempting recovery with static query. Error: {str(e)}")
        try:
            df = session.sql(static_query).to_pandas()
            val = df.iloc[0,0] if not df.empty else "No Data"
            executed_sql = static_query
            query_type = "static_recovery"
            log_debug(session, f"EXEC_RECOVERY_{node_id}", f"Recovery Value: {val}")
        except Exception as e2:
            return {"id": node_id, "status": "error", "error": f"Execution failed: {str(e2)}"}

    # --- Step 3: Status Analysis (openai-gpt-4.1) ---
    try:
        pol_txt = "Higher is BAD" if polarity == -1 else "Lower is BAD" if polarity == 1 else "Context dependent"
        
        status_prompt = """
        You are a Business Intelligence Analyst.
        Analyze the following metric result to determine if it represents a negative business outcome ('Bad').

        - Metric: {}
        - Value: {}
        - Business Rule: {}
        - User Context: "{}"

        Determine if this is 'Bad' for the business. Provide a concise reason.
        """.format(metric_name, val, pol_txt, user_query)

        safe_status_prompt = status_prompt.replace(DQ, "")
        
        status_options_str = json.dumps({
            "type": "json",
            "schema": {
                "type": "object",
                "properties": {
                    "is_bad": { "type": "boolean" },
                    "reason": { "type": "string" }
                },
                "required": ["is_bad", "reason"],
                "additionalProperties": False  # <--- ADD THIS LINE
            }
        })

        cmd = (
            "SELECT AI_COMPLETE("
            "model => 'openai-gpt-4.1', "
            "prompt => " + DQ + safe_status_prompt + DQ + ", "
            "response_format => PARSE_JSON(" + DQ + status_options_str + DQ + ")"
            ")"
        )
        
        resp = session.sql(cmd).collect()
        raw_result = resp[0][0]
        
        if isinstance(raw_result, str):
            status_obj = StatusCheckResponse.model_validate_json(raw_result)
        else:
            status_obj = StatusCheckResponse.model_validate(raw_result)
            
        status = "bad" if status_obj.is_bad else "good"
        
        return {
            "id": node_id,
            "name": metric_name,
            "value": str(val),
            "status": status,
            "reason": status_obj.reason,
            "query_type": query_type,
            "executed_sql": executed_sql
        }
        
    except Exception as e:
        log_debug(session, f"STATUS_FAIL_{node_id}", str(e))
        return {
            "id": node_id, 
            "name": metric_name, 
            "value": str(val), 
            "status": "unknown", 
            "error": "Status check failed",
            "query_type": query_type,
            "executed_sql": executed_sql
        }

# ==========================================
# 4. BATCH EXECUTOR
# ==========================================
def execute_batch(session: Session, nodes_list: list, user_query: str) -> list:
    if not nodes_list: 
        return []
    
    log_debug(session, "BATCH_START", f"Processing batch of {len(nodes_list)} nodes")
    
    # Efficient Metadata Loading
    DQ = "$" + "$"
    safe_names = [f"{DQ}{n['metric']}{DQ}" for n in nodes_list]
    names_str = ",".join(safe_names)
    
    meta_sql = f"SELECT METRIC_NAME, SQL_QUERY, FEW_SHOT_EXAMPLES FROM KPI_KNOWLEDGE_BASE_NEW WHERE METRIC_NAME IN ({names_str})"
    meta_df = session.sql(meta_sql).to_pandas()
    meta_map = {row['METRIC_NAME']: row.to_dict() for _, row in meta_df.iterrows()}
    
    results = []
    # Parallel Processing
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        futures = [executor.submit(process_node, session, n, user_query, meta_map) for n in nodes_list]
        for f in concurrent.futures.as_completed(futures):
            results.append(f.result())
            
    return results

# ==========================================
# 5. MAIN ENTRY POINT
# ==========================================
def main(session: Session, user_query: str, parent_node_ids_json: str, tree_id: str):
    try:
        log_debug(session, "MAIN_ENTRY", f"TreeID: {tree_id}, Query: {user_query}")

        # 1. Load Decision Tree Structure
        tree_row = session.sql("SELECT GRAPH_JSON FROM DECISION_TREE_STORE_NEW WHERE TREE_ID = ?", params=[tree_id]).collect()
        
        if not tree_row: 
            return json.dumps({"error": f"Tree ID not found: {tree_id}"})
        
        full_json = json.loads(tree_row[0]['GRAPH_JSON'])
        graph = full_json.get('graph', full_json)
        
        # Build Lookup Maps
        all_nodes_map = {n['id']: n for n in graph.get('nodes', [])}
        name_to_node_map = {n['metric']: n for n in graph.get('nodes', [])}
        all_edges = graph.get('edges', [])

        # 2. Determine Strategy (Search vs Drill)
        targets_phase_1 = []
        
        # Robust Null Check
        is_drill_mode = (
            parent_node_ids_json is not None and 
            parent_node_ids_json != '' and 
            parent_node_ids_json.upper() != 'NULL' and 
            parent_node_ids_json != '[]'
        )

        if not is_drill_mode and user_query:
            log_debug(session, "STRATEGY", "Vector Search (No parent IDs provided)")
            # Vector Search for Entry Point
            vec_sql = """
            SELECT METRIC_NAME FROM KPI_KNOWLEDGE_BASE_NEW 
            ORDER BY VECTOR_L2_DISTANCE(SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m-v1.5', ?), KPI_EMBEDDING) ASC 
            LIMIT 1
            """
            kb_row = session.sql(vec_sql, params=[user_query]).collect()
            if kb_row and kb_row[0]['METRIC_NAME'] in name_to_node_map:
                found_metric = kb_row[0]['METRIC_NAME']
                targets_phase_1.append(name_to_node_map[found_metric])
                log_debug(session, "ANCHOR_FOUND", f"Matched: {found_metric}")

        elif is_drill_mode:
            log_debug(session, "STRATEGY", "Drill Down (Parent IDs provided)")
            try:
                p_ids = json.loads(parent_node_ids_json)
                if not isinstance(p_ids, list): p_ids = [p_ids]
                parent_set = set(p_ids)
                
                # Find children of provided parents
                for edge in all_edges:
                    if edge['source'] in all_nodes_map and edge['target'] in parent_set:
                         # Note: Depending on edge direction (source->target), adjust accordingly.
                         # Assuming Source=Parent, Target=Child? Or Source=Metric, Target=Root?
                         # Usually standard DAG is Parent -> Child. 
                         # If input is 'Parent Nodes', we want their children.
                         # CHECK: Your previous code checked 'target' in parent_set. 
                         # If Edge is Source(Parent)->Target(Child), then Target in ParentSet means we are looking for parents?
                         # Let's assume the previous logic was correct: "Find nodes connected to these parents"
                         if edge['target'] in parent_set:
                             targets_phase_1.append(all_nodes_map[edge['source']])
            except Exception as e:
                log_debug(session, "DRILL_ERROR", str(e))

        # 3. Execute Phase 1 (Entry Nodes)
        results_phase_1 = execute_batch(session, targets_phase_1, user_query)

        # 4. Execute Phase 2 (Greedy Traversal of "Bad" Nodes)
        bad_ids = {r['id'] for r in results_phase_1 if r.get('status') == 'bad'}
        targets_phase_2 = []
        
        if bad_ids:
            log_debug(session, "PHASE_2", f"Expanding bad nodes: {list(bad_ids)}")
            for edge in all_edges:
                # If a node was bad, check its relationships
                if edge['target'] in bad_ids and edge['source'] in all_nodes_map:
                    targets_phase_2.append(all_nodes_map[edge['source']])
        
        results_phase_2 = execute_batch(session, targets_phase_2, user_query)

        # 5. Final Output
        return json.dumps({
            "nodes_analyzed": results_phase_1 + results_phase_2,
            "edges_traversed": [e for e in all_edges if e['target'] in bad_ids or e['source'] in bad_ids]
        })

    except Exception as e:
        log_debug(session, "MAIN_CRASH", str(e))
        return json.dumps({"error": "Critical Procedure Error", "details": str(e), "trace": traceback.format_exc()})
$$;



DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB
 / 
AI_POC
 / 
DIAGNOSTIC_TOOL(VARCHAR, VARCHAR, VARCHAR)


**Role & Objective**
You are an **Autonomous Root Cause Analysis Agent** designed to traverse a metric decision tree from the root node down to the leaf nodes. Your goal is to identify specific "bad" metrics driving a business problem by systematically drilling down into the data.

ALWAYS USE tree_id as Pharma_Master_v1.

**Available Tools**
You have access to a single diagnostic tool. You must use this tool for all data retrieval.

* `Diagnostic_tool(USER_QUERY: str, PARENT_NODE_IDS_JSON: str)`
* **Description:** Retrieves metric status for a given context. It operates in two modes:
1. **Search Mode:** Used to find the starting "Root" node based on a user's question.
2. **Drill Mode:** Used to find the children of specific parent nodes to investigate "bad" metrics further.


* **Arguments:**
* `USER_QUERY`: The user's natural language question (e.g., "Why is cost high?"). Set to `NULL` during Drill Mode.
* `PARENT_NODE_IDS_JSON`: A JSON string list of Node IDs (e.g., `'["node_123", "node_456"]'`). Set to `NULL` during Search Mode.


**Operational Procedure (The Analysis Loop)**
You must strictly follow this recursive logic for every user request:

**Step 1: Initialization (Find the Root)**

* If this is the start of the conversation, call `Diagnostic_tool` in **Search Mode**.
* **Action:** Call `Diagnostic_tool(USER_QUERY=user_question, PARENT_NODE_IDS_JSON=NULL)`.
* **Next:** Proceed to Step 2 with the results.

**Step 2: Status Evaluation & Decision**
Analyze the list of nodes returned by the tool. Focus **only** on nodes where `"status": "bad"`.

* **Check:** For every "bad" node, do you see its children in the current list?
* **Condition A (Drill Needed):** If a node is "bad" and you do **not** see its children, you must drill down. Collect these Node IDs.
* **Condition B (Keep Going):** If a node is "bad" but you **already** see its children, ignore it and evaluate the children instead.
* **Condition C (Stop):** If you find "bad" nodes but they have no children (empty results from a drill), these are your **Root Causes**.

**Step 3: Execution (Drill Down)**

* If Condition A occurred: Call `Diagnostic_tool` in **Drill Mode**.
* **Action:** Call `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["ID_1", "ID_2"]')`.


* **Next:** Recursively repeat Step 2 with the new output.
* If Condition C occurred: Present the final root causes to the user.

**Constraints**

* **Never** guess the status of a metric; always use the tool.
* **Never** stop at a "bad" node if it has unexplored children; you must find the leaf node.
* Output your reasoning briefly before calling the tool.

---

### **Few-Shot Example: Root Cause Analysis**

**User:** "Why is profitability down?"

**Assistant:**
*Thought: The user is asking a high-level question about a metric. I need to find the starting node and its immediate children to see what is happening.*
**Tool Use:** `Diagnostic_tool(USER_QUERY='Why is profitability down?', PARENT_NODE_IDS_JSON=NULL)`

**Tool Output:**

```json
[
  {"id": "N0", "metric": "Profitability", "status": "bad"},
  {"id": "N1", "metric": "Revenue", "status": "good"},
  {"id": "N2", "metric": "Total Cost", "status": "bad"}
]

```

**Assistant:**
*Thought: I have the initial results. `N0` (Profitability) is "bad", which confirms the user's issue. I see two children: `N1` (Revenue) is "good", so that branch is healthy. However, `N2` (Total Cost) is "bad". Since I do not see the children of `N2` in the current list, I must drill down into `N2` to understand what is driving the high cost.*
**Tool Use:** `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["N2"]')`

**Tool Output:**

```json
[
  {"id": "N3", "metric": "Fixed Cost", "status": "good"},
  {"id": "N4", "metric": "Variable Cost", "status": "bad"}
]

```

**Assistant:**
*Thought: I have drilled into Total Cost. `N3` (Fixed Cost) is "good", so I can ignore it. `N4` (Variable Cost) is "bad". I do not see the children of `N4` yet. I need to drill deeper to see if there is a more specific cause.*
**Tool Use:** `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["N4"]')`

**Tool Output:**

```json
[]

```

**Assistant:**
*Thought: The tool returned an empty list for `N4`. This indicates that `N4` (Variable Cost) is a leaf node in the decision tree. I have reached the bottom of the "bad" chain. I can now conclude the diagnosis.*
**Final Answer:** Profitability is down because **Variable Cost (N4)** is performing poorly. While Revenue and Fixed Costs are stable, the high Variable Cost is driving the overall decline.


**Response Structure (STRICT)**
You must output exactly two sections in the following order. Do not include any conversational filler before or after these sections.

#### **SECTION 1: Diagnostic Executive Summary**

* **Tone:** Professional, constructive, and polite. Even if metrics are "bad," frame the insights objectively (e.g., instead of "Critical Failure," use "Performance Deviation").
* **Headline:** State the Root Cause clearly (e.g., *"Root Cause Identified: [Deepest Bad Metric Name]"*).
* **The Narrative:** Briefly explain the chain of events in simple English. Start from the top node and explain the path of "bad" metrics that led to the root cause. Mention which branches were ruled out because they were "stable" or "good."
* **Actionable Insight:** Based on the metadata or description of the root node, suggest a generic, helpful next step.

---

#### **SECTION 2: React Flow JSON Artifact**

You must generate a valid JSON object inside a Markdown code block (`json ... `). You act as the layout engine.

**1. Coordinate Logic (The "Mental Layout"):**
Assign `position: {x, y}` to nodes based on their hierarchy level to ensure a clean tree structure:

* **Level 0 (Root):** `x: 0, y: 0`
* **Level 1 Children:** `y: 150`. Spread `x` widely (e.g., `-300` and `300`).
* **Level 2 Grandchildren:** `y: 300`. Spread `x` relative to their parent (e.g., `parent_x - 100` and `parent_x + 100`).
* **Level 3+:** Increase `y` by `150` for each level.

**2. Visual Style Logic:**

* **Red (`#FFE2E5`):** Status is "bad", "high", or "error".
* **Green (`#CDE8E6`):** Status is "good", "normal", or "stable".
* **Orange (`#FFCCBC`):** The **Root Cause** (the deepest "bad" node in the chain with no "bad" children).
* **White (`#FFFFFF`):** Unexplored or neutral nodes.

**3. Edge Labeling Logic:**

* Every edge must have a `label` explaining the relationship.
* If Parent is Bad and Child is Bad, label = "contributes to impact".
* If Parent is Bad and Child is Good, label = "stable factor".

**4. JSON Schema (Must Follow Exactly):**
Construct the JSON using this structure. Ensure all strings are double-quoted.

```json
{
  "nodes": [
    {
      "id": "N0",
      "type": "default",
      "data": { "label": "Metric Name: Value" },
      "position": { "x": 0, "y": 0 },
      "style": { 
         "background": "#FFE2E5", 
         "width": 180, 
         "color": "#333", 
         "border": "1px solid #777", 
         "borderRadius": "8px" 
      }
    }
  ],
  "edges": [
    {
      "id": "e-N0-N1",
      "source": "N0",
      "target": "N1",
      "label": "driven by",
      "animated": true,
      "style": { "stroke": "#555", "strokeWidth": 2 }
    }
  ]
}

```

---

### **Few-Shot Example**

**User:** "Visualize the results for the Profitability analysis."
*(Context provided: Profit is down, Revenue is stable, Cost is up, Variable Cost is up)*

**Assistant:**

#### **SECTION 1: Diagnostic Executive Summary**

**Root Cause Identified: Variable Cost (N3)**
We have completed the analysis of your Profitability metrics. The investigation began with **Profitability**, which is currently tracking below target. To understand why, we analyzed its two main drivers: **Revenue** and **Total Cost**.

We found that Revenue is stable and performing well, which is great news. However, **Total Cost** is higher than expected. By drilling deeper into the cost structure, we identified that while Fixed Costs are normal, **Variable Costs** are significantly elevated.

**Actionable Insight:** We recommend reviewing the primary components of your variable spend, such as raw material procurement or hourly labor efficiency, to identify potential savings.

---

#### **SECTION 2: React Flow JSON Artifact**

```json
{
  "nodes": [
    {
      "id": "N0",
      "type": "default",
      "data": { "label": "Profitability: -15%" },
      "position": { "x": 0, "y": 0 },
      "style": { "background": "#FFE2E5", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }
    },
    {
      "id": "N1",
      "type": "default",
      "data": { "label": "Revenue: $1.2M" },
      "position": { "x": -300, "y": 150 },
      "style": { "background": "#CDE8E6", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }
    },
    {
      "id": "N2",
      "type": "default",
      "data": { "label": "Total Cost: $900k" },
      "position": { "x": 300, "y": 150 },
      "style": { "background": "#FFE2E5", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }
    },
    {
      "id": "N3",
      "type": "default",
      "data": { "label": "Variable Cost: $600k" },
      "position": { "x": 300, "y": 300 },
      "style": { "background": "#FFCCBC", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }
    }
  ],
  "edges": [
    {
      "id": "e-N0-N1",
      "source": "N0",
      "target": "N1",
      "label": "stable factor",
      "animated": false
    },
    {
      "id": "e-N0-N2",
      "source": "N0",
      "target": "N2",
      "label": "driven by",
      "animated": true,
      "style": { "stroke": "#FF0000", "strokeWidth": 2 }
    },
    {
      "id": "e-N2-N3",
      "source": "N2",
      "target": "N3",
      "label": "primary driver",
      "animated": true,
      "style": { "stroke": "#FF0000", "strokeWidth": 2 }
    }
  ]
}

```


 response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {}, 
            payload, 
            {}, 
            TIMEOUT_MS
        )
        
        response_content = response_dict.get("content", "")
        if not response_content: return "Error: Empty response"
            
        parsed_response = json.loads(response_content)

        # --- PARSING ---
        if isinstance(parsed_response, dict) and 'message' in parsed_response:
            content_items = parsed_response['message'].get('content', [])
            for item in reversed(content_items):
                if isinstance(item, dict) and item.get("type") == "text":
                    return item.get("text")
        
        # Fallback for list/stream format
        elif isinstance(parsed_response, list):
            for item in parsed_response:
                if item.get('event') == 'response':
                    try:
                        content_list = item.get('data', {}).get('content', [])
                        for part in content_list:
                            if part.get('type') == 'text':
                                return part.get('text')
                    except Exception: continue
            
            # Simple Text Delta reconstruction
            chunks = []
            for item in parsed_response:
                if item.get('event') == 'response.text.delta':
                    chunks.append(item.get('data', {}).get('text', ''))
            if chunks: return "".join(chunks)

        return "Could not parse response."
100357 (P0000): Cannot create a Python function with the specified packages. Please check your packages specification and try again.






import json
import logging
import _snowflake # Native Snowflake module for internal API calls
from typing import List, Dict, Any, Optional, Literal
from pydantic import BaseModel, Field
from snowflake.snowpark import Session

# ==========================================
# 1. PYDANTIC MODELS
# ==========================================

class PlannerDecision(BaseModel):
    """
    Structured output from the Planner Agent.
    """
    route: Literal['diagnostic', 'clarify'] = Field(..., description="The designated agent to handle the request")
    query: str = Field(..., description="The refined query to pass to the agent")
    reasoning: str = Field(..., description="Why this route was chosen")

class DiagnosticResult(BaseModel):
    """
    Structured output from the Diagnostic Agent.
    """
    summary_text: str = Field(..., description="Executive summary of findings")
    react_flow_json: Optional[Dict] = Field(None, description="The visual graph data")
    clarifying_question: Optional[str] = Field(None, description="Question to ask user if intent is unclear")

class AgentConfig(BaseModel):
    model: str = "claude-4-sonnet"
    diagnostic_udf: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.DIAGNOSTIC_TOOL"
    warehouse: str = "DEV_IMPACT_WH"

# ==========================================
# 2. CORTEX API WRAPPER (Native _snowflake)
# ==========================================

def invoke_cortex_agent(session: Session, payload: Dict) -> Dict:
    """
    Invokes the Cortex Agent API using the internal _snowflake module.
    Parses specific event streams for text, charts, tables, and tool results.
    """
    AGENT_ENDPOINT_URL = "/api/v2/cortex/agent:run"
    TIMEOUT_MS = 90000  # 90 seconds
    
    # Construct headers - Authorization is handled internally by _snowflake but headers dict is required
    headers = {
        "Content-Type": "application/json",
        "Accept": "text/event-stream"
    }

    try:
        # Native Snowflake API call
        response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {}, 
            payload, 
            {}, 
            TIMEOUT_MS
        )
        
        response_content = response_dict.get("content", "")
        if not response_content:
            return {"error": "Empty response from Agent API"}
            
        # The internal API often returns the stream as a valid JSON list of event objects
        # or a single JSON object if not streaming.
        try:
            parsed_response = json.loads(response_content)
        except json.JSONDecodeError:
            return {"error": "Failed to decode JSON response content"}

        # --- COMPREHENSIVE PARSING ---
        # We need to extract all artifacts: Text, Tables, Charts, Tool info, etc.
        extracted = {
            "text": "",
            "tables": [],
            "charts": [],
            "tool_uses": [],
            "annotations": [],
            "debug_trace": [] # For thinking, status, etc.
        }

        # Helper to process a content list (from final 'response' or individual events)
        def process_content_list(content_items):
            for item in content_items:
                c_type = item.get("type")
                if c_type == "text":
                    extracted["text"] += item.get("text", "")
                    if item.get("annotations"):
                        extracted["annotations"].extend(item["annotations"])
                elif c_type == "table":
                    extracted["tables"].append(item.get("result_set", {}))
                elif c_type == "chart":
                    extracted["charts"].append(item.get("chart", {}))
                elif c_type == "tool_use":
                    extracted["tool_uses"].append(item)

        # 1. Handle List of Events (Streaming format)
        if isinstance(parsed_response, list):
            # First pass: Look for the final 'response' event which is the aggregation
            final_response_event = next((item for item in parsed_response if item.get('event') == 'response'), None)
            
            if final_response_event:
                # Best Case: We have the fully aggregated response
                data = final_response_event.get('data', {})
                process_content_list(data.get('content', []))
            else:
                # Fallback: Reconstruct from deltas
                text_parts = []
                for item in parsed_response:
                    evt = item.get('event')
                    data = item.get('data', {})
                    
                    if evt == 'response.text.delta':
                        text_parts.append(data.get('text', ''))
                    elif evt == 'response.text.annotation':
                        extracted["annotations"].append(data.get('annotation'))
                    elif evt == 'response.chart':
                        extracted["charts"].append(data.get('chart') or data.get('chart_spec'))
                    elif evt == 'response.table':
                        extracted["tables"].append(data.get('result_set'))
                    elif evt == 'response.tool_use':
                        extracted["tool_uses"].append(data)
                    elif evt == 'response.thinking.delta':
                        extracted["debug_trace"].append(f"Thinking: {data.get('text')}")
                    elif evt == 'response.status':
                        extracted["debug_trace"].append(f"Status: {data.get('status')}")
                    elif evt == 'error':
                        return {"error": f"Agent Error: {data.get('message')}"}
                
                extracted["text"] = "".join(text_parts)

        # 2. Handle Single Object (Non-streaming or final format)
        elif isinstance(parsed_response, dict):
            if 'message' in parsed_response:
                process_content_list(parsed_response['message'].get('content', []))
            elif 'choices' in parsed_response: # Legacy/Standard REST format
                process_content_list(parsed_response['choices'][0]['message'].get('content', []))

        return extracted

    except Exception as e:
        return {"error": f"API Execution Exception: {str(e)}"}

# ==========================================
# 3. PAYLOAD FACTORY
# ==========================================

class PayloadFactory:
    @staticmethod
    def create(
        query: str,
        instructions: Dict[str, str],
        tools: List[Dict] = None,
        tool_resources: Dict = None,
        history: List[Dict] = None
    ) -> Dict:
        config = AgentConfig()
        messages = history if history else []
        messages.append({"role": "user", "content": [{"type": "text", "text": query}]})

        return {
            # "thread_id": 0,
            # "parent_message_id": 0,
            "messages": messages,
            "tool_choice": {"type": "auto"},
            "models": {"orchestration": config.model},
            "instructions": instructions,
            "tools": tools or [],
            "tool_resources": tool_resources or {}
        }

# ==========================================
# 4. AGENT LOGIC
# ==========================================

class AgentManager:
    def __init__(self, session: Session):
        self.session = session
        self.config = AgentConfig()

    def run_rephraser(self, query: str, history: List[Dict]) -> str:
        """Step 1: Rephrase Query"""
        instructions = {
            "system": "You are a sophisticated Query Refinement Engine.",
            "response": "Output ONLY the final rephrased string. Do not include prefixes like 'Rephrased:'.",
            "orchestration": (
                "Your goal is to transform the user's latest input into a fully standalone, unambiguous query.\n"
                "1. Resolve all pronouns (it, they, that, this) using the conversation history.\n"
                "2. If the user refers to a specific metric or ID from history, explicitly include it.\n"
                "3. If the query is already clear, return it unchanged.\n\n"
                "**Few-Shot Examples:**\n"
                "- Context: [User: 'How is Revenue?'] -> Input: 'Why did it drop?' -> Output: 'Why did Revenue drop?'\n"
                "- Context: [] -> Input: 'Explain the dip in Profit' -> Output: 'Explain the dip in Profit'\n"
                "- Context: [Asst: 'Cost is high'] -> Input: 'Drill down' -> Output: 'Drill down into Cost'"
            )
        }
        
        payload = PayloadFactory.create(
            f"Rephrase this to be standalone: {query}",
            instructions,
            history=history
        )
        
        resp = invoke_cortex_agent(self.session, payload)
        
        # Extracted text is at the top level now
        return resp.get('text', query).strip() or query

    def run_planner(self, query: str) -> PlannerDecision:
        """Step 2: Plan Route"""
        schema_hint = '{"route": "diagnostic" | "clarify", "query": "string", "reasoning": "string"}'
        instructions = {
            "system": "You are a Strategic Intent Classifier.",
            "response": f"Respond ONLY with valid JSON matching: {schema_hint}",
            "orchestration": (
                "Analyze the user's input to determine the precise analytical needs.\n"
                "**Routing Logic:**\n"
                "1. **diagnostic**: Use for questions seeking explanations, root causes, drivers, or specific metric analysis (e.g., 'Why', 'What caused', 'Analyze', 'Drill down').\n"
                "2. **clarify**: Use for vague inputs, greetings, or when the metric/topic is completely missing (e.g., 'Hello', 'Help', 'Data').\n\n"
                "**Few-Shot Examples:**\n"
                "- Input: 'Why is Net Income trending down?' -> Route: 'diagnostic'\n"
                "- Input: 'Can you explain the variance in COGS?' -> Route: 'diagnostic'\n"
                "- Input: 'Hi, how are you?' -> Route: 'clarify'\n"
                "- Input: 'What is the root cause?' -> Route: 'diagnostic'"
            )
        }
        
        payload = PayloadFactory.create(f"Analyze this query: {query}", instructions)
        resp = invoke_cortex_agent(self.session, payload)
        
        try:
            raw_text = resp.get('text', '')
            clean_json = raw_text.replace("```json", "").replace("```", "").strip()
            return PlannerDecision.model_validate_json(clean_json)
        except Exception:
            return PlannerDecision(route="diagnostic", query=query, reasoning="Fallback error")

    def run_diagnostic_agent(self, query: str) -> DiagnosticResult:
        """Step 3: Run Diagnostic Tool"""
        response_format_instruction = """
        You must output a JSON object with these exact keys:
        {
            "summary_text": "The executive summary...",
            "react_flow_json": { ... valid React Flow JSON object ... },
            "clarifying_question": null or "Question string if needed"
        }
        """

        instructions = {
            "system": "You are an Autonomous Root Cause Analysis Agent.",
            "orchestration": f"""**Role & Objective**

You are an **Autonomous Root Cause Analysis Agent** designed to traverse a metric decision tree from the root node down to the leaf nodes. Your goal is to identify specific "bad" metrics driving a business problem by systematically drilling down into the data.



ALWAYS USE tree_id as Pharma_Master_v1.



**Available Tools**

You have access to a single diagnostic tool. You must use this tool for all data retrieval.



* `Diagnostic_tool(USER_QUERY: str, PARENT_NODE_IDS_JSON: str)`

* **Description:** Retrieves metric status for a given context. It operates in two modes:

1. **Search Mode:** Used to find the starting "Root" node based on a user's question.

2. **Drill Mode:** Used to find the children of specific parent nodes to investigate "bad" metrics further.





* **Arguments:**

* `USER_QUERY`: The user's natural language question (e.g., "Why is cost high?"). Set to `NULL` during Drill Mode.

* `PARENT_NODE_IDS_JSON`: A JSON string list of Node IDs (e.g., `'["node_123", "node_456"]'`). Set to `NULL` during Search Mode.





**Operational Procedure (The Analysis Loop)**

You must strictly follow this recursive logic for every user request:



**Step 1: Initialization (Find the Root)**



* If this is the start of the conversation, call `Diagnostic_tool` in **Search Mode**.

* **Action:** Call `Diagnostic_tool(USER_QUERY=user_question, PARENT_NODE_IDS_JSON=NULL)`.

* **Next:** Proceed to Step 2 with the results.



**Step 2: Status Evaluation & Decision**

Analyze the list of nodes returned by the tool. Focus **only** on nodes where `"status": "bad"`.



* **Check:** For every "bad" node, do you see its children in the current list?

* **Condition A (Drill Needed):** If a node is "bad" and you do **not** see its children, you must drill down. Collect these Node IDs.

* **Condition B (Keep Going):** If a node is "bad" but you **already** see its children, ignore it and evaluate the children instead.

* **Condition C (Stop):** If you find "bad" nodes but they have no children (empty results from a drill), these are your **Root Causes**.



**Step 3: Execution (Drill Down)**



* If Condition A occurred: Call `Diagnostic_tool` in **Drill Mode**.

* **Action:** Call `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["ID_1", "ID_2"]')`.





* **Next:** Recursively repeat Step 2 with the new output.

* If Condition C occurred: Present the final root causes to the user.



**Constraints**



* **Never** guess the status of a metric; always use the tool.

* **Never** stop at a "bad" node if it has unexplored children; you must find the leaf node.

* Output your reasoning briefly before calling the tool.



---



### **Few-Shot Example: Root Cause Analysis**



**User:** "Why is profitability down?"



**Assistant:**

*Thought: The user is asking a high-level question about a metric. I need to find the starting node and its immediate children to see what is happening.*

**Tool Use:** `Diagnostic_tool(USER_QUERY='Why is profitability down?', PARENT_NODE_IDS_JSON=NULL)`



**Tool Output:**



```json

[

  {"id": "N0", "metric": "Profitability", "status": "bad"},

  {"id": "N1", "metric": "Revenue", "status": "good"},

  {"id": "N2", "metric": "Total Cost", "status": "bad"}

]



```



**Assistant:**

*Thought: I have the initial results. `N0` (Profitability) is "bad", which confirms the user's issue. I see two children: `N1` (Revenue) is "good", so that branch is healthy. However, `N2` (Total Cost) is "bad". Since I do not see the children of `N2` in the current list, I must drill down into `N2` to understand what is driving the high cost.*

**Tool Use:** `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["N2"]')`



**Tool Output:**



```json

[

  {"id": "N3", "metric": "Fixed Cost", "status": "good"},

  {"id": "N4", "metric": "Variable Cost", "status": "bad"}

]



```



**Assistant:**

*Thought: I have drilled into Total Cost. `N3` (Fixed Cost) is "good", so I can ignore it. `N4` (Variable Cost) is "bad". I do not see the children of `N4` yet. I need to drill deeper to see if there is a more specific cause.*

**Tool Use:** `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["N4"]')`



**Tool Output:**



```json

[]



```



**Assistant:**

*Thought: The tool returned an empty list for `N4`. This indicates that `N4` (Variable Cost) is a leaf node in the decision tree. I have reached the bottom of the "bad" chain. I can now conclude the diagnosis.*

**Final Answer:** Profitability is down because **Variable Cost (N4)** is performing poorly. While Revenue and Fixed Costs are stable, the high Variable Cost is driving the overall decline.
""",
            "response": f"""**Response Structure (STRICT)**

You must output exactly two sections in the following order. Do not include any conversational filler before or after these sections.



#### **SECTION 1: Diagnostic Executive Summary**



* **Tone:** Professional, constructive, and polite. Even if metrics are "bad," frame the insights objectively (e.g., instead of "Critical Failure," use "Performance Deviation").

* **Headline:** State the Root Cause clearly (e.g., *"Root Cause Identified: [Deepest Bad Metric Name]"*).

* **The Narrative:** Briefly explain the chain of events in simple English. Start from the top node and explain the path of "bad" metrics that led to the root cause. Mention which branches were ruled out because they were "stable" or "good."

* **Actionable Insight:** Based on the metadata or description of the root node, suggest a generic, helpful next step.



---



#### **SECTION 2: React Flow JSON Artifact**



You must generate a valid JSON object inside a Markdown code block (`json ... `). You act as the layout engine.



**1. Coordinate Logic (The "Mental Layout"):**

Assign `position: {x, y}` to nodes based on their hierarchy level to ensure a clean tree structure:



* **Level 0 (Root):** `x: 0, y: 0`

* **Level 1 Children:** `y: 150`. Spread `x` widely (e.g., `-300` and `300`).

* **Level 2 Grandchildren:** `y: 300`. Spread `x` relative to their parent (e.g., `parent_x - 100` and `parent_x + 100`).

* **Level 3+:** Increase `y` by `150` for each level.



**2. Visual Style Logic:**



* **Red (`#FFE2E5`):** Status is "bad", "high", or "error".

* **Green (`#CDE8E6`):** Status is "good", "normal", or "stable".

* **Orange (`#FFCCBC`):** The **Root Cause** (the deepest "bad" node in the chain with no "bad" children).

* **White (`#FFFFFF`):** Unexplored or neutral nodes.



**3. Edge Labeling Logic:**



* Every edge must have a `label` explaining the relationship.

* If Parent is Bad and Child is Bad, label = "contributes to impact".

* If Parent is Bad and Child is Good, label = "stable factor".



**4. JSON Schema (Must Follow Exactly):**

Construct the JSON using this structure. Ensure all strings are double-quoted.



```json

{

  "nodes": [

    {

      "id": "N0",

      "type": "default",

      "data": { "label": "Metric Name: Value" },

      "position": { "x": 0, "y": 0 },

      "style": { 

         "background": "#FFE2E5", 

         "width": 180, 

         "color": "#333", 

         "border": "1px solid #777", 

         "borderRadius": "8px" 

      }

    }

  ],

  "edges": [

    {

      "id": "e-N0-N1",

      "source": "N0",

      "target": "N1",

      "label": "driven by",

      "animated": true,

      "style": { "stroke": "#555", "strokeWidth": 2 }

    }

  ]

}



```



---



### **Few-Shot Example**



**User:** "Visualize the results for the Profitability analysis."

*(Context provided: Profit is down, Revenue is stable, Cost is up, Variable Cost is up)*



**Assistant:**



#### **SECTION 1: Diagnostic Executive Summary**



**Root Cause Identified: Variable Cost (N3)**

We have completed the analysis of your Profitability metrics. The investigation began with **Profitability**, which is currently tracking below target. To understand why, we analyzed its two main drivers: **Revenue** and **Total Cost**.



We found that Revenue is stable and performing well, which is great news. However, **Total Cost** is higher than expected. By drilling deeper into the cost structure, we identified that while Fixed Costs are normal, **Variable Costs** are significantly elevated.



**Actionable Insight:** We recommend reviewing the primary components of your variable spend, such as raw material procurement or hourly labor efficiency, to identify potential savings.



---



#### **SECTION 2: React Flow JSON Artifact**



```json

{

  "nodes": [

    {

      "id": "N0",

      "type": "default",

      "data": { "label": "Profitability: -15%" },

      "position": { "x": 0, "y": 0 },

      "style": { "background": "#FFE2E5", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }

    },

    {

      "id": "N1",

      "type": "default",

      "data": { "label": "Revenue: $1.2M" },

      "position": { "x": -300, "y": 150 },

      "style": { "background": "#CDE8E6", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }

    },

    {

      "id": "N2",

      "type": "default",

      "data": { "label": "Total Cost: $900k" },

      "position": { "x": 300, "y": 150 },

      "style": { "background": "#FFE2E5", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }

    },

    {

      "id": "N3",

      "type": "default",

      "data": { "label": "Variable Cost: $600k" },

      "position": { "x": 300, "y": 300 },

      "style": { "background": "#FFCCBC", "width": 180, "border": "1px solid #777", "borderRadius": "8px" }

    }

  ],

  "edges": [

    {

      "id": "e-N0-N1",

      "source": "N0",

      "target": "N1",

      "label": "stable factor",

      "animated": false

    },

    {

      "id": "e-N0-N2",

      "source": "N0",

      "target": "N2",

      "label": "driven by",

      "animated": true,

      "style": { "stroke": "#FF0000", "strokeWidth": 2 }

    },

    {

      "id": "e-N2-N3",

      "source": "N2",

      "target": "N3",

      "label": "primary driver",

      "animated": true,

      "style": { "stroke": "#FF0000", "strokeWidth": 2 }

    }

  ]

}



```.

Ensure your final answer is strictly valid JSON matching: {response_format_instruction}"""
        }

        tools = [{
            "tool_spec": {
                "type": "generic",
                "name": "Diagnostic_tool",
                "description": "Retrieves metric status. Search Mode or Drill Mode.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "USER_QUERY": {"type": "string", "description": "Query or NULL"},
                        "PARENT_NODE_IDS_JSON": {"type": "string", "description": "NULL or list of IDs"},
                        "TREE_ID": {"type": "string", "description": "Always 'Pharma_Master_v1'"}
                    },
                    "required": ["USER_QUERY", "PARENT_NODE_IDS_JSON", "TREE_ID"]
                }
            }
        }]

        tool_resources = {
            "Diagnostic_tool": {
                "type": "procedure",
                "execution_environment": {
                    "type": "warehouse",
                    "warehouse": self.config.warehouse
                },
                "identifier": self.config.diagnostic_udf
            }
        }

        payload = PayloadFactory.create(query, instructions, tools, tool_resources)
        resp = invoke_cortex_agent(self.session, payload)
        print(resp)
        try:
            # The agent returns the JSON structure inside the 'text' field
            raw_text = resp.get('text', '')
            clean_content = raw_text.replace("```json", "").replace("```", "").strip()
            return DiagnosticResult.model_validate_json(clean_content)
        except Exception as e:
            return DiagnosticResult(
                summary_text=f"Analysis failed to format correctly. Agent Output: {resp.get('text', '')}", 
                react_flow_json=None
            )

# ==========================================
# 5. MAIN HANDLER
# ==========================================

def main(session: Session):
    manager = AgentManager(session)
    user_query = "Why is the Total Market Volume being driven by the Opioid Category , specifically Tramadol which is seeing massive New Growth, yet the Orthopedic Segment historically the core driver for this drugis contributing less than 7% of that growth?"
    # Simulating a conversation where the user previously asked about performance
    history = [
        {"role": "user", "content": [{"type": "text", "text": "How is the overall Profitability looking?"}]},
        {"role": "assistant", "content": [{"type": "text", "text": "Profitability is currently down by 15% compared to last month."}]}
    ]
    
    # 1. Rephrase
    rephrased = manager.run_rephraser(user_query, history)
    print(rephrased)
    # 2. Plan
    decision = manager.run_planner(rephrased)
    print(decision)
    
    final_output = {}

    if decision.route == "diagnostic":
        # 3. Diagnose
        result = manager.run_diagnostic_agent(decision.query)
        
        if result.clarifying_question:
            final_output = {"type": "clarification", "message": result.clarifying_question}
        else:
            final_output = {
                "type": "diagnostic_report",
                "summary": result.summary_text,
                "graph": result.react_flow_json
            }
    elif decision.route == "clarify":
        final_output = {"type": "clarification", "message": decision.reasoning}

    return final_output



import json
import logging
import _snowflake # Native Snowflake module for internal API calls
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Literal
from pydantic import BaseModel, Field
from snowflake.snowpark import Session

# ==========================================
# 1. PYDANTIC CONFIGURATION & MODELS
# ==========================================

class AgentConfig(BaseModel):
    """Configuration for the agent execution"""
    model: str = "claude-4-sonnet"
    # Update paths as per your environment
    diagnostic_udf: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.DIAGNOSTIC_TOOL"
    # Updated to point to an existing Cortex Analyst Object (Semantic Model)
    cortex_analyst_object: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.LATEST_MASTER_VIEW"
    warehouse: str = "DEV_IMPACT_WH"

class PlannerDecision(BaseModel):
    """
    Structured output from the Planner Agent.
    Allows for parallel execution by defining multiple query targets.
    """
    sql_query: Optional[str] = Field(None, description="Query for data analysis, metrics, or lists. Null if not needed.")
    diagnostic_query: Optional[str] = Field(None, description="Query for root cause analysis or deep dives. Null if not needed.")
    clarification: Optional[str] = Field(None, description="Question to ask user if intent is completely vague.")
    reasoning: str = Field(..., description="Why these routes were chosen")

class DiagnosticResult(BaseModel):
    """Structured output from the Diagnostic Agent"""
    summary_text: str = Field(..., description="Executive summary of findings")
    react_flow_json: Optional[Dict] = Field(None, description="The visual graph data")
    clarifying_question: Optional[str] = Field(None, description="Question to ask user if intent is unclear")

class SqlResult(BaseModel):
    """Structured output from the Cortex Analyst (SQL Agent)"""
    answer_text: str = Field(..., description="The text response from the analyst")
    tables: List[Any] = Field(default_factory=list, description="List of result sets (raw data)")
    charts: List[Dict] = Field(default_factory=list, description="List of Vega-Lite specs")
    # Extended Metadata Fields
    sql_generated: Optional[str] = Field(None, description="The generated SQL query")
    sql_explanation: Optional[str] = Field(None, description="Explanation of the generated SQL")
    is_verified_query: bool = Field(False, description="Whether a verified query was used")
    reasoning_trace: Optional[str] = Field(None, description="The thinking process of the analyst")

# Global Config Instance
CONFIG = AgentConfig()

# ==========================================
# 2. DEBUGGING / CHECKPOINT HELPER
# ==========================================

def log_checkpoint(step: str, details: Any):
    """
    Prints checkpoint info to stdout. Handles Pydantic models serialization.
    """
    print(f"\n{'='*50}")
    print(f" CHECKPOINT: {step}")
    print(f"{'-'*50}")
    
    if isinstance(details, BaseModel):
        print(details.model_dump_json(indent=2))
    elif isinstance(details, (dict, list)):
        print(json.dumps(details, indent=2))
    else:
        print(str(details))
    print(f"{'='*50}\n")

# ==========================================
# 3. CORTEX API WRAPPER
# ==========================================

def invoke_cortex_agent(session: Session, payload: Dict, agent_name: str) -> Dict:
    """
    Invokes the Cortex Agent API via _snowflake.send_snow_api_request.
    Includes robust parsing for streaming, especially Analyst Deltas.
    """
    AGENT_ENDPOINT_URL = "/api/v2/cortex/agent:run"
    TIMEOUT_MS = 120 * 1000  # 120 seconds
    
    headers = {
        "Content-Type": "application/json",
        "Accept": "text/event-stream"
    }

    log_checkpoint(f"INVOKING AGENT: {agent_name}", {
        "model": payload.get("models", {}).get("orchestration"),
        "tools": [t['tool_spec']['name'] for t in payload.get("tools", [])]
    })

    try:
        response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {}, 
            payload, 
            {}, 
            TIMEOUT_MS
        )
        
        response_content = response_dict.get("content", "")
        if not response_content:
            return {"error": "Empty response from Agent API"}
            
        # Try parsing as JSON
        try:
            parsed_response = json.loads(response_content)
        except json.JSONDecodeError:
            return {"error": "Failed to decode JSON response content"}

        # Extraction container
        extracted = {
            "text": "",
            "tables": [],
            "charts": [],
            "annotations": [],
            "tool_uses": [],
            "debug_trace": [],
            "sql_generated": None,
            "sql_explanation": None,
            "is_verified": False
        }

        # Helper to process a content list (from final 'response')
        def process_content_list(content_items):
            for item in content_items:
                c_type = item.get("type")
                if c_type == "text":
                    extracted["text"] += item.get("text", "")
                    if item.get("annotations"):
                        extracted["annotations"].extend(item["annotations"])
                elif c_type == "table":
                    extracted["tables"].append(item.get("result_set", {}))
                elif c_type == "chart":
                    extracted["charts"].append(item.get("chart", {}))
                elif c_type == "tool_use":
                    extracted["tool_uses"].append(item)

        # 1. Handle List of Events (Streaming format)
        if isinstance(parsed_response, list):
            # First pass: Look for the final 'response' event
            final_response_event = next((item for item in parsed_response if item.get('event') == 'response'), None)
            
            if final_response_event:
                data = final_response_event.get('data', {})
                process_content_list(data.get('content', []))
                
                # Even if we have final response, sometimes metadata like 'verified_query_used' 
                # is best captured from the deltas if the final aggregation missed detailed tool props.
                # So we scan deltas for analyst specifics below.
            
            # Scan deltas for reconstruction and metadata extraction
            text_parts = []
            for item in parsed_response:
                evt = item.get('event')
                data = item.get('data', {})
                
                if evt == 'response.text.delta':
                    text_parts.append(data.get('text', ''))
                elif evt == 'response.text.annotation':
                    extracted["annotations"].append(data.get('annotation'))
                elif evt == 'response.chart':
                    extracted["charts"].append(data.get('chart') or data.get('chart_spec'))
                elif evt == 'response.table':
                    extracted["tables"].append(data.get('result_set'))
                elif evt == 'response.tool_use':
                    extracted["tool_uses"].append(data)
                elif evt == 'response.thinking.delta':
                    extracted["debug_trace"].append(f"Think: {data.get('text')}")
                
                # --- CRITICAL: ANALYST TOOL DELTA PARSING ---
                elif evt == 'response.tool_result.analyst.delta':
                    delta = data.get('delta', {})
                    
                    # 1. SQL Code
                    if delta.get('sql'):
                        extracted["sql_generated"] = delta.get('sql')
                    
                    # 2. Explanation
                    if delta.get('sql_explanation'):
                        extracted["sql_explanation"] = delta.get('sql_explanation')
                    
                    # 3. Verified Query Flag
                    if 'verified_query_used' in delta:
                        extracted["is_verified"] = delta.get('verified_query_used')
                    
                    # 4. Result Set (The Table Data)
                    if delta.get('result_set'):
                        # Ensure we don't duplicate if response.table already caught it
                        # Simple check: if not in tables yet
                        rs = delta.get('result_set')
                        if rs not in extracted["tables"]:
                            extracted["tables"].append(rs)
                    
                    # 5. Reasoning/Think
                    if delta.get('think'):
                        extracted["debug_trace"].append(f"Analyst Think: {delta.get('think')}")
                    
                    # 6. Suggestions (Optional, appended to debug for now)
                    if delta.get('suggestions'):
                        sug = delta.get('suggestions')
                        extracted["debug_trace"].append(f"Suggestion: {sug.get('delta')}")

                elif evt == 'error':
                    return {"error": f"Agent Error: {data.get('message')}"}
            
            if not extracted["text"]:
                extracted["text"] = "".join(text_parts)

        # 2. Handle Single Object (Non-streaming)
        elif isinstance(parsed_response, dict):
            if 'message' in parsed_response:
                process_content_list(parsed_response['message'].get('content', []))
            elif 'choices' in parsed_response:
                process_content_list(parsed_response['choices'][0]['message'].get('content', []))

        return extracted

    except Exception as e:
        return {"error": f"API Execution Exception: {str(e)}"}

# ==========================================
# 4. PAYLOAD FACTORY
# ==========================================

class PayloadFactory:
    @staticmethod
    def create(query: str, instructions: Dict[str, str], tools: List[Dict] = None, tool_resources: Dict = None, history: List[Dict] = None) -> Dict:
        messages = history if history else []
        messages.append({"role": "user", "content": [{"type": "text", "text": query}]})

        return {
            "messages": messages,
            "tool_choice": {"type": "auto"},
            "models": {"orchestration": CONFIG.model},
            "instructions": instructions,
            "tools": tools or [],
            "tool_resources": tool_resources or {}
        }

# ==========================================
# 5. AGENT MANAGERS
# ==========================================

class AgentManager:
    def __init__(self, session: Session):
        self.session = session

    def run_rephraser(self, query: str, history: List[Dict] = None) -> str:
        """Step 1: Rephrase"""
        log_checkpoint("START STEP 1: Rephraser", f"Input: {query}")

        instructions = {
            "system": "You are a sophisticated Query Refinement Engine.",
            "response": "Output ONLY the final rephrased string.",
            "orchestration": (
                "Transform the user's input into a standalone query.\n"
                "1. Resolve pronouns (it, they, that).\n"
                "2. Preserve metrics and IDs.\n"
            )
        }
        
        payload = PayloadFactory.create(f"Rephrase this to be standalone: {query}", instructions, history=history)
        resp = invoke_cortex_agent(self.session, payload, "Rephraser")
        
        result = resp.get('text', query).strip() or query
        log_checkpoint("END STEP 1: Rephraser", f"Output: {result}")
        return result

    def run_planner(self, query: str) -> PlannerDecision:
        """Step 2: Plan Route"""
        log_checkpoint("START STEP 2: Planner", f"Input: {query}")

        # Schema defining optional fields for both routes
        schema_hint = '{"sql_query": "string or null", "diagnostic_query": "string or null", "clarification": "string or null", "reasoning": "string"}'
        
        instructions = {
            "system": "You are a Strategic Intent Classifier.",
            "response": f"Respond ONLY with a valid JSON object matching: {schema_hint}",
            "orchestration": (
                "Analyze the user's input.\n"
                "1. If the user asks for **Data/Lists/Aggregations** ('Show me', 'What is', 'List'), set `sql_query`.\n"
                "2. If the user asks for **Causes/Reasons/Explanations** ('Why', 'Explain', 'Drill down'), set `diagnostic_query`.\n"
                "3. If the query requires BOTH (e.g., 'What is revenue and why did it drop?'), fill BOTH fields.\n"
                "4. If vague, set `clarification`.\n"
                "5. **Visualize:** If sql_query is set, append ' and generate a table and chart' to it."
            )
        }
        
        payload = PayloadFactory.create(f"Plan this query: {query}", instructions)
        resp = invoke_cortex_agent(self.session, payload, "Planner")
        
        try:
            raw = resp.get('text', '')
            clean = raw.replace("```json", "").replace("```", "").strip()
            return PlannerDecision.model_validate_json(clean)
        except Exception:
            # Fallback assumption
            return PlannerDecision(diagnostic_query=query, reasoning="Fallback error during planning")

    def run_sql_agent(self, query: str) -> SqlResult:
        """Step 3b: Run Cortex Analyst (SQL Agent)"""
        log_checkpoint("START STEP 3b: SQL Agent", f"Input: {query}")

        instructions = {
            "system": "You are an expert Data Analyst.",
            "orchestration": "Use the analyst_tool to answer. Always generate a table for data requests. If specific numeric comparisons or trends are found, generate a chart.",
            "response": "Provide a summary. Tables/Charts are handled by the system."
        }

        tools = [{
            "tool_spec": {
                "type": "cortex_analyst_text_to_sql",
                "name": "analyst_tool"
            }
        }]

        tool_resources = {
            "analyst_tool": {
                "type": "cortex_analyst_text_to_sql",
                "semantic_view": CONFIG.cortex_analyst_object,
                "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse}
            }
        }

        payload = PayloadFactory.create(query, instructions, tools, tool_resources)
        resp = invoke_cortex_agent(self.session, payload, "SQL Agent")

        # Map parsed fields to SqlResult
        result = SqlResult(
            answer_text=resp.get('text', ''),
            tables=resp.get('tables', []),
            charts=resp.get('charts', []),
            sql_generated=resp.get('sql_generated'),
            sql_explanation=resp.get('sql_explanation'),
            is_verified_query=resp.get('is_verified', False),
            reasoning_trace="\n".join(resp.get('debug_trace', []))
        )
        
        log_checkpoint("END STEP 3b: SQL Agent", result)
        return result

    def run_diagnostic_agent(self, query: str) -> DiagnosticResult:
        """Step 3a: Run Diagnostic Tool"""
        log_checkpoint("START STEP 3a: Diagnostic", f"Input: {query}")

        response_format = '{"summary_text": "string", "react_flow_json": {}, "clarifying_question": null}'
        instructions = {
            "system": "You are an Autonomous Root Cause Analysis Agent.",
            "orchestration": "Use Diagnostic_tool. Loop: Search -> Evaluate -> Drill.",
            "response": f"Ensure valid JSON matching: {response_format}"
        }

        tools = [{
            "tool_spec": {
                "type": "generic",
                "name": "Diagnostic_tool",
                "description": "Retrieves metric status.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "USER_QUERY": {"type": "string"},
                        "PARENT_NODE_IDS_JSON": {"type": "string"},
                        "TREE_ID": {"type": "string"}
                    },
                    "required": ["USER_QUERY", "PARENT_NODE_IDS_JSON", "TREE_ID"]
                }
            }
        }]

        tool_resources = {
            "Diagnostic_tool": {
                "type": "procedure",
                "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse},
                "identifier": CONFIG.diagnostic_udf
            }
        }

        payload = PayloadFactory.create(query, instructions, tools, tool_resources)
        resp = invoke_cortex_agent(self.session, payload, "Diagnostic Agent")

        try:
            raw = resp.get('text', '{}')
            clean = raw.replace("```json", "").replace("```", "").strip()
            result = DiagnosticResult.model_validate_json(clean)
        except Exception as e:
            result = DiagnosticResult(summary_text=f"Error: {str(e)} - Raw: {raw}", react_flow_json=None)

        log_checkpoint("END STEP 3a: Diagnostic", result)
        return result

# ==========================================
# 6. MAIN EXECUTION (PARALLEL)
# ==========================================

def main(session: Session):
    manager = AgentManager(session)
    history = []
    
    # Example Input
    user_query = "show ME all the health care providers in arizona and Why is the Total Market Volume being driven by the Opioid Category , specifically Tramadol which is seeing massive New Growth, yet the Orthopedic Segment historically the core driver for this drugis contributing less than 7% of that growth?."
    
    log_checkpoint("CONTEXT", {"history": history, "current_query": user_query})
    
    # 1. Rephrase
    rephrased = manager.run_rephraser(user_query, history)
    
    # 2. Plan
    decision = manager.run_planner(rephrased)
    
    final_output = {
        "original_query": user_query,
        "processed_query": rephrased,
        "execution_plan": decision.model_dump(),
        "results": []
    }
    log_checkpoint("FINAL ANSWER BEFORE EXECUTION",{"final_output_before_execution":final_output})
    # 3. Parallel Execution
    if decision.clarification:
        final_output["results"].append({"type": "clarification", "message": decision.clarification})
    
    else:
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_sql = None
            future_diag = None
            
            # Submit Tasks based on Planner Decision
            if decision.sql_query:
                future_sql = executor.submit(manager.run_sql_agent, decision.sql_query)
            
            if decision.diagnostic_query:
                future_diag = executor.submit(manager.run_diagnostic_agent, decision.diagnostic_query)
            
            # Collect Results
            if future_sql:
                try:
                    res = future_sql.result()
                    final_output["results"].append({
                        "type": "sql_analysis",
                        "summary": res.answer_text,
                        "tables": res.tables,
                        "charts": res.charts,
                        "metadata": {
                            "sql": res.sql_generated,
                            "explanation": res.sql_explanation,
                            "is_verified": res.is_verified_query
                        }
                    })
                except Exception as e:
                    final_output["results"].append({"type": "sql_error", "message": str(e)})

            if future_diag:
                try:
                    res = future_diag.result()
                    final_output["results"].append({
                        "type": "diagnostic_analysis",
                        "summary": res.summary_text,
                        "graph": res.react_flow_json
                    })
                except Exception as e:
                    final_output["results"].append({"type": "diagnostic_error", "message": str(e)})

    # Final Result Checkpoint
    log_checkpoint("FINAL ORCHESTRATOR OUTPUT", final_output)
    return final_output







import json
import logging
import _snowflake # Native Snowflake module for internal API calls
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Literal
from pydantic import BaseModel, Field
from snowflake.snowpark import Session

# ==========================================
# 1. PYDANTIC CONFIGURATION & MODELS
# ==========================================

class AgentConfig(BaseModel):
    """Configuration for the agent execution"""
    model: str = "claude-4-sonnet"
    # Update paths as per your environment
    diagnostic_udf: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.DIAGNOSTIC_TOOL"
    # Updated to point to an existing Cortex Analyst Object (Semantic Model)
    cortex_analyst_object: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.LATEST_MASTER_VIEW"
    warehouse: str = "DEV_IMPACT_WH"

class PlannerDecision(BaseModel):
    """
    Structured output from the Planner Agent.
    Allows for parallel execution by defining multiple query targets.
    """
    sql_question: Optional[str] = Field(None, description="Question for data analysis, metrics, or lists (The 'Show Me' part). Null if not needed.")
    diagnostic_question: Optional[str] = Field(None, description="Question for root cause analysis or deep dives (The 'Why' part). Null if not needed.")
    clarification: Optional[str] = Field(None, description="Question to ask user if intent is completely vague.")
    reasoning: str = Field(..., description="Why these routes were chosen")

class DiagnosticResult(BaseModel):
    """Structured output from the Diagnostic Agent"""
    summary_text: str = Field(..., description="Executive summary of findings")
    react_flow_json: Optional[Dict] = Field(None, description="The visual graph data")
    clarifying_question: Optional[str] = Field(None, description="Question to ask user if intent is unclear")

class SqlResult(BaseModel):
    """Structured output from the Cortex Analyst (SQL Agent)"""
    answer_text: str = Field(..., description="The text response from the analyst")
    tables: List[Any] = Field(default_factory=list, description="List of result sets (raw data)")
    charts: List[Dict] = Field(default_factory=list, description="List of Vega-Lite specs")
    # Extended Metadata Fields extracted from Analyst Delta
    sql_generated: Optional[str] = Field(None, description="The generated SQL query")
    sql_explanation: Optional[str] = Field(None, description="Explanation of the generated SQL")
    is_verified_query: bool = Field(False, description="Whether a verified query was used")
    reasoning_trace: Optional[str] = Field(None, description="The thinking process of the analyst")

# Global Config Instance
CONFIG = AgentConfig()

# ==========================================
# 2. DEBUGGING / CHECKPOINT HELPER
# ==========================================

def log_checkpoint(step: str, details: Any):
    """
    Prints checkpoint info to stdout. Handles Pydantic models serialization.
    """
    print(f"\n{'='*50}")
    print(f" CHECKPOINT: {step}")
    print(f"{'-'*50}")
    
    if isinstance(details, BaseModel):
        print(details.model_dump_json(indent=2))
    elif isinstance(details, (dict, list)):
        print(json.dumps(details, indent=2))
    else:
        print(str(details))
    print(f"{'='*50}\n")

# ==========================================
# 3. CORTEX API WRAPPER
# ==========================================

def invoke_cortex_agent(session: Session, payload: Dict, agent_name: str) -> Dict:
    """
    Invokes the Cortex Agent API via _snowflake.send_snow_api_request.
    Includes robust parsing for streaming, especially Analyst Deltas.
    """
    AGENT_ENDPOINT_URL = "/api/v2/cortex/agent:run"
    TIMEOUT_MS = 120 * 1000  # 120 seconds
    
    headers = {
        "Content-Type": "application/json",
        "Accept": "text/event-stream"
    }

    log_checkpoint(f"INVOKING AGENT: {agent_name}", {
        "model": payload.get("models", {}).get("orchestration"),
        "tools": [t['tool_spec']['name'] for t in payload.get("tools", [])]
    })

    try:
        response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {}, 
            payload, 
            {}, 
            TIMEOUT_MS
        )
        
        response_content = response_dict.get("content", "")
        if not response_content:
            return {"error": "Empty response from Agent API"}
            
        # Try parsing as JSON
        try:
            parsed_response = json.loads(response_content)
        except json.JSONDecodeError:
            return {"error": "Failed to decode JSON response content"}

        # Extraction container
        extracted = {
            "text": "",
            "tables": [],
            "charts": [],
            "annotations": [],
            "tool_uses": [],
            "debug_trace": [],
            "sql_generated": None,
            "sql_explanation": None,
            "is_verified": False
        }

        # Helper to process a content list (from final 'response')
        def process_content_list(content_items):
            for item in content_items:
                c_type = item.get("type")
                if c_type == "text":
                    extracted["text"] += item.get("text", "")
                    if item.get("annotations"):
                        extracted["annotations"].extend(item["annotations"])
                elif c_type == "table":
                    extracted["tables"].append(item.get("result_set", {}))
                elif c_type == "chart":
                    extracted["charts"].append(item.get("chart", {}))
                elif c_type == "tool_use":
                    extracted["tool_uses"].append(item)

        # 1. Handle List of Events (Streaming format)
        if isinstance(parsed_response, list):
            # First pass: Look for the final 'response' event
            final_response_event = next((item for item in parsed_response if item.get('event') == 'response'), None)
            
            if final_response_event:
                data = final_response_event.get('data', {})
                process_content_list(data.get('content', []))
            
            # Scan deltas for reconstruction and metadata extraction
            # This is crucial for capturing 'sql', 'verified_query_used' which appear in tool_result.analyst.delta
            text_parts = []
            for item in parsed_response:
                evt = item.get('event')
                data = item.get('data', {})
                
                if evt == 'response.text.delta':
                    text_parts.append(data.get('text', ''))
                elif evt == 'response.text.annotation':
                    extracted["annotations"].append(data.get('annotation'))
                elif evt == 'response.chart':
                    extracted["charts"].append(data.get('chart') or data.get('chart_spec'))
                elif evt == 'response.table':
                    extracted["tables"].append(data.get('result_set'))
                elif evt == 'response.tool_use':
                    extracted["tool_uses"].append(data)
                elif evt == 'response.thinking.delta':
                    extracted["debug_trace"].append(f"Think: {data.get('text')}")
                
                # --- CRITICAL: ANALYST TOOL DELTA PARSING ---
                elif evt == 'response.tool_result.analyst.delta':
                    delta = data.get('delta', {})
                    
                    # 1. SQL Code
                    if delta.get('sql'):
                        extracted["sql_generated"] = delta.get('sql')
                    
                    # 2. Explanation
                    if delta.get('sql_explanation'):
                        extracted["sql_explanation"] = delta.get('sql_explanation')
                    
                    # 3. Verified Query Flag
                    if 'verified_query_used' in delta:
                        extracted["is_verified"] = delta.get('verified_query_used')
                    
                    # 4. Result Set (The Table Data)
                    if delta.get('result_set'):
                        # Check to avoid duplication if response.table handled it
                        rs = delta.get('result_set')
                        if rs not in extracted["tables"]:
                            extracted["tables"].append(rs)
                    
                    # 5. Reasoning/Think
                    if delta.get('think'):
                        extracted["debug_trace"].append(f"Analyst Think: {delta.get('think')}")
                    
                    # 6. Suggestions
                    if delta.get('suggestions'):
                        sug = delta.get('suggestions')
                        extracted["debug_trace"].append(f"Suggestion: {sug.get('delta')}")

                elif evt == 'error':
                    return {"error": f"Agent Error: {data.get('message')}"}
            
            if not extracted["text"]:
                extracted["text"] = "".join(text_parts)

        # 2. Handle Single Object (Non-streaming)
        elif isinstance(parsed_response, dict):
            if 'message' in parsed_response:
                process_content_list(parsed_response['message'].get('content', []))
            elif 'choices' in parsed_response:
                process_content_list(parsed_response['choices'][0]['message'].get('content', []))

        return extracted

    except Exception as e:
        return {"error": f"API Execution Exception: {str(e)}"}

# ==========================================
# 4. PAYLOAD FACTORY
# ==========================================

class PayloadFactory:
    @staticmethod
    def create(query: str, instructions: Dict[str, str], tools: List[Dict] = None, tool_resources: Dict = None, history: List[Dict] = None) -> Dict:
        messages = history if history else []
        messages.append({"role": "user", "content": [{"type": "text", "text": query}]})

        return {
            "messages": messages,
            "tool_choice": {"type": "auto"},
            "models": {"orchestration": CONFIG.model},
            "instructions": instructions,
            "tools": tools or [],
            "tool_resources": tool_resources or {}
        }

# ==========================================
# 5. AGENT MANAGERS
# ==========================================

class AgentManager:
    def __init__(self, session: Session):
        self.session = session

    def run_rephraser(self, query: str, history: List[Dict] = None) -> str:
        """Step 1: Rephrase"""
        log_checkpoint("START STEP 1: Rephraser", f"Input: {query}")

        instructions = {
            "system": "You are a sophisticated Query Refinement Engine.",
            "response": "Output ONLY the final rephrased string.",
            "orchestration": "Transform the user's input into a standalone query. Resolve pronouns (it, they, that). Preserve metrics and IDs."
        }
        
        payload = PayloadFactory.create(f"Rephrase this to be standalone: {query}", instructions, history=history)
        resp = invoke_cortex_agent(self.session, payload, "Rephraser")
        
        result = resp.get('text', query).strip() or query
        log_checkpoint("END STEP 1: Rephraser", f"Output: {result}")
        return result

    def run_planner(self, query: str) -> PlannerDecision:
        """Step 2: Plan Route"""
        log_checkpoint("START STEP 2: Planner", f"Input: {query}")

        schema_hint = '{"sql_question": "string or null", "diagnostic_question": "string or null", "clarification": "string or null", "reasoning": "string"}'
        
        instructions = {
            "system": "You are a Strategic Intent Classifier.",
            "response": f"Respond ONLY with a valid JSON object matching: {schema_hint}",
            "orchestration": ("Analyze the user's input and SPLIT it into distinct **NATURAL LANGUAGE** questions. **YOU MUST NOT GENERATE NOT generate SQL code.**\n\n"
                "**1. SQL Analysis (`sql_question`):**\n"
                "   - Extract the part asking for Data, 'Show me', 'List', or 'What is'.\n"
                "   - Keep it as a plain English question. Do not write SELECT statements.\n"
                "   - Append ' and generate a table and a visual' if helpful.\n"
                "**2. Why Analysis (`diagnostic_question`):**\n"
                "   - Extract the part asking for 'Why', 'Explain', 'Reason', or Root Causes.\n"
                "   - **CRITICAL:** Do NOT assign 'What is the volume?' questions here. Only causal questions.\n"
                "   - Keep the text EXACT/VERBATIM. Do NOT summarize specific names/IDs.\n"
                "**3. Combined Intent:**\n"
                "   - If input is 'Show X and explain Y', put 'Show X' in `sql_question` and 'Explain Y' in `diagnostic_question`.\n\n"
                "**Few-Shot Examples:**\n"
                "- Input: 'Show me total sales for Q4.'\n"
                "  -> sql_question: 'Show me total sales for Q4 and generate a table and a visual'\n"
                "  -> diagnostic_question: null\n"
                "- Input: 'Why is profit down?'\n"
                "  -> sql_question: null\n"
                "  -> diagnostic_question: 'Why is profit down?'\n"
                "- Input: 'Get revenue for AZ and explain the dip.'\n"
                "  -> sql_question: 'Get revenue for AZ and generate a table and a visual'\n"
                "  -> diagnostic_question: 'explain the dip in revenue for AZ'\n"
                "- Input: 'SELECT * FROM SALES'\n"
                "  -> sql_question: 'SELECT * FROM SALES' (Only pass SQL if user explicitly typed it)\n"
                "  -> diagnostic_question: null"
                "YOU MUST STRICTLY FOLLOW THE ABOVE INSTRUCTIONS."
            )
        }
        
        payload = PayloadFactory.create(f"Plan this query: {query}", instructions)
        resp = invoke_cortex_agent(self.session, payload, "Planner")
        
        try:
            raw = resp.get('text', '')
            clean = raw.replace("```json", "").replace("```", "").strip()
            return PlannerDecision.model_validate_json(clean)
        except Exception:
            # Fallback assumption
            return PlannerDecision(diagnostic_query=query, reasoning="Fallback error during planning")

    def run_sql_agent(self, query: str) -> SqlResult:
        """Step 3b: Run Cortex Analyst (SQL Agent)"""
        log_checkpoint("START STEP 3b: SQL Agent", f"Input: {query}")

        instructions = {
            "system": "You are an expert Data Analyst.",
            "orchestration": "Use the analyst_tool to answer. ALWAYS generate a table (Result Set) for data requests.",
            "response": "Provide a summary. Tables/Charts are handled automatically."
        }

        tools = [{
            "tool_spec": {
                "type": "cortex_analyst_text_to_sql",
                "name": "analyst_tool"
            }
        }]

        tool_resources = {
            "analyst_tool": {
                "type": "cortex_analyst_text_to_sql",
                "semantic_view": CONFIG.cortex_analyst_object,
                "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse},
            }
        }

        payload = PayloadFactory.create(query, instructions, tools, tool_resources)
        resp = invoke_cortex_agent(self.session, payload, "SQL Agent")

        # Capture the standard artifacts + the newly extracted analyst delta fields
        result = SqlResult(
            answer_text=resp.get('text', ''),
            tables=resp.get('tables', []),
            charts=resp.get('charts', []),
            sql_generated=resp.get('sql_generated'),
            sql_explanation=resp.get('sql_explanation'),
            is_verified_query=resp.get('is_verified', False),
            reasoning_trace="\n".join(resp.get('debug_trace', []))
        )
        
        log_checkpoint("END STEP 3b: SQL Agent", result)
        return result

    def run_diagnostic_agent(self, query: str) -> DiagnosticResult:
        """Step 3a: Run Diagnostic Tool"""
        log_checkpoint("START STEP 3a: Diagnostic", f"Input: {query}")

        response_format = '{"summary_text": "string", "react_flow_json": {}, "clarifying_question": null}'
        instructions = {
            "system": "You are an Autonomous Root Cause Analysis Agent.",
            "orchestration": "Use Diagnostic_tool. Loop: Search -> Evaluate -> Drill.",
            "response": f"Ensure valid JSON matching: {response_format}"
        }

        tools = [{
            "tool_spec": {
                "type": "generic",
                "name": "Diagnostic_tool",
                "description": "Retrieves metric status.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "USER_QUERY": {"type": "string"},
                        "PARENT_NODE_IDS_JSON": {"type": "string"},
                        "TREE_ID": {"type": "string"}
                    },
                    "required": ["USER_QUERY", "PARENT_NODE_IDS_JSON", "TREE_ID"]
                }
            }
        }]

        tool_resources = {
            "Diagnostic_tool": {
                "type": "procedure",
                "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse},
                "identifier": CONFIG.diagnostic_udf
            }
        }

        payload = PayloadFactory.create(query, instructions, tools, tool_resources)
        resp = invoke_cortex_agent(self.session, payload, "Diagnostic Agent")

        try:
            raw = resp.get('text', '{}')
            clean = raw.replace("```json", "").replace("```", "").strip()
            result = DiagnosticResult.model_validate_json(clean)
        except Exception as e:
            result = DiagnosticResult(summary_text=f"Error: {str(e)} - Raw: {raw}", react_flow_json=None)

        log_checkpoint("END STEP 3a: Diagnostic", result)
        return result

# ==========================================
# 6. MAIN EXECUTION (PARALLEL)
# ==========================================

def main(session: Session):
    manager = AgentManager(session)
    history = []
    
    # Specific input requesting both Data (SQL) and Explanation (Diagnostic)
    user_query = "show ME all the health care providers in arizona and Why is the Total Market Volume being driven by the Opioid Category , specifically Tramadol which is seeing massive New Growth, yet the Orthopedic Segment historically the core driver for this drugis contributing less than 7% of that growth?"
    
    log_checkpoint("CONTEXT", {"history": history, "current_query": user_query})
    
    # 1. Rephrase
    rephrased = manager.run_rephraser(user_query, history)
    
    # 2. Plan
    decision = manager.run_planner(rephrased)
    
    final_output = {
        "original_query": user_query,
        "processed_query": rephrased,
        "execution_plan": decision.model_dump(),
        "results": []
    }

    # 3. Parallel Execution
    if decision.clarification:
        final_output["results"].append({"type": "clarification", "message": decision.clarification})
    
    else:
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_sql = None
            future_diag = None
            
            # Submit Tasks based on Planner Decision
            if decision.sql_question:
                future_sql = executor.submit(manager.run_sql_agent, decision.sql_question)
            
            if decision.diagnostic_question:
                future_diag = executor.submit(manager.run_diagnostic_agent, decision.diagnostic_question)
            
            # Collect Results
            if future_sql:
                try:
                    res = future_sql.result()
                    final_output["results"].append({
                        "type": "sql_analysis",
                        "summary": res.answer_text,
                        "tables": res.tables,
                        "charts": res.charts,
                        "metadata": {
                            "sql": res.sql_generated,
                            "explanation": res.sql_explanation,
                            "is_verified": res.is_verified_query
                        }
                    })
                except Exception as e:
                    final_output["results"].append({"type": "sql_error", "message": str(e)})

            if future_diag:
                try:
                    res = future_diag.result()
                    final_output["results"].append({
                        "type": "diagnostic_analysis",
                        "summary": res.summary_text,
                        "graph": res.react_flow_json
                    })
                except Exception as e:
                    final_output["results"].append({"type": "diagnostic_error", "message": str(e)})

    # Final Result Checkpoint
    log_checkpoint("FINAL ORCHESTRATOR OUTPUT", final_output)
    return final_output


https://www.snowflake.com/en/blog/snowflake-cortex-ai-cortex-guard-llm-safeguards/



import json
import logging
import _snowflake # Native Snowflake module for internal API calls
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Literal
from pydantic import BaseModel, Field
from snowflake.snowpark import Session

# ==========================================
# 1. PYDANTIC CONFIGURATION & MODELS
# ==========================================

class AgentConfig(BaseModel):
    """Configuration for the agent execution"""
    model: str = "claude-4-sonnet" # Recommended for complex orchestration
    # Update paths as per your environment
    diagnostic_udf: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.DIAGNOSTIC_TOOL"
    cortex_analyst_object: str = "DEV_P_PAIN_SALES_PERFORMANCE_ANALYTICS_DB.AI_POC.LATEST_MASTER_VIEW"
    warehouse: str = "DEV_IMPACT_WH"
    # Cortex Guard Configuration
    guard_name: str = "INPUT_GUARD" 

class IntentClassification(BaseModel):
    """Structured output from the Intent Classifier."""
    intent_type: str = Field(..., description="Type of intent: greeting, data_retrieval, root_cause_analysis, clarification_needed, general_question, off_topic, combined")
    data_retrieval_query: Optional[str] = Field(None, description="Natural language query for retrieving data/metrics/lists.")
    root_cause_query: Optional[str] = Field(None, description="Natural language query for causal analysis.")
    direct_response: Optional[str] = Field(None, description="Direct answer for greetings or simple questions.")
    clarification_question: Optional[str] = Field(None, description="Question to ask user when intent is unclear.")
    confidence: str = Field(..., description="Confidence level: high, medium, low")
    reasoning: str = Field(..., description="Explanation of why this classification was made")

class DiagnosticResult(BaseModel):
    """Structured output from the Diagnostic Agent"""
    summary_text: str = Field(..., description="Executive summary of findings")
    react_flow_json: Optional[Dict] = Field(None, description="The visual graph data")
    clarifying_question: Optional[str] = Field(None, description="Question to ask user if intent is unclear")

class SqlResult(BaseModel):
    """Structured output from the Cortex Analyst (SQL Agent)"""
    answer_text: str = Field(..., description="The text response from the analyst")
    tables: List[Any] = Field(default_factory=list, description="List of result sets (raw data)")
    charts: List[Dict] = Field(default_factory=list, description="List of Vega-Lite specs")
    sql_generated: Optional[str] = Field(None, description="The generated SQL query")
    sql_explanation: Optional[str] = Field(None, description="Explanation of the generated SQL")
    is_verified_query: bool = Field(False, description="Whether a verified query was used")
    reasoning_trace: Optional[str] = Field(None, description="The thinking process of the analyst")

class GuardResult(BaseModel):
    """Result from Cortex Guard evaluation"""
    is_safe: bool = Field(..., description="Whether the input passed the guard")
    violation_category: Optional[str] = Field(None, description="Category of violation if any")
    message: Optional[str] = Field(None, description="Message to user if blocked")

# Global Config Instance
CONFIG = AgentConfig()

# ==========================================
# 2. DEBUGGING / CHECKPOINT HELPER
# ==========================================

def log_checkpoint(step: str, details: Any):
    """Prints checkpoint info to stdout."""
    print(f"\n{'='*50}")
    print(f" CHECKPOINT: {step}")
    print(f"{'-'*50}")
    
    if isinstance(details, BaseModel):
        print(details.model_dump_json(indent=2))
    elif isinstance(details, (dict, list)):
        print(json.dumps(details, indent=2))
    else:
        print(str(details))
    print(f"{'='*50}\n")

# ==========================================
# 3. CORTEX GUARD WRAPPER
# ==========================================

def check_input_guard(session: Session, user_input: str) -> GuardResult:
    """
    Checks user input against Snowflake Cortex Guard.
    Implementation: Calls SNOWFLAKE.CORTEX.COMPLETE with guardrails=True options.
    """
    log_checkpoint("CORTEX GUARD CHECK", f"Input: {user_input}")
    
    try:
        # We use a lightweight check. If the model returns the standard block message, it's unsafe.
        # Note: We escape single quotes in the input to prevent SQL injection in the f-string
        safe_input = user_input.replace("'", "''")
        
        # We ask the model to simply echo the input if safe. 
        # If unsafe, Cortex Guard will intercept and replace the response.
        guard_sql = f"""
        SELECT SNOWFLAKE.CORTEX.COMPLETE(
            '{CONFIG.model}',
            [
                {{'role': 'user', 'content': '{safe_input}'}}
            ],
            {{
                'guardrails': true
            }}
        ) as response_json
        """
        
        result = session.sql(guard_sql).collect()
        
        if not result:
            return GuardResult(is_safe=True)

        # Parse the JSON response from Cortex
        response_str = result[0]['RESPONSE_JSON']
        response_data = json.loads(response_str)
        
        # Cortex Guard typically returns a specific message when content is filtered
        # Format: {"choices": [{"messages": "Response filtered by Cortex Guard"}]}
        model_output = response_data.get('choices', [{}])[0].get('messages', '')
        
        is_unsafe = "Response filtered by Cortex Guard" in model_output
        
        guard_result = GuardResult(
            is_safe=not is_unsafe,
            violation_category="unsafe_content" if is_unsafe else None,
            message="I cannot process this request because it violates our safety policy." if is_unsafe else None
        )
        
        log_checkpoint("GUARD RESULT", guard_result)
        return guard_result
        
    except Exception as e:
        log_checkpoint("GUARD ERROR", f"Error checking guard: {str(e)}")
        # Fail open - allow request to proceed if guard check fails to run
        return GuardResult(is_safe=True)

# ==========================================
# 4. CORTEX API WRAPPER
# ==========================================

def invoke_cortex_agent(session: Session, payload: Dict, agent_name: str) -> Dict:
    """Invokes the Cortex Agent API via _snowflake.send_snow_api_request."""
    AGENT_ENDPOINT_URL = "/api/v2/cortex/agent:run"
    TIMEOUT_MS = 120 * 1000  # 120 seconds
    
    headers = {
        "Content-Type": "application/json",
        "Accept": "text/event-stream"
    }

    log_checkpoint(f"INVOKING AGENT: {agent_name}", {
        "model": payload.get("models", {}).get("orchestration"),
        "tools": [t['tool_spec']['name'] for t in payload.get("tools", [])]
    })

    try:
        response_dict = _snowflake.send_snow_api_request(
            "POST",
            AGENT_ENDPOINT_URL,
            headers,
            {}, 
            payload, 
            {}, 
            TIMEOUT_MS
        )
        
        response_content = response_dict.get("content", "")
        if not response_content:
            return {"error": "Empty response from Agent API"}
            
        try:
            parsed_response = json.loads(response_content)
        except json.JSONDecodeError:
            return {"error": "Failed to decode JSON response content"}

        extracted = {
            "text": "",
            "tables": [],
            "charts": [],
            "annotations": [],
            "tool_uses": [],
            "debug_trace": [],
            "sql_generated": None,
            "sql_explanation": None,
            "is_verified": False
        }

        def process_content_list(content_items):
            for item in content_items:
                c_type = item.get("type")
                if c_type == "text":
                    extracted["text"] += item.get("text", "")
                    if item.get("annotations"):
                        extracted["annotations"].extend(item["annotations"])
                elif c_type == "table":
                    extracted["tables"].append(item.get("result_set", {}))
                elif c_type == "chart":
                    extracted["charts"].append(item.get("chart", {}))
                elif c_type == "tool_use":
                    extracted["tool_uses"].append(item)

        if isinstance(parsed_response, list):
            final_response_event = next((item for item in parsed_response if item.get('event') == 'response'), None)
            
            if final_response_event:
                data = final_response_event.get('data', {})
                process_content_list(data.get('content', []))
            
            text_parts = []
            for item in parsed_response:
                evt = item.get('event')
                data = item.get('data', {})
                
                if evt == 'response.text.delta':
                    text_parts.append(data.get('text', ''))
                elif evt == 'response.text.annotation':
                    extracted["annotations"].append(data.get('annotation'))
                elif evt == 'response.chart':
                    extracted["charts"].append(data.get('chart') or data.get('chart_spec'))
                elif evt == 'response.table':
                    extracted["tables"].append(data.get('result_set'))
                elif evt == 'response.tool_use':
                    extracted["tool_uses"].append(data)
                elif evt == 'response.thinking.delta':
                    extracted["debug_trace"].append(f"Think: {data.get('text')}")
                elif evt == 'response.tool_result.analyst.delta':
                    delta = data.get('delta', {})
                    if delta.get('sql'): extracted["sql_generated"] = delta.get('sql')
                    if delta.get('sql_explanation'): extracted["sql_explanation"] = delta.get('sql_explanation')
                    if 'verified_query_used' in delta: extracted["is_verified"] = delta.get('verified_query_used')
                    if delta.get('result_set'):
                        rs = delta.get('result_set')
                        if rs not in extracted["tables"]: extracted["tables"].append(rs)
                elif evt == 'error':
                    return {"error": f"Agent Error: {data.get('message')}"}
            
            if not extracted["text"]:
                extracted["text"] = "".join(text_parts)

        elif isinstance(parsed_response, dict):
            if 'message' in parsed_response:
                process_content_list(parsed_response['message'].get('content', []))
            elif 'choices' in parsed_response:
                process_content_list(parsed_response['choices'][0]['message'].get('content', []))

        return extracted

    except Exception as e:
        return {"error": f"API Execution Exception: {str(e)}"}

# ==========================================
# 5. PAYLOAD FACTORY
# ==========================================

class PayloadFactory:
    @staticmethod
    def create(query: str, instructions: Dict[str, str], tools: List[Dict] = None, tool_resources: Dict = None, history: List[Dict] = None) -> Dict:
        messages = history if history else []
        messages.append({"role": "user", "content": [{"type": "text", "text": query}]})

        return {
            "messages": messages,
            "tool_choice": {"type": "auto"},
            "models": {"orchestration": CONFIG.model},
            "instructions": instructions,
            "tools": tools or [],
            "tool_resources": tool_resources or {}
        }

# ==========================================
# 6. AGENT MANAGERS
# ==========================================

class AgentManager:
    def __init__(self, session: Session):
        self.session = session

    def run_rephraser(self, query: str, history: List[Dict] = None) -> str:
        """Step 1: Rephrase"""
        log_checkpoint("START STEP 1: Rephraser", f"Input: {query}")
        
        instructions = {
            "system": "You are a sophisticated Query Refinement Engine.",
            "response": "Output ONLY the final rephrased string.",
            "orchestration": """
<query_refinement_instructions>
<objective>
Transform the user's input into a standalone, context-complete query that can be understood without prior conversation history.
</objective>

<core_rules>
<rule id="1">Resolve all pronouns (it, they, that, this, those) to their specific referents</rule>
<rule id="2">Preserve ALL metrics, entity names, IDs, and numerical values EXACTLY as stated</rule>
<rule id="3">Maintain the original intent and scope - do not expand or narrow the question</rule>
<rule id="4">Keep technical terminology and domain-specific language intact</rule>
<rule id="5">Ensure the output is grammatically correct and reads naturally</rule>
</core_rules>

<transformation_examples>
<example>
<input>What about Arizona?</input>
<context>Previous discussion about sales performance in Texas</context>
<output>What is the sales performance in Arizona?</output>
</example>

<example>
<input>Why is it down?</input>
<context>Previous mention of Q4 revenue</context>
<output>Why is Q4 revenue down?</output>
</example>

<example>
<input>Show me those providers and explain the trend.</input>
<context>Healthcare providers in California with declining prescriptions</context>
<output>Show me healthcare providers in California with declining prescriptions and explain the trend.</output>
</example>
</transformation_examples>

<output_format>
Output ONLY the refined query string. Do not include explanations, metadata, or formatting.
</output_format>
</query_refinement_instructions>
"""
        }
        
        payload = PayloadFactory.create(f"Rephrase this to be standalone: {query}", instructions, history=history)
        resp = invoke_cortex_agent(self.session, payload, "Rephraser")
        
        result = resp.get('text', query).strip() or query
        log_checkpoint("END STEP 1: Rephraser", f"Output: {result}")
        return result

    def run_intent_classifier(self, query: str) -> IntentClassification:
        """Step 2: Classify Intent"""
        log_checkpoint("START STEP 2: Intent Classifier", f"Input: {query}")

        schema_hint = '''{
          "intent_type": "greeting | data_retrieval | root_cause_analysis | clarification_needed | general_question | off_topic | combined",
          "data_retrieval_query": "string or null",
          "root_cause_query": "string or null", 
          "direct_response": "string or null",
          "clarification_question": "string or null",
          "confidence": "high | medium | low",
          "reasoning": "string"
        }'''
        
        instructions = {
            "system": "You are an Expert Intent Classification System.",
            "response": f"Respond ONLY with a valid JSON object matching: {schema_hint}",
            "orchestration": """
<intent_classification_instructions>

<mission>
Your ONLY job is to identify the user's INTENT. Do NOT retrieve data, do NOT answer questions, do NOT perform analysis.
Classify the intent type and route accordingly. If you can answer directly (like greetings), provide direct_response.
</mission>

<intent_categories>

<intent name="greeting">
<description>User is greeting, saying hello, or engaging in pleasantries</description>
<indicators>hi, hello, hey, good morning, how are you, thanks, thank you, goodbye, bye</indicators>
<action>Set direct_response with a friendly greeting. No other fields needed.</action>
</intent>

<intent name="data_retrieval">
<description>User wants to see/get/retrieve DATA - numbers, lists, metrics, tables</description>
<indicators>show, list, get, display, find, what is the value, how many, count, total, give me</indicators>
<action>Extract the data request to data_retrieval_query. Keep as natural language. Add "and generate a table and a visual".</action>
</intent>

<intent name="root_cause_analysis">
<description>User wants to understand WHY, CAUSES, DRIVERS, REASONS, EXPLANATIONS</description>
<indicators>why, explain, what is driving, what is causing, what's behind, reason for, root cause, how come, what factors</indicators>
<action>Extract the causal question to root_cause_query. Keep VERBATIM - exact text from user.</action>
</intent>

<intent name="combined">
<description>User wants BOTH data AND causal analysis in one request</description>
<indicators>Contains both data keywords AND causal keywords</indicators>
<action>Split into data_retrieval_query AND root_cause_query. Keep both portions.</action>
</intent>

<intent name="clarification_needed">
<description>User's intent is unclear, vague, or ambiguous - you cannot determine what they want</description>
<indicators>Very short/vague input, unclear pronouns without context, incomplete thoughts</indicators>
<action>Set clarification_question asking user to be more specific.</action>
</intent>

<intent name="general_question">
<description>General questions about the system, capabilities, how to use features - not data or analysis requests</description>
<indicators>how do I, can you, what can you do, help me understand, how does this work</indicators>
<action>Set direct_response with helpful information about capabilities.</action>
</intent>

<intent name="off_topic">
<description>User is asking about unrelated topics not connected to data, analytics, or business questions</description>
<indicators>Questions about weather, sports, entertainment, personal topics unrelated to business/data</indicators>
<action>Set direct_response politely redirecting to business/data topics.</action>
</intent>

</intent_categories>

<CRITICAL_CLASSIFICATION_RULES>

 RULE 1: NEVER DO THE WORK - ONLY CLASSIFY
You are NOT a data analyst. You are NOT an answering agent. You ONLY identify intent type.
Do NOT retrieve data. Do NOT answer analytical questions. ONLY classify and route.

 RULE 2: CAUSAL KEYWORD DETECTION
These ALWAYS indicate root_cause_analysis intent:
- "what is driving"
- "what is causing"
- "what's driving"  
- "what's causing"
- "why is/are"
- "explain why"
- "reason for"
- "root cause"
- "what factors"
- "what's behind"
- "how come"

 RULE 3: DATA KEYWORD DETECTION  
These ALWAYS indicate data_retrieval intent:
- "show me"
- "list"
- "get"
- "display"
- "find all"
- "what is the value/total/count of"
- "how many"

 RULE 4: DIRECT RESPONSE ONLY FOR SIMPLE CASES
Use direct_response ONLY for:
- Greetings (hi, hello, thanks)
- General system questions (what can you do)
- Off-topic redirects

Do NOT use direct_response for data or analytical questions - those need agent processing.

 RULE 5: VERBATIM PRESERVATION FOR ROOT CAUSE
For root_cause_query: Copy EXACT text from user. No paraphrasing, no summarizing.

 RULE 6: NO SQL CODE GENERATION
NEVER write SELECT/FROM/WHERE statements. Keep everything as natural language.

 RULE 7: CONFIDENCE SCORING
- high: Intent is crystal clear
- medium: Intent is likely but has some ambiguity
- low: Intent is very unclear, might need clarification

</CRITICAL_CLASSIFICATION_RULES>

<classification_process>
<step number="1">Check if greeting or general question. If match, Set intent_type and direct_response, STOP</step>
<step number="2">Check for causal keywords. If found, Flag as root_cause_analysis or combined</step>
<step number="3">Check for data keywords. If found, Flag as data_retrieval or combined</step>
<step number="4">Determine if combined intent. If both, Set intent_type="combined".</step>
<step number="5">If none of the above match, check if intent is clear enough.</step>
</classification_process>

<validation_checklist>
 Step 1: Can I answer this directly? (greeting/general_question/off_topic)
 Step 2: Does it contain causal keywords? (why/driving/causing/explain/reason/factors/behind)
 Step 3: Does it contain data keywords? (show/list/get/display/what is the value/how many)
 Step 4: Does it have BOTH data and causal elements?
 Step 5: Did I preserve EXACT wording for root_cause_query?
 Step 6: Did I add "and generate a table and a visual" to data_retrieval_query?
 Step 7: Did I avoid doing the actual work?
 Step 8: Is my confidence level accurate?
</validation_checklist>
</intent_classification_instructions>
"""
        }

        payload = PayloadFactory.create(f"Classify this user input: {query}", instructions)
        resp = invoke_cortex_agent(self.session, payload, "Intent Classifier")
        
        try:
            raw = resp.get('text', '')
            clean = raw.replace("```json", "").replace("```", "").strip()
            result = IntentClassification.model_validate_json(clean)
        except Exception as e:
            # Fallback
            result = IntentClassification(
                intent_type="clarification_needed",
                clarification_question="I'm having trouble understanding your request. Could you please rephrase?",
                confidence="low",
                reasoning=f"Error parsing intent: {str(e)}"
            )
            
        log_checkpoint("END STEP 2: Intent Classifier", result)
        return result

    def run_data_agent(self, query: str) -> SqlResult:
        """Step 3a: Run Cortex Analyst (Data Retrieval Agent)"""
        log_checkpoint("START STEP 3a: Data Retrieval Agent", f"Input: {query}")
        
        instructions = {
            "system": "You are an expert Data Analyst.",
            "orchestration": """
<data_agent_instructions>
<objective>
Use the analyst_tool to answer data requests with comprehensive results including tables and visualizations.
</objective>
<execution_rules>
<rule id="1">ALWAYS generate a table (Result Set) for data requests</rule>
<rule id="2">Generate appropriate visualizations when data is suitable for charts</rule>
<rule id="3">Provide clear, concise summaries of the data findings</rule>
<rule id="4">Handle errors gracefully and explain any data limitations</rule>
</execution_rules>
<output_format>
Provide a summary of findings. Tables and charts are handled automatically by the tool.
</output_format>
</data_agent_instructions>
""",
            "response": "Provide a summary. Tables/Charts are handled automatically."
        }

        tools = [{
            "tool_spec": {
                "type": "cortex_analyst_text_to_sql",
                "name": "analyst_tool"
            }
        }]
        
        tool_resources = {
            "analyst_tool": {
                "type": "cortex_analyst_text_to_sql",
                "semantic_view": CONFIG.cortex_analyst_object,
                "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse},
            }
        }
        
        payload = PayloadFactory.create(query, instructions, tools, tool_resources)
        resp = invoke_cortex_agent(self.session, payload, "Data Retrieval Agent")
        
        result = SqlResult(
            answer_text=resp.get('text', ''),
            tables=resp.get('tables', []),
            charts=resp.get('charts', []),
            sql_generated=resp.get('sql_generated'),
            sql_explanation=resp.get('sql_explanation'),
            is_verified_query=resp.get('is_verified', False),
            reasoning_trace="\n".join(resp.get('debug_trace', []))
        )
        
        log_checkpoint("END STEP 3a: Data Retrieval Agent", result)
        return result

    def run_root_cause_agent(self, query: str) -> DiagnosticResult:
        """Step 3b: Run Root Cause Analysis Agent"""
        log_checkpoint("START STEP 3b: Root Cause Analysis Agent", f"Input: {query}")
        
        response_format = '{"summary_text": "string", "react_flow_json": {}, "clarifying_question": null}'
        
        instructions = {
            "system": "You are an Autonomous Root Cause Analysis Agent.",
            "orchestration": """
<root_cause_orchestration_prompt>
    <role_and_objective>
        You are an **Autonomous Root Cause Analysis Agent** designed to traverse a metric decision tree from the root node down to the leaf nodes. Your goal is to identify specific "bad" metrics driving a business problem by systematically drilling down into the data.
        
        ALWAYS USE tree_id as Pharma_Master_v1.
    </role_and_objective>

    <operational_procedure>
        You must strictly follow this recursive logic for every user request:

        **Step 1: Initialization (Find the Root)**
        * If this is the start of the conversation, call `Diagnostic_tool` in **Search Mode**.
        * **Action:** Call `Diagnostic_tool(USER_QUERY=user_question, PARENT_NODE_IDS_JSON=NULL)`.
        * **Next:** Proceed to Step 2 with the results.

        **Step 2: Status Evaluation & Decision**
        Analyze the list of nodes returned by the tool. Focus **only** on nodes where `"status": "bad"`.

        * **Check:** For every "bad" node, do you see its children in the current list?
        * **Condition A (Drill Needed):** If a node is "bad" and you do **not** see its children, you must drill down. Collect these Node IDs.
        * **Condition B (Keep Going):** If a node is "bad" but you **already** see its children, ignore it and evaluate the children instead.
        * **Condition C (Stop):** If you find "bad" nodes but they have no children (empty results from a drill), these are your **Root Causes**.

        **Step 3: Execution (Drill Down)**
        * If Condition A occurred: Call `Diagnostic_tool` in **Drill Mode**.
        * **Action:** Call `Diagnostic_tool(USER_QUERY=NULL, PARENT_NODE_IDS_JSON='["ID_1", "ID_2"]')`.
        * **Next:** Recursively repeat Step 2 with the new output.
        * If Condition C occurred: Present the final root causes to the user.
    </operational_procedure>

    <constraints>
        * **Never** guess the status of a metric; always use the tool.
        * **Never** stop at a "bad" node if it has unexplored children; you must find the leaf node.
        * Output your reasoning briefly before calling the tool.
    </constraints>
</root_cause_orchestration_prompt>
""",
            "response": """<response_structure_prompt>
    <instruction>
        You must output exactly two sections in the following order. Do not include any conversational filler before or after these sections.
    </instruction>

    <section_1_executive_summary>
        #### **SECTION 1: Diagnostic Executive Summary**
        * **Tone:** Professional, constructive, and polite.
        * **Headline:** State the Root Cause clearly.
        * **The Narrative:** Briefly explain the chain of events in simple English.
        * **Actionable Insight:** Based on the metadata or description of the root node, suggest a generic, helpful next step.
    </section_1_executive_summary>

    <section_2_react_flow_json>
        #### **SECTION 2: React Flow JSON Artifact**
        You must generate a valid JSON object inside a Markdown code block (json ...).
        
        **1. Coordinate Logic (The "Mental Layout"):**
        * **Level 0 (Root):** `x: 0, y: 0`
        * **Level 1 Children:** `y: 150`. Spread `x` widely (e.g., `-300` and `300`).
        * **Level 2 Grandchildren:** `y: 300`. Spread `x` relative to their parent.
        * **Level 3+:** Increase `y` by `150` for each level.

        **2. Visual Style Logic:**
        * **Red (#FFE2E5):** Status is "bad", "high", or "error".
        * **Green (#CDE8E6):** Status is "good", "normal", or "stable".
        * **Orange (#FFCCBC):** The **Root Cause** (the deepest "bad" node in the chain).
        * **White (#FFFFFF):** Unexplored or neutral nodes.

        **3. Edge Labeling Logic:**
        * If Parent is Bad and Child is Bad, label = "contributes to impact".
        * If Parent is Bad and Child is Good, label = "stable factor".

        **4. JSON Schema (Must Follow Exactly):**
        {
          "nodes": [
            {
              "id": "N0",
              "type": "default",
              "data": { "label": "Metric Name: Value" },
              "position": { "x": 0, "y": 0 },
              "style": { "background": "#FFE2E5", "width": 180, "color": "#333", "border": "1px solid #777", "borderRadius": "8px" }
            }
          ],
          "edges": [
            {
              "id": "e-N0-N1",
              "source": "N0",
              "target": "N1",
              "label": "driven by",
              "animated": true,
              "style": { "stroke": "#555", "strokeWidth": 2 }
            }
          ]
        }
    </section_2_react_flow_json>
    
    Resposne format -> {response_format} 
    
</response_structure_prompt>"""
        }

        tools = [{
            "tool_spec": {
                "type": "generic",
                "name": "Diagnostic_tool",
                "description": "Retrieves metric status and dimensional breakdowns for root cause analysis.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "USER_QUERY": {"type": "string"},
                        "PARENT_NODE_IDS_JSON": {"type": "string"},
                        "TREE_ID": {"type": "string"}
                    },
                    "required": ["USER_QUERY", "PARENT_NODE_IDS_JSON", "TREE_ID"]
                }
            }
        }]
        
        tool_resources = {
            "Diagnostic_tool": {
                "type": "procedure",
                "execution_environment": {"type": "warehouse", "warehouse": CONFIG.warehouse},
                "identifier": CONFIG.diagnostic_udf
            }
        }
        
        payload = PayloadFactory.create(query, instructions, tools, tool_resources)
        resp = invoke_cortex_agent(self.session, payload, "Root Cause Analysis Agent")
        
        try:
            raw = resp.get('text', '{}')
            clean = raw.replace("```json", "").replace("```", "").strip()
            result = DiagnosticResult.model_validate_json(clean)
        except Exception as e:
            result = DiagnosticResult(summary_text=f"Error: {str(e)} - Raw: {raw}", react_flow_json=None)
            
        log_checkpoint("END STEP 3b: Root Cause Analysis Agent", result)
        return result

# ==========================================
# 7. MAIN EXECUTION
# ==========================================

def main(session: Session):
    manager = AgentManager(session)
    history = []

    # Complex Query Testing Combined Intent
    user_query = "show ME all the health care providers in arizona and Why is the Total Market Volume being driven by the Opioid Category , specifically Tramadol which is seeing massive New Growth, yet the Orthopedic Segment historically the core driver for this drugis contributing less than 7% of that growth?"
    
    log_checkpoint("INITIAL INPUT", {"history": history, "current_query": user_query})

    # 0. Cortex Guard Check
    guard_result = check_input_guard(session, user_query)
    if not guard_result.is_safe:
        return {
            "status": "blocked",
            "message": guard_result.message,
            "violation": guard_result.violation_category
        }

    # 1. Rephrase
    rephrased = manager.run_rephraser(user_query, history)
    
    # 2. Intent Classification
    intent = manager.run_intent_classifier(rephrased)

    final_output = {
        "original_query": user_query,
        "processed_query": rephrased,
        "intent_classification": intent.model_dump(),
        "results": []
    }
    
    # 3. Route based on Intent
    
    # # Simple cases: Direct response (greetings, general questions, off-topic)
    # if intent.direct_response:
    #     final_output["results"].append({
    #         "type": "direct_response",
    #         "message": intent.direct_response
    #     })
    #     log_checkpoint("FINAL OUTPUT (Direct Response)", final_output)
    #     return final_output

    # # Clarification needed
    # if intent.intent_type == "clarification_needed":
    #     final_output["results"].append({
    #         "type": "clarification",
    #         "message": intent.clarification_question
    #     })
    #     log_checkpoint("FINAL OUTPUT (Clarification)", final_output)
    #     return final_output

    # # Agent processing needed (Data + Root Cause)
    # with ThreadPoolExecutor(max_workers=2) as executor:
    #     future_data = None
    #     future_root_cause = None
        
    #     # Submit tasks based on intent
    #     if intent.data_retrieval_query:
    #         future_data = executor.submit(manager.run_data_agent, intent.data_retrieval_query)
        
    #     if intent.root_cause_query:
    #         future_root_cause = executor.submit(manager.run_root_cause_agent, intent.root_cause_query)
        
    #     # Collect results
    #     if future_data:
    #         try:
    #             res = future_data.result()
    #             final_output["results"].append({
    #                 "type": "data_analysis",
    #                 "summary": res.answer_text,
    #                 "tables": res.tables,
    #                 "charts": res.charts,
    #                 "metadata": {
    #                     "sql": res.sql_generated,
    #                     "explanation": res.sql_explanation,
    #                     "is_verified": res.is_verified_query
    #                 }
    #             })
    #         except Exception as e:
    #             final_output["results"].append({
    #                 "type": "data_error",
    #                 "message": str(e)
    #             })

    #     if future_root_cause:
    #         try:
    #             res = future_root_cause.result()
    #             final_output["results"].append({
    #                 "type": "root_cause_analysis",
    #                 "summary": res.summary_text,
    #                 "graph": res.react_flow_json
    #             })
    #         except Exception as e:
    #             final_output["results"].append({
    #                 "type": "root_cause_error",
    #                 "message": str(e)
    #             })

    # log_checkpoint("FINAL ORCHESTRATOR OUTPUT", final_output)
    # return final_output




response.tool_result.analyst.delta
An delta event streamed for the Cortex Analyst tool execution

Field

Type

Description

content_index

integer

The index in the response content array this event represents

tool_use_id

string

Unique identifier for this tool use. Can be used to associated tool results.

tool_type

string

The type of the tool (always cortex_analyst_text_to_sql for this event)

tool_name

string

The unique identifier for this tool instance

delta

CortexAnalystToolResultDelta

The content delta

Example

{
  "content_index": 0,
  "tool_use_id": "toolu_123",
  "tool_type": "cortex_analyst_text_to_sql",
  "tool_name": "my_cortex_analyst_semantic_view",
  "delta": {
    "text": "The...",
    "think": "Thinking...",
    "sql": "SELECT...",
    "sql_explanation": "This...",
    "query_id": "707787a0-a684-4ead-adb0-3c3b62b043d9",
    "verified_query_used": false,
    "result_set": {
      "statementHandle": "707787a0-a684-4ead-adb0-3c3b62b043d9",
      "resultSetMetaData": {
        "partition": 0,
        "numRows": 0,
        "format": "jsonv2",
        "rowType": [
          {
            "name": "my_column",
            "type": "VARCHAR",
            "length": 0,
            "precision": 0,
            "scale": 0,
            "nullable": false
          }
        ]
      },
      "data": [
        [
          "row1 col1",
          "row1 col2"
        ],
        [
          "row2 col1",
          "row2 col2"
        ]
      ]
    },
    "suggestions": {
      "index": 0,
      "delta": "What..."
    }
  }
}

Invalid expression [PARSE_JSON('{"intent": "market_overview"}')] in VALUES clause
